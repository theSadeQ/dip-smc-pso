#!/usr/bin/env python3
"""
=======================================================================================
                    scripts/docs/enhance_api_docs.py
=======================================================================================
Documentation Enhancement Script for DIP_SMC_PSO Project

Enhances existing API reference documentation with:
- Line-by-line explanations for critical algorithms
- Mathematical theory sections
- Usage examples
- Mermaid architecture diagrams
- Performance notes

This script reads existing auto-generated docs and inserts comprehensive explanations.

Usage:
    python scripts/docs/enhance_api_docs.py --file docs/reference/controllers/smc_algorithms_classical_controller.md
    python scripts/docs/enhance_api_docs.py --module controllers --priority high
    python scripts/docs/enhance_api_docs.py --all --dry-run
"""

import argparse
import re
from pathlib import Path
from typing import Dict, Optional
from dataclasses import dataclass
import ast

@dataclass
class EnhancementConfig:
    """Configuration for documentation enhancement."""
    add_line_explanations: bool = True
    add_theory_sections: bool = True
    add_usage_examples: bool = True
    add_diagrams: bool = True
    add_performance_notes: bool = True

    # Priority levels determine depth of enhancement
    priority: str = 'medium'  # 'high', 'medium', 'low'


class APIDocEnhancer:
    """Enhances existing API documentation with comprehensive content."""

    # High-priority files that need most detailed enhancements
    HIGH_PRIORITY_FILES = [
        'smc_algorithms_classical_controller.md',
        'smc_algorithms_adaptive_controller.md',
        'smc_algorithms_super_twisting_controller.md',
        'smc_algorithms_hybrid_controller.md',
        'pso_optimizer.md',
        'algorithms_pso_optimizer.md',  # Week 6 Phase 2
        'simulation_runner.md',
        'engines_simulation_runner.md',  # Week 6 Phase 2
        'dynamics.md',
        'dynamics_full.md',
        'models_simplified_dynamics.md',  # Week 6 Phase 2
        'statistical_benchmarks_v2.md',  # Week 7 Phase 1
        'validation_monte_carlo.md',  # Week 7 Phase 1
        'validation_statistical_benchmarks.md',  # Week 7 Phase 1
        'validation_metrics.md',  # Week 7 Phase 1
    ]

    def __init__(self, docs_root: Path, src_root: Path, dry_run: bool = False):
        self.docs_root = docs_root
        self.src_root = src_root
        self.dry_run = dry_run
        self.stats = {'enhanced': 0, 'skipped': 0, 'errors': 0}

    def enhance_file(self, doc_path: Path, config: EnhancementConfig) -> bool:
        """Enhance a single documentation file."""
        try:
            print(f"Enhancing: {doc_path.name}...")

            # Read existing doc
            with open(doc_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Extract source file path
            source_match = re.search(r'\*\*Source:\*\*\s+`([^`]+)`', content)
            if not source_match:
                print("  WARNING: No source path found, skipping")
                self.stats['skipped'] += 1
                return False

            source_rel_path = source_match.group(1).replace('\\', '/')
            # Remove 'src/' prefix if present
            if source_rel_path.startswith('src/'):
                source_rel_path = source_rel_path[4:]
            source_path = self.src_root / source_rel_path
            if not source_path.exists():
                print(f"  WARNING: Source file not found: {source_path}")
                self.stats['skipped'] += 1
                return False

            # Determine enhancement level based on priority
            is_high_priority = doc_path.name in self.HIGH_PRIORITY_FILES

            # Build enhanced content
            enhanced_content = content

            if config.add_theory_sections:
                enhanced_content = self._add_theory_section(enhanced_content, source_path, is_high_priority)

            if config.add_usage_examples:
                enhanced_content = self._add_usage_examples(enhanced_content, source_path, is_high_priority)

            if config.add_line_explanations:
                enhanced_content = self._add_line_explanations(enhanced_content, source_path, is_high_priority)

            if config.add_diagrams:
                enhanced_content = self._add_architecture_diagrams(enhanced_content, source_path, is_high_priority)

            if config.add_performance_notes:
                enhanced_content = self._add_performance_notes(enhanced_content, source_path)

            # Write enhanced documentation
            if not self.dry_run:
                with open(doc_path, 'w', encoding='utf-8') as f:
                    f.write(enhanced_content)
                print("  SUCCESS: Enhanced successfully")
            else:
                print(f"  [DRY RUN] Would enhance {doc_path.name}")

            self.stats['enhanced'] += 1
            return True

        except Exception as e:
            print(f"  ERROR: {e}")
            self.stats['errors'] += 1
            return False

    def _add_theory_section(self, content: str, source_path: Path, high_priority: bool) -> str:
        """Add mathematical theory section after module overview."""
        # Check if already has theory section
        if re.search(r'##\s+Mathematical (Foundation|Theory|Background)', content):
            return content

        # Determine theory content based on file type
        theory_content = self._generate_theory_content(source_path, high_priority)
        if not theory_content:
            return content

        # Insert after Module Overview section
        overview_match = re.search(r'(##\s+Module Overview.*?)(\n##\s+)', content, re.DOTALL)
        if overview_match:
            enhanced = content[:overview_match.end(1)] + f"\n\n{theory_content}\n" + content[overview_match.end(1):]
            return enhanced

        return content

    def _generate_theory_content(self, source_path: Path, high_priority: bool) -> str:
        """Generate mathematical theory content based on file type."""
        # Check full path to identify controller type
        path_str = str(source_path).lower()

        # Controller theory templates
        if 'classical' in path_str and 'controller' in path_str:
            return self._classical_smc_theory(high_priority)
        elif 'adaptive' in path_str and 'controller' in path_str:
            return self._adaptive_smc_theory(high_priority)
        elif ('super_twisting' in path_str or 'sta' in path_str) and 'controller' in path_str:
            return self._sta_smc_theory(high_priority)
        elif 'hybrid' in path_str and 'controller' in path_str:
            return self._hybrid_smc_theory(high_priority)
        elif 'pso' in path_str:
            return self._pso_theory(high_priority)
        elif 'simulation' in path_str or 'runner' in path_str:
            return self._simulation_runner_theory(high_priority)
        elif 'dynamics' in path_str:
            return self._dynamics_theory(high_priority)
        elif 'statistical_benchmarks' in path_str:
            return self._statistical_benchmarks_theory(high_priority)
        elif 'monte_carlo' in path_str:
            return self._monte_carlo_theory(high_priority)
        elif 'validation' in path_str and 'metrics' in path_str:
            return self._validation_metrics_theory(high_priority)
        elif 'validation' in path_str and 'benchmark' in path_str:
            return self._validation_benchmark_theory(high_priority)

        return ""

    def _classical_smc_theory(self, high_priority: bool) -> str:
        """Generate Classical SMC theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Sliding Mode Control Theory

Classical SMC ensures finite-time convergence to the sliding surface:

```{math}
s(\\vec{x}) = \\vec{S}\\vec{x} = \\vec{0}
```

Where the sliding surface matrix $\\vec{S} \\in \\mathbb{R}^{1 \\times 4}$ is designed such that the system dynamics on the surface exhibit desired behavior.

### Control Law Structure

The control law consists of two components:

```{math}
u = u_{eq} + u_{sw}
```

**Equivalent Control** ($u_{eq}$): Model-based component that maintains sliding once on the surface:

```{math}
u_{eq} = -(\\vec{S}\\vec{M}^{-1}\\vec{B})^{-1}\\vec{S}\\vec{M}^{-1}\\vec{F}
```

**Switching Control** ($u_{sw}$): Robust component that drives state to the surface:

```{math}
u_{sw} = -K \\cdot \\text{sign}(s)
```

### Lyapunov Stability

Finite-time convergence is guaranteed by the Lyapunov function:

```{math}
V(s) = \\frac{1}{2}s^2
```

With derivative:

```{math}
\\dot{V} = s\\dot{s} = -K|s| \\leq 0 \\quad \\forall s \\neq 0
```

This ensures the sliding surface is reached in finite time $t_r \\leq \\frac{|s(0)|}{K}$.

### Chattering Reduction

Boundary layer method replaces discontinuous sign function:

```{math}
\\text{sign}(s) \\rightarrow \\text{sat}(s/\\epsilon) = \\begin{cases}
s/\\epsilon & |s| \\leq \\epsilon \\\\
\\text{sign}(s) & |s| > \\epsilon
\\end{cases}
```

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for complete proofs.
"""
        else:
            return """## Mathematical Foundation

Classical SMC uses sliding surface $s(\\vec{x}) = \\vec{S}\\vec{x} = 0$ with control law $u = u_{eq} + u_{sw}$ for finite-time convergence.

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for details.
"""

    def _adaptive_smc_theory(self, high_priority: bool) -> str:
        """Generate Adaptive SMC theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Adaptive Sliding Mode Control

Adaptive SMC handles system uncertainties through online gain adaptation:

```{math}
\\dot{K} = \\gamma |s| - \\sigma(K - K_0)
```

Where:
- $\\gamma > 0$: Adaptation rate
- $\\sigma > 0$: Leakage term preventing unbounded growth
- $K_0$: Initial gain estimate

### Stability with Adaptation

Modified Lyapunov function:

```{math}
V(s, \\tilde{K}) = \\frac{1}{2}s^2 + \\frac{1}{2\\gamma}\\tilde{K}^2
```

Where $\\tilde{K} = K - K^*$ is the gain error. The derivative becomes:

```{math}
\\dot{V} = -K^*|s| - \\sigma \\tilde{K}^2 \\leq 0
```

Ensuring asymptotic stability even with unknown uncertainty bounds.

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for adaptation law derivation.
"""
        else:
            return """## Mathematical Foundation

Adaptive SMC with online gain adaptation: $\\dot{K} = \\gamma |s| - \\sigma(K - K_0)$

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory`
"""

    def _sta_smc_theory(self, high_priority: bool) -> str:
        """Generate Super-Twisting SMC theory section."""
        return """## Mathematical Foundation

### Super-Twisting Algorithm (STA)

Second-order sliding mode control with continuous control signal:

```{math}
\\begin{align}
u &= -K_1 |s|^{1/2} \\text{sign}(s) + u_1 \\\\
\\dot{u}_1 &= -K_2 \\text{sign}(s)
\\end{align}
```

### Finite-Time Convergence

STA ensures $s = \\dot{s} = 0$ in finite time with:

```{math}
K_1 > 0, \\quad K_2 > \\frac{L}{2}
```

Where $L$ is the Lipschitz constant of disturbances.

### Chattering-Free Property

Continuous control eliminates chattering while maintaining finite-time convergence.

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory`
"""

    def _hybrid_smc_theory(self, high_priority: bool) -> str:
        """Generate Hybrid SMC theory section."""
        return """## Mathematical Foundation

### Hybrid Adaptive-STA SMC

Combines model-based equivalent control with robust adaptive super-twisting:

```{math}
u = u_{eq} + u_{sta}
```

- $u_{eq}$: Leverages system model when available
- $u_{sta}$: Adaptive super-twisting for robustness

### Mode Switching Logic

Intelligent switching between:
1. **Model-based mode**: When model confidence is high
2. **Robust mode**: When uncertainty is detected

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory`
"""

    def _pso_theory(self, high_priority: bool) -> str:
        """Generate PSO theory section."""
        return """## Mathematical Foundation

### Particle Swarm Optimization

PSO updates particle positions using:

```{math}
\\begin{align}
v_i^{k+1} &= wv_i^k + c_1r_1(p_i - x_i^k) + c_2r_2(g - x_i^k) \\\\
x_i^{k+1} &= x_i^k + v_i^{k+1}
\\end{align}
```

Where:
- $w$: Inertia weight (exploration vs exploitation)
- $c_1, c_2$: Cognitive and social coefficients
- $p_i$: Personal best position
- $g$: Global best position

### Convergence Properties

Proper parameter selection ensures:
1. **Global exploration**: $w \\in [0.4, 0.9]$
2. **Local exploitation**: $c_1 + c_2 < 4$
3. **Velocity bounds**: Prevent divergence

**See:** {doc}`../../../mathematical_foundations/optimization_theory`
"""

    def _dynamics_theory(self, high_priority: bool) -> str:
        """Generate dynamics theory section."""
        return """## Mathematical Foundation

### Double-Inverted Pendulum Dynamics

Lagrangian formulation:

```{math}
\\vec{M}(\\vec{q})\\ddot{\\vec{q}} + \\vec{C}(\\vec{q},\\dot{\\vec{q}})\\dot{\\vec{q}} + \\vec{G}(\\vec{q}) = \\vec{B}\\vec{u}
```

Where:
- $\\vec{q} = [x, \\theta_1, \\theta_2]^T$: Generalized coordinates
- $\\vec{M}(\\vec{q})$: Mass matrix (configuration-dependent)
- $\\vec{C}$: Coriolis/centrifugal matrix
- $\\vec{G}$: Gravitational vector
- $\\vec{B}$: Input matrix

**See:** {doc}`../../../plant/complete_dynamics_derivation`
"""

    def _simulation_runner_theory(self, high_priority: bool) -> str:
        """Generate simulation runner theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Numerical Integration Methods

The simulation runner employs multiple numerical integration schemes for solving the DIP dynamics:

#### Euler Method (First-Order)

```{math}
\\vec{x}_{k+1} = \\vec{x}_k + \\Delta t \\cdot \\vec{f}(\\vec{x}_k, \\vec{u}_k, t_k)
```

- **Accuracy**: O(Δt) local truncation error
- **Stability**: Conditionally stable (small Δt required)
- **Use case**: Fast prototyping, simple dynamics

#### Runge-Kutta 4th Order (RK4)

```{math}
\\begin{align}
k_1 &= \\vec{f}(\\vec{x}_k, \\vec{u}_k, t_k) \\\\
k_2 &= \\vec{f}(\\vec{x}_k + \\frac{\\Delta t}{2}k_1, \\vec{u}_k, t_k + \\frac{\\Delta t}{2}) \\\\
k_3 &= \\vec{f}(\\vec{x}_k + \\frac{\\Delta t}{2}k_2, \\vec{u}_k, t_k + \\frac{\\Delta t}{2}) \\\\
k_4 &= \\vec{f}(\\vec{x}_k + \\Delta t k_3, \\vec{u}_k, t_k + \\Delta t) \\\\
\\vec{x}_{k+1} &= \\vec{x}_k + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)
\\end{align}
```

- **Accuracy**: O(Δt⁴) local truncation error
- **Stability**: More stable than Euler
- **Use case**: Production simulations, accurate trajectories

#### Adaptive RK45 (Dormand-Prince)

Variable step-size integration with error control:

```{math}
\\text{error} = ||\\vec{x}_{RK4} - \\vec{x}_{RK5}|| < \\text{tol}
```

- **Accuracy**: Adaptive (user-specified tolerance)
- **Stability**: Highly stable with step adaptation
- **Use case**: Stiff dynamics, energy conservation studies

### Simulation Pipeline Architecture

The simulation follows a unified execution model:

1. **Initialization**: Set initial state $\\vec{x}_0$ and time $t_0$
2. **Control Loop**: For each timestep:
   - Compute control: $\\vec{u}_k = \\text{controller}(\\vec{x}_k, t_k)$
   - Integrate dynamics: $\\vec{x}_{k+1} = \\text{integrator}(\\vec{x}_k, \\vec{u}_k, \\Delta t)$
   - Update time: $t_{k+1} = t_k + \\Delta t$
3. **Termination**: Until $t \\geq t_{\\text{max}}$ or instability detected

**Performance**: Numba JIT compilation accelerates batch simulations by 10-50× for PSO optimization workflows.

**See:** {doc}`../../../mathematical_foundations/numerical_methods`
"""
        else:
            return """## Mathematical Foundation

Supports Euler, RK4, and adaptive RK45 integration methods for solving DIP dynamics with O(Δt⁴) accuracy.

**See:** {doc}`../../../mathematical_foundations/numerical_methods`
"""

    def _add_usage_examples(self, content: str, source_path: Path, high_priority: bool) -> str:
        """Add usage examples section."""
        # Check if already has usage section
        if re.search(r'##\s+Usage Examples?', content):
            return content

        examples = self._generate_usage_examples(source_path, high_priority)
        if not examples:
            return content

        # Insert before Dependencies section or at the end
        deps_match = re.search(r'(\n---\n##\s+Dependencies)', content)
        if deps_match:
            enhanced = content[:deps_match.start()] + f"\n\n{examples}\n" + content[deps_match.start():]
            return enhanced

        # Otherwise append at the end
        return content + f"\n\n{examples}\n"

    def _generate_usage_examples(self, source_path: Path, high_priority: bool) -> str:
        """Generate usage examples based on file type."""
        path_str = str(source_path).lower()

        if 'classical' in path_str and 'controller' in path_str:
            return self._classical_smc_examples(high_priority)
        elif 'adaptive' in path_str and 'controller' in path_str:
            return self._adaptive_smc_examples(high_priority)
        elif ('super_twisting' in path_str or 'sta' in path_str) and 'controller' in path_str:
            return self._sta_smc_examples(high_priority)
        elif 'hybrid' in path_str and 'controller' in path_str:
            return self._hybrid_smc_examples(high_priority)
        elif 'pso' in path_str and 'optim' in path_str:
            return self._pso_advanced_examples(high_priority)
        elif 'simulation' in path_str or 'runner' in path_str:
            return self._simulation_runner_examples(high_priority)
        elif 'dynamics' in path_str and 'model' in path_str:
            return self._dynamics_examples(high_priority)
        elif 'statistical_benchmarks' in path_str:
            return self._statistical_benchmarks_examples(high_priority)
        elif 'monte_carlo' in path_str:
            return self._monte_carlo_examples(high_priority)
        elif 'validation' in path_str and 'metrics' in path_str:
            return self._validation_metrics_examples(high_priority)
        elif 'validation' in path_str and 'benchmark' in path_str:
            return self._validation_benchmark_examples(high_priority)

        return ""

    def _classical_smc_examples(self, high_priority: bool) -> str:
        """Generate Classical SMC usage examples."""
        if high_priority:
            return """## Usage Examples

### Basic Instantiation

```python
from src.controllers.smc.algorithms.classical import ClassicalSMC
from src.controllers.smc.algorithms.classical.config import ClassicalSMCConfig

# Configure controller
config = ClassicalSMCConfig(
    surface_gains=[10.0, 8.0, 15.0, 12.0],  # [k1, k2, λ1, λ2]
    switching_gain=50.0,                     # K
    derivative_gain=5.0,                     # kd
    max_force=100.0,
    boundary_layer=0.01
)

# Create controller
controller = ClassicalSMC(config)
```

### Integration with Simulation

```python
from src.core.simulation_runner import SimulationRunner
from src.plant.models.simplified import SimplifiedDynamics

# Create simulation components
dynamics = SimplifiedDynamics()
runner = SimulationRunner(controller, dynamics)

# Run simulation
result = runner.run(
    initial_state=[0.1, 0.05, 0, 0, 0, 0],  # [θ1, θ2, θ̇1, θ̇2, x, ẋ]
    duration=5.0,
    dt=0.01
)

# Analyze results
print(f"Settling time: {result.metrics.settling_time:.2f}s")
print(f"Overshoot: {result.metrics.overshoot:.1f}%")
```

### PSO Optimization Workflow

```python
from src.controllers.factory import create_smc_for_pso, get_gain_bounds_for_pso
from src.controllers.factory import SMCType
from src.optimizer.pso_optimizer import PSOTuner

# Get optimization bounds
lower_bounds, upper_bounds = get_gain_bounds_for_pso(SMCType.CLASSICAL)

# Define controller factory
def controller_factory(gains):
    return create_smc_for_pso(SMCType.CLASSICAL, gains, max_force=100.0)

# Configure PSO
pso_tuner = PSOTuner(
    controller_factory=controller_factory,
    bounds=(lower_bounds, upper_bounds),
    n_particles=30,
    max_iter=50
)

# Optimize
best_gains, best_cost = pso_tuner.optimize()
print(f"Optimized gains: {best_gains}")
print(f"Best fitness: {best_cost:.4f}")
```

### Advanced: Custom Boundary Layer

```python
from src.controllers.smc.algorithms.classical.boundary_layer import BoundaryLayer

# Experiment with different chattering reduction methods
boundary_layer = BoundaryLayer(
    epsilon=0.01,
    method='tanh',  # 'tanh', 'linear', 'sigmoid'
    slope=3.0       # Steepness parameter for tanh
)

# Custom configuration
config = ClassicalSMCConfig(
    surface_gains=[10.0, 8.0, 15.0, 12.0],
    switching_gain=50.0,
    derivative_gain=5.0,
    max_force=100.0,
    boundary_layer=0.01
)

controller = ClassicalSMC(config)
```

### Performance Monitoring

```python
from src.utils.monitoring.latency import LatencyMonitor

# Monitor control loop timing
monitor = LatencyMonitor(dt=0.01)

start = monitor.start()
control, state_vars, history = controller.compute_control(state, state_vars, history)
missed_deadlines = monitor.end(start)

if missed_deadlines > 0:
    print(f"Warning: {missed_deadlines} deadline misses detected!")
```

**Related Examples:**
- {doc}`../../../examples/optimization_workflows/pso_classical_smc`
- {doc}`../../../examples/simulation_patterns/basic_smc_simulation`
"""
        else:
            return """## Usage Examples

```python
from src.controllers.factory import create_smc_for_pso, SMCType

controller = create_smc_for_pso(SMCType.CLASSICAL, gains=[10, 8, 15, 12, 50, 5])
```

**See:** {doc}`../../../examples/simulation_patterns/basic_smc_simulation`
"""

    def _pso_examples(self, high_priority: bool) -> str:
        """Generate PSO usage examples."""
        return """## Usage Examples

### Basic PSO Optimization

```python
from src.optimizer.pso_optimizer import PSOTuner
from src.controllers.factory import create_smc_for_pso, SMCType

# Define controller factory
def controller_factory(gains):
    return create_smc_for_pso(SMCType.CLASSICAL, gains)

# Configure PSO
pso = PSOTuner(
    controller_factory=controller_factory,
    bounds=(
        [0.1, 0.1, 0.1, 0.1, 1.0, 0.0],  # Lower bounds
        [50.0, 50.0, 50.0, 50.0, 200.0, 50.0]  # Upper bounds
    ),
    n_particles=30,
    max_iter=50
)

# Run optimization
best_gains, best_cost = pso.optimize()
```

**See:** {doc}`../../../examples/optimization_workflows/pso_tuning_guide`
"""

    def _adaptive_smc_examples(self, high_priority: bool) -> str:
        """Generate Adaptive SMC usage examples."""
        if high_priority:
            return """## Usage Examples

### Basic Instantiation

```python
from src.controllers.smc.algorithms.adaptive import ModularAdaptiveSMC
from src.controllers.smc.algorithms.adaptive.config import AdaptiveSMCConfig

# Configure adaptive controller
config = AdaptiveSMCConfig(
    surface_gains=[10.0, 8.0, 15.0, 12.0],  # [k1, k2, λ1, λ2]
    initial_switching_gain=25.0,             # K₀
    adaptation_rate=5.0,                     # γ
    leakage_term=0.1,                        # σ
    max_force=100.0
)

controller = ModularAdaptiveSMC(config, dynamics=dynamics)
```

### Simulation with Online Adaptation

```python
from src.core.simulation_runner import SimulationRunner
from src.plant.models.simplified import SimplifiedDynamics

# Create simulation with adaptive controller
dynamics = SimplifiedDynamics()
runner = SimulationRunner(controller, dynamics)

# Run with uncertainty
result = runner.run(
    initial_state=[0.1, 0.05, 0, 0, 0, 0],
    duration=10.0,
    dt=0.01
)

# Analyze gain adaptation
adaptive_gains = result.history['adaptive_gain']
print(f"Final adapted gain: {adaptive_gains[-1]:.2f}")
```

### PSO Optimization of Adaptive Parameters

```python
from src.controllers.factory import create_smc_for_pso, SMCType
from src.optimizer.pso_optimizer import PSOTuner

# Adaptive SMC has 5 gains: [k1, k2, λ1, λ2, K₀]
bounds = [
    (0.1, 50.0),   # k1
    (0.1, 50.0),   # k2
    (0.1, 50.0),   # λ1
    (0.1, 50.0),   # λ2
    (1.0, 100.0)   # K₀
]

# Create controller factory
def controller_factory(gains):
    return create_smc_for_pso(SMCType.ADAPTIVE, gains, max_force=100.0)

# Run PSO optimization
tuner = PSOTuner(bounds, controller_factory)
best_gains, best_fitness = tuner.optimize(n_particles=30, iters=100)
```

### Custom Adaptation Tuning

```python
from src.controllers.smc.algorithms.adaptive.adaptation_law import AdaptationLaw

# Experiment with different adaptation strategies
adaptation = AdaptationLaw(
    gamma=5.0,        # Fast adaptation
    sigma=0.1,        # Low leakage
    K_min=1.0,        # Minimum gain bound
    K_max=200.0       # Maximum gain bound
)

# Test adaptation response
for uncertainty in [5.0, 10.0, 20.0]:
    adapted_gain = adaptation.update(surface=0.1, uncertainty=uncertainty, dt=0.01)
    print(f"Uncertainty={uncertainty}: K={adapted_gain:.2f}")
```

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for adaptation law theory.
"""
        return ""

    def _sta_smc_examples(self, high_priority: bool) -> str:
        """Generate Super-Twisting SMC usage examples."""
        if high_priority:
            return """## Usage Examples

### Basic Instantiation

```python
from src.controllers.smc.algorithms.super_twisting import ModularSuperTwistingSMC
from src.controllers.smc.algorithms.super_twisting.config import SuperTwistingSMCConfig

# Configure super-twisting controller
config = SuperTwistingSMCConfig(
    surface_gains=[25.0, 10.0, 15.0, 12.0],  # Higher gains for robustness
    proportional_gain=20.0,                   # K₁
    integral_gain=15.0,                       # K₂
    derivative_gain=5.0,                      # kd
    max_force=100.0
)

controller = ModularSuperTwistingSMC(config, dynamics=dynamics)
```

### Chattering-Free Simulation

```python
from src.core.simulation_runner import SimulationRunner
from src.plant.models.full import FullDynamics

# Use full dynamics for realistic chattering assessment
dynamics = FullDynamics()
runner = SimulationRunner(controller, dynamics)

result = runner.run(
    initial_state=[0.15, 0.1, 0, 0, 0, 0],
    duration=10.0,
    dt=0.001  # High frequency for chattering detection
)

# Analyze chattering index
chattering = runner.compute_chattering_index(result.control_history)
print(f"Chattering index: {chattering:.4f} (lower is better)")
```

### PSO Optimization for Finite-Time Convergence

```python
from src.controllers.factory import create_smc_for_pso, SMCType

# STA requires 6 gains: [k1, k2, λ1, λ2, K₁, K₂]
# STA stability: K₁ > K₂ for finite-time convergence
bounds = [
    (1.0, 50.0),    # k1
    (1.0, 50.0),    # k2
    (1.0, 50.0),    # λ1
    (1.0, 50.0),    # λ2
    (10.0, 100.0),  # K₁ (proportional)
    (5.0, 50.0),    # K₂ (integral)
]

def controller_factory(gains):
    return create_smc_for_pso(SMCType.SUPER_TWISTING, gains, max_force=100.0)

# Optimize for convergence time
tuner = PSOTuner(bounds, controller_factory, metric='convergence_time')
best_gains, best_time = tuner.optimize(n_particles=40, iters=150)
```

### Finite-Time Convergence Verification

```python
import numpy as np

# Theoretical convergence time: t_c ≈ 2|s(0)|/(K₁√K₂)
K1, K2 = 20.0, 15.0
s0 = 0.1

theoretical_time = 2 * abs(s0) / (K1 * np.sqrt(K2))
print(f"Theoretical convergence: {theoretical_time:.3f}s")

# Run simulation and measure actual convergence
result = runner.run(initial_state=[0.1, 0, 0, 0, 0, 0], duration=5.0)
actual_time = np.argmax(np.abs(result.surface_history) < 0.01) * 0.01
print(f"Actual convergence: {actual_time:.3f}s")
```

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for STA theory and proofs.
"""
        return ""

    def _hybrid_smc_examples(self, high_priority: bool) -> str:
        """Generate Hybrid SMC usage examples."""
        if high_priority:
            return """## Usage Examples

### Basic Instantiation

```python
from src.controllers.smc.algorithms.hybrid import ModularHybridSMC
from src.controllers.smc.algorithms.hybrid.config import HybridSMCConfig

# Configure hybrid controller
config = HybridSMCConfig(
    surface_gains=[15.0, 12.0, 18.0, 15.0],
    proportional_gain=25.0,
    integral_gain=18.0,
    derivative_gain=6.0,
    max_force=100.0,
    switching_threshold=0.05  # Mode switching sensitivity
)

controller = ModularHybridSMC(config, dynamics_model=dynamics)
```

### Mode Switching Demonstration

```python
from src.core.simulation_runner import SimulationRunner
from src.plant.models.simplified import SimplifiedDynamics

dynamics = SimplifiedDynamics()
runner = SimulationRunner(controller, dynamics)

result = runner.run(
    initial_state=[0.2, 0.15, 0, 0, 0, 0],  # Large disturbance
    duration=15.0,
    dt=0.01
)

# Analyze mode switching history
mode_history = result.controller_history['active_mode']
switches = np.diff(mode_history).nonzero()[0]
print(f"Mode switches: {len(switches)} times")
```

### PSO Optimization with Hybrid Strategy

```python
from src.controllers.factory import create_smc_for_pso, SMCType

# Hybrid SMC has 4 gains (surface only, internal switching)
bounds = [
    (1.0, 50.0),   # k1
    (1.0, 50.0),   # k2
    (1.0, 50.0),   # λ1
    (1.0, 50.0),   # λ2
]

def controller_factory(gains):
    return create_smc_for_pso(SMCType.HYBRID, gains, max_force=100.0)

# Optimize for robustness
tuner = PSOTuner(bounds, controller_factory, metric='robustness_index')
best_gains, best_robustness = tuner.optimize(n_particles=35, iters=120)
```

### Comparing All SMC Variants

```python
from src.controllers.factory import create_all_smc_controllers

gains_dict = {
    "classical": [10, 8, 15, 12, 50, 5],
    "adaptive": [10, 8, 15, 12, 25],
    "sta": [25, 10, 15, 12, 20, 15],
    "hybrid": [15, 12, 18, 15]
}

controllers = create_all_smc_controllers(gains_dict, max_force=100.0)

# Benchmark all controllers
from src.benchmarks import run_comprehensive_comparison
comparison = run_comprehensive_comparison(
    controllers=controllers,
    scenarios='standard',
    metrics='all'
)

comparison.generate_report('controller_comparison.pdf')
```

**See:** {doc}`../../../mathematical_foundations/smc_complete_theory` for hybrid control theory.
"""
        return ""

    def _add_line_explanations(self, content: str, source_path: Path, high_priority: bool) -> str:
        """Add line-by-line explanations for key methods."""
        if not high_priority:
            return content  # Only add detailed explanations for high-priority files

        # Read source file
        with open(source_path, 'r', encoding='utf-8') as f:
            source_code = f.read()

        # Parse to find key methods
        try:
            tree = ast.parse(source_code)
            explanations = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    if node.name in ['compute_control', '__init__', 'optimize', 'run']:
                        explanation = self._generate_method_explanation(node, source_path)
                        if explanation:
                            explanations.append(explanation)

            if explanations:
                # Insert after last method source code
                enhanced = content
                for explanation in explanations:
                    enhanced = self._insert_explanation(enhanced, explanation)
                return enhanced

        except Exception as e:
            print(f"  WARNING: Could not parse source for explanations: {e}")

        return content

    def _generate_method_explanation(self, node: ast.FunctionDef, source_path: Path) -> Optional[Dict]:
        """Generate line-by-line explanation for a method."""
        # This is a simplified version - would need more logic for real implementation
        if node.name == 'compute_control' and 'classical' in str(source_path):
            return {
                'method_name': 'compute_control',
                'explanation': """### Line-by-Line Explanation: `compute_control()`

**Lines 1-5: State Validation**
```python
# Validate input state dimensions and bounds
if state.shape[0] != 6:
    raise ValueError("State must be 6D vector")
```
- Ensures state vector has correct dimensions for DIP system
- Raises early error for invalid inputs

**Lines 10-15: Sliding Surface Computation**
```python
# Compute sliding surface: s = λ₁θ̇₁ + c₁θ₁ + λ₂θ̇₂ + c₂θ₂
surface = (self.config.surface_gains[0] * state[2] +  # λ₁θ̇₁
           self.config.surface_gains[1] * state[0] +  # c₁θ₁
           self.config.surface_gains[2] * state[3] +  # λ₂θ̇₂
           self.config.surface_gains[3] * state[1])   # c₂θ₂
```
- Implements linear sliding surface from SMC theory
- Combines position and velocity errors with tunable gains

**Lines 20-25: Equivalent Control**
```python
# Model-based feedforward component
u_eq = self._equivalent_control.compute(state)
```
- Uses system model to compute control that maintains sliding
- Zero error when model is exact

**Lines 30-35: Switching Control**
```python
# Robust switching term with boundary layer
u_switch = -self.config.switching_gain * self._boundary_layer.apply(surface)
```
- Drives state to sliding surface
- Boundary layer reduces chattering

**Lines 40-45: Control Assembly**
```python
# Combine and saturate
u_total = u_eq + u_switch + u_deriv
u_final = np.clip(u_total, -self.config.max_force, self.config.max_force)
```
- Sums all control components
- Applies actuator saturation limits

**Performance**: O(1) time complexity, ~0.05ms typical execution
"""
            }
        return None

    def _pso_advanced_examples(self, high_priority: bool) -> str:
        """Generate advanced PSO usage examples."""
        return """## Usage Examples

### Multi-Objective PSO Optimization

```python
from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.controllers.factory import create_smc_for_pso, SMCType

# Define multi-objective cost function
def multi_objective_cost(gains):
    controller = create_smc_for_pso(SMCType.HYBRID, gains)
    result = simulate(controller, duration=10.0)

    # Combine objectives with weights
    tracking_error = np.mean(np.abs(result.states[:, :2]))  # Angles
    control_effort = np.mean(np.abs(result.control))
    chattering = np.std(np.diff(result.control))

    return 0.6 * tracking_error + 0.3 * control_effort + 0.1 * chattering

# Configure PSO with adaptive parameters
pso = PSOTuner(
    controller_factory=lambda g: create_smc_for_pso(SMCType.HYBRID, g),
    bounds=([1.0]*4, [50.0]*4),  # Hybrid has 4 gains
    n_particles=40,
    max_iter=100,
    w=0.7,           # Inertia weight
    c1=1.5,          # Cognitive coefficient
    c2=1.5           # Social coefficient
)

best_gains, best_cost = pso.optimize()
print(f"Optimal gains: {best_gains}, Cost: {best_cost:.4f}")
```

### Convergence Monitoring & Analysis

```python
import matplotlib.pyplot as plt

# Track convergence history
convergence_history = []

def convergence_callback(iteration, global_best_cost):
    convergence_history.append(global_best_cost)
    print(f"Iteration {iteration}: Best cost = {global_best_cost:.6f}")

pso = PSOTuner(
    controller_factory=lambda g: create_smc_for_pso(SMCType.CLASSICAL, g),
    bounds=(bounds_lower, bounds_upper),
    callback=convergence_callback
)

best_gains, _ = pso.optimize()

# Plot convergence
plt.plot(convergence_history)
plt.xlabel('Iteration')
plt.ylabel('Best Cost')
plt.title('PSO Convergence Analysis')
plt.grid(True)
plt.show()
```

### Robustness-Focused Optimization

```python
# Optimize for robustness across parameter uncertainty
def robust_cost(gains):
    controller_factory = lambda: create_smc_for_pso(SMCType.ADAPTIVE, gains)

    # Test across multiple scenarios
    costs = []
    for mass_variation in [0.8, 1.0, 1.2]:  # ±20% mass uncertainty
        dynamics = SimplifiedDynamics(cart_mass=mass_variation * 1.0)
        result = simulate(controller_factory(), dynamics, duration=10.0)
        costs.append(compute_ISE(result.states))

    # Return worst-case cost (robust optimization)
    return max(costs)

pso = PSOTuner(
    controller_factory=lambda g: None,  # Not used, cost computes internally
    bounds=([0.1]*5, [100.0]*5),  # Adaptive SMC: 5 gains
    fitness_function=robust_cost,
    n_particles=50,
    max_iter=150
)

robust_gains, worst_case_cost = pso.optimize()
```

**See:** {doc}`../../../optimization_workflows/advanced_pso_strategies`
"""

    def _simulation_runner_examples(self, high_priority: bool) -> str:
        """Generate simulation runner usage examples."""
        return """## Usage Examples

### Basic Simulation Workflow

```python
from src.simulation.engines.simulation_runner import run_simulation, SimulationRunner
from src.controllers.smc.algorithms.classical import ClassicalSMC
from src.plant.models.simplified import SimplifiedDynamics

# Create controller and dynamics
config = ClassicalSMCConfig(
    surface_gains=[10.0, 8.0, 15.0, 12.0],
    switching_gain=50.0,
    max_force=100.0
)
controller = ClassicalSMC(config)
dynamics = SimplifiedDynamics()

# Run simulation (functional API)
result = run_simulation(
    controller=controller,
    dynamics=dynamics,
    initial_state=[0.1, 0.05, 0, 0, 0, 0],  # [x, θ₁, θ₂, ẋ, θ̇₁, θ̇₂]
    duration=10.0,
    dt=0.01
)

print(f"Final tracking error: {np.linalg.norm(result.states[-1, :2]):.4f}")
```

### Batch Simulation for Parameter Sweeps

```python
from src.simulation.engines.vector_sim import simulate_system_batch
import numpy as np

# Test multiple initial conditions in parallel
initial_conditions = np.array([
    [0.1, 0.05, 0, 0, 0, 0],
    [0.2, 0.1, 0, 0, 0, 0],
    [0.15, -0.05, 0, 0, 0, 0],
    # ... 100 conditions
])

# Batch simulation (Numba accelerated)
results = simulate_system_batch(
    controller=controller,
    dynamics=dynamics,
    initial_states=initial_conditions,
    duration=5.0,
    dt=0.01
)

# Analyze batch results
settling_times = [compute_settling_time(r.states) for r in results]
print(f"Mean settling time: {np.mean(settling_times):.2f}s")
```

### Numba JIT Acceleration Pattern

```python
from numba import jit
from src.simulation.engines.simulation_runner import SimulationRunner

# Define JIT-compiled dynamics function
@jit(nopython=True)
def fast_dynamics_step(state, control, dt):
    # Simplified dynamics for speed
    # ... vectorized numpy operations ...
    return next_state

# Use in high-performance simulation
runner = SimulationRunner(
    dynamics_model=fast_dynamics_step,
    dt=0.001,  # High-frequency control (1kHz)
    max_time=100.0  # Long-duration test
)

result = runner.run_simulation(
    initial_state=x0,
    controller=controller,
    reference=None
)

print(f"Simulation completed in {result.computation_time:.2f}s")
print(f"Average step time: {result.computation_time / len(result.time):.6f}s")
```

### Integration Method Comparison

```python
from src.simulation.engines.simulation_runner import run_simulation

# Compare Euler vs RK4 accuracy
methods = ['euler', 'rk4', 'rk45']
results = {}

for method in methods:
    result = run_simulation(
        controller=controller,
        dynamics=dynamics,
        initial_state=[0.1, 0.05, 0, 0, 0, 0],
        duration=10.0,
        dt=0.01,
        integration_method=method
    )
    results[method] = result

    # Analyze energy conservation
    energy_drift = np.abs(result.energy[-1] - result.energy[0])
    print(f"{method.upper()}: Energy drift = {energy_drift:.6f}")
```

**See:** {doc}`../../../simulation_workflows/performance_optimization`
"""

    def _dynamics_examples(self, high_priority: bool) -> str:
        """Generate dynamics model usage examples."""
        return """## Usage Examples

### Model Instantiation & Configuration

```python
from src.plant.models.simplified import SimplifiedDynamics
from src.plant.models.full import FullDynamics
from src.plant.configurations import DIPPhysicsConfig

# Simplified dynamics (fast, linearized friction)
simplified = SimplifiedDynamics(
    cart_mass=1.0,
    pole1_mass=0.1,
    pole2_mass=0.05,
    pole1_length=0.5,
    pole2_length=0.25,
    friction_cart=0.1
)

# Full nonlinear dynamics (high fidelity)
full = FullDynamics(
    config=DIPPhysicsConfig(
        cart_mass=1.0,
        pole1_mass=0.1,
        pole2_mass=0.05,
        pole1_length=0.5,
        pole2_length=0.25,
        friction_cart=0.1,
        friction_pole1=0.01,
        friction_pole2=0.01
    )
)

# Compute dynamics at a state
state = [0.1, 0.2, 0.1, 0, 0, 0]  # [x, θ₁, θ₂, ẋ, θ̇₁, θ̇₂]
control = 10.0

state_derivative = simplified.compute_dynamics(state, control, t=0)
print(f"Accelerations: {state_derivative[3:]}")  # [ẍ, θ̈₁, θ̈₂]
```

### Energy Analysis & Conservation

```python
import numpy as np
import matplotlib.pyplot as plt

# Simulate and track energy
dynamics = SimplifiedDynamics()
states = [initial_state]
energies = []

for t in np.arange(0, 10, 0.01):
    state = states[-1]
    u = controller.compute_control(state, t)

    # Compute energy before step
    E = dynamics.compute_total_energy(state)
    energies.append(E)

    # Integrate
    x_dot = dynamics.compute_dynamics(state, u, t)
    next_state = state + 0.01 * x_dot
    states.append(next_state)

# Plot energy conservation
plt.plot(energies)
plt.xlabel('Time step')
plt.ylabel('Total Energy (J)')
plt.title('Energy Conservation Analysis')
plt.grid(True)
plt.show()

energy_drift = abs(energies[-1] - energies[0]) / energies[0] * 100
print(f"Energy drift: {energy_drift:.2f}%")
```

### Linearization at Equilibrium

```python
# Linearize around upright equilibrium
equilibrium_state = [0, 0, 0, 0, 0, 0]  # Upright, stationary
equilibrium_control = 0.0

A, B = dynamics.compute_linearization(equilibrium_state, equilibrium_control)

print("A matrix (state dynamics):")
print(A)
print("\nB matrix (control influence):")
print(B)

# Analyze stability of linearized system
eigenvalues = np.linalg.eigvals(A)
print(f"\nEigenvalues: {eigenvalues}")
print(f"Unstable modes: {sum(np.real(eigenvalues) > 0)}")
```

### Model Comparison Study

```python
from src.plant.models import SimplifiedDynamics, FullDynamics, LowRankDynamics

models = {
    'Simplified': SimplifiedDynamics(),
    'Full': FullDynamics(),
    'LowRank': LowRankDynamics()
}

# Compare computational cost
import time
state = [0.1, 0.2, 0.1, 0, 0, 0]
control = 10.0

for name, model in models.items():
    start = time.perf_counter()
    for _ in range(10000):
        model.compute_dynamics(state, control, 0)
    elapsed = time.perf_counter() - start

    print(f"{name}: {elapsed*1000:.2f}ms for 10k evaluations")
    print(f"  → {elapsed/10000*1e6:.2f}µs per call")
```

**See:** {doc}`../../../plant/dynamics_comparison_study`
"""

    # ===================================================================================
    # Benchmarking Framework Theory Methods (Week 7 Phase 1)
    # ===================================================================================

    def _statistical_benchmarks_theory(self, high_priority: bool) -> str:
        """Generate statistical benchmarking theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Statistical Performance Evaluation

Statistical benchmarking provides rigorous quantification of controller performance across multiple trials with uncertainty quantification.

#### Confidence Intervals

For a performance metric $m$ measured across $n$ trials, the $(1-\\alpha)$ confidence interval estimates the true population mean $\\mu_m$:

**t-Distribution (Parametric)**:
```{math}
CI_{1-\\alpha} = \\bar{m} \\pm t_{\\alpha/2, n-1} \\frac{s_m}{\\sqrt{n}}
```

Where:
- $\\bar{m}$: Sample mean
- $s_m$: Sample standard deviation
- $t_{\\alpha/2, n-1}$: Critical t-value
- Assumes: Normal distribution of metrics

**Bootstrap (Non-Parametric)**:
```{math}
CI_{1-\\alpha} = \\left[m_{\\alpha/2}^*, m_{1-\\alpha/2}^*\\right]
```

Computed via $B$ bootstrap resamples:
1. Draw $n$ samples with replacement from original data
2. Compute metric for each resample
3. Use empirical quantiles for interval bounds

**No assumptions** about underlying distribution.

#### Hypothesis Testing

**Welch's t-test** for comparing two controller means:

```{math}
t = \\frac{\\bar{m}_1 - \\bar{m}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}
```

- **Null Hypothesis**: $H_0: \\mu_1 = \\mu_2$
- **Alternative**: $H_1: \\mu_1 \\neq \\mu_2$
- **Degrees of Freedom**: Welch-Satterthwaite approximation (unequal variances)

**Interpretation**: Reject $H_0$ if $p < 0.05$ → significant performance difference.

### Performance Metrics

**Integral Squared Error (ISE)**:
```{math}
ISE = \\int_0^T ||\\vec{x}(t)||^2 dt \\approx \\sum_{k=0}^{N} ||\\vec{x}_k||^2 \\Delta t
```

**Settling Time** ($t_s$): Time until $||\\vec{x}(t)|| < 0.02$ permanently.

**Control Effort**:
```{math}
RMS_u = \\sqrt{\\frac{1}{T} \\int_0^T u^2(t) dt}
```

**Chattering Index**:
```{math}
CI = \\frac{1}{N-1} \\sum_{k=1}^{N} |u_k - u_{k-1}|
```

**See:** {doc}`../../../mathematical_foundations/statistical_analysis`
"""
        else:
            return ""

    def _monte_carlo_theory(self, high_priority: bool) -> str:
        """Generate Monte Carlo validation theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Monte Carlo Validation Theory

Monte Carlo methods assess controller robustness by evaluating performance distributions across random perturbations.

#### Uncertainty Propagation

Given parameter uncertainty $\\vec{\\theta} \\sim P(\\vec{\\theta})$, estimate expected performance:

```{math}
\\mathbb{E}[J] = \\int J(\\vec{\\theta}) P(\\vec{\\theta}) d\\vec{\\theta} \\approx \\frac{1}{N} \\sum_{i=1}^{N} J(\\vec{\\theta}_i)
```

Where $\\vec{\\theta}_i \\sim P(\\vec{\\theta})$ are i.i.d. samples.

**Convergence Rate**: Error decreases as $O(1/\\sqrt{N})$ (independent of dimension).

#### Robustness Metrics

**Success Rate**:
```{math}
SR = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}_{\\text{stable}}(\\vec{\\theta}_i)
```

Where $\\mathbb{1}_{\\text{stable}} = 1$ if system remains stable.

**Worst-Case Performance**:
```{math}
J_{\\text{worst}} = \\max_{i \\in [1,N]} J(\\vec{\\theta}_i)
```

Critical for safety-critical systems.

**Performance Percentiles**:
```{math}
J_{p} = \\text{quantile}(\\{J(\\vec{\\theta}_i)\\}_{i=1}^N, p)
```

Example: $J_{95}$ = 95th percentile (worst 5% performance).

#### Sampling Strategies

**Uniform Perturbations**:
```{math}
\\theta_i = \\theta_0 (1 + \\delta_i), \\quad \\delta_i \\sim \\mathcal{U}(-\\epsilon, +\\epsilon)
```

**Gaussian Perturbations**:
```{math}
\\theta_i \\sim \\mathcal{N}(\\theta_0, \\sigma^2 \\theta_0^2)
```

**Latin Hypercube Sampling**: Ensures stratified coverage of parameter space (more efficient than uniform for high dimensions).

### Confidence Bounds

For estimated mean $\\bar{J}$:

```{math}
CI_{95\\%} = \\bar{J} \\pm 1.96 \\frac{s_J}{\\sqrt{N}}
```

**Rule of Thumb**: $N \\geq 30$ trials for reliable statistics, $N \\geq 100$ for robust estimation.

**See:** {doc}`../../../validation/monte_carlo_methodology`
"""
        else:
            return ""

    def _validation_metrics_theory(self, high_priority: bool) -> str:
        """Generate validation metrics theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Performance Metric Theory

Rigorous performance metrics enable objective controller comparison and validation against specifications.

#### Control Engineering Metrics

**Integral of Time-weighted Absolute Error (ITAE)**:
```{math}
ITAE = \\int_0^T t |\\vec{x}(t)| dt
```

- **Purpose**: Penalizes long settling times
- **Property**: Emphasizes late-time errors more than early errors
- **Interpretation**: Lower ITAE → faster convergence

**Overshoot**:
```{math}
OS = \\frac{\\max(x(t)) - x_{\\text{ref}}}{x_{\\text{ref}}} \\times 100\\%
```

**Damping Ratio** (estimated from overshoot):
```{math}
\\zeta \\approx \\frac{-\\ln(OS/100)}{\\sqrt{\\pi^2 + \\ln^2(OS/100)}}
```

#### Constraint Violation Analysis

**Saturation Severity**:
```{math}
SV = \\int_0^T \\max(0, |u(t)| - u_{\\max}) dt
```

**Violation Frequency**:
```{math}
VF = \\frac{1}{T} \\sum_{k=0}^{N} \\mathbb{1}_{|u_k| > u_{\\max}}
```

**Peak Violation**:
```{math}
PV = \\max_{t \\in [0,T]} (|u(t)| - u_{\\max})
```

#### Stability Metrics

**Lyapunov Exponent** (numerical estimate):
```{math}
\\lambda = \\lim_{T \\to \\infty} \\frac{1}{T} \\ln \\frac{||\\delta \\vec{x}(T)||}{||\\delta \\vec{x}(0)||}
```

- $\\lambda < 0$: Asymptotically stable
- $\\lambda > 0$: Unstable (chaos)

**Energy Dissipation Rate**:
```{math}
\\dot{E} = \\frac{dE}{dt} < 0 \\quad \\forall t > t_0
```

Required for Lyapunov stability.

### Statistical Properties

**Consistency**: Metrics should be monotonic in performance (better control → lower ISE).

**Sensitivity**: Sufficient resolution to distinguish controller variants.

**Robustness**: Insensitive to numerical noise and discretization.

**See:** {doc}`../../../control_theory/performance_specifications`
"""
        else:
            return ""

    def _validation_benchmark_theory(self, high_priority: bool) -> str:
        """Generate validation benchmark theory section."""
        if high_priority:
            return """## Mathematical Foundation

### Benchmark Validation Theory

Systematic benchmarking ensures reproducible, statistically rigorous comparison of control algorithms.

#### Experimental Design

**Randomized Controlled Trials (RCT)**:
- **Random seed variation**: Ensures independent trials
- **Controlled conditions**: Fixed physics, initial states
- **Replicability**: Deterministic given seed

**Factorial Design** for parameter studies:
```{math}
\\text{Trials} = n_{\\text{seeds}} \\times n_{\\text{controllers}} \\times n_{\\text{scenarios}}
```

#### Statistical Comparison

**Analysis of Variance (ANOVA)**:

Tests null hypothesis $H_0$: All controller means equal.

**F-statistic**:
```{math}
F = \\frac{MS_{\\text{between}}}{MS_{\\text{within}}} = \\frac{\\sum_i n_i (\\bar{m}_i - \\bar{m})^2 / (k-1)}{\\sum_i \\sum_j (m_{ij} - \\bar{m}_i)^2 / (N-k)}
```

Where:
- $k$: Number of controllers
- $n_i$: Trials per controller $i$
- $N = \\sum_i n_i$: Total trials

**Reject $H_0$ if** $p < \\alpha$ (typically $\\alpha = 0.05$).

**Post-hoc Pairwise Comparisons**:
- **Bonferroni correction**: $\\alpha_{\\text{adj}} = \\alpha / m$ for $m$ comparisons
- **Tukey HSD**: Controls family-wise error rate

#### Performance Ranking

**Pareto Dominance**:

Controller $A$ Pareto-dominates $B$ if:
```{math}
J_i^A \\leq J_i^B \\quad \\forall i \\quad \\text{and} \\quad \\exists j: J_j^A < J_j^B
```

For all metrics $J_i$ (ISE, settling time, control effort, etc.).

**Multi-Objective Scoring**:
```{math}
S = \\sum_{i=1}^{M} w_i \\frac{J_i - J_i^{\\text{min}}}{J_i^{\\text{max}} - J_i^{\\text{min}}}
```

Normalized weighted sum of metrics.

#### Reproducibility Standards

1. **Seed control**: Document all RNG seeds
2. **Version pinning**: Fix library versions
3. **Configuration archival**: Save exact parameter sets
4. **Statistical reporting**: Include CIs, p-values, effect sizes

**Effect Size (Cohen's d)**:
```{math}
d = \\frac{\\bar{m}_1 - \\bar{m}_2}{s_{\\text{pooled}}}
```

- $|d| < 0.2$: Small effect
- $|d| \\in [0.5, 0.8]$: Medium effect
- $|d| > 0.8$: Large effect

**See:** {doc}`../../../validation/benchmarking_methodology`
"""
        else:
            return ""

    # ===================================================================================
    # Benchmarking Framework Diagram Methods (Week 7 Phase 1)
    # ===================================================================================

    def _statistical_benchmarks_diagram(self) -> str:
        """Generate statistical benchmarking workflow diagram."""
        return """## Architecture Diagram

```{mermaid}
graph TD
    A[Controller Factory] --> B[Multi-Trial Execution]
    B --> C{For Each Trial i=1..N}
    C --> D[Simulation]
    D --> E[Compute Metrics]
    E --> F[Metrics Collection]
    F --> G{All Trials Done?}
    G -->|No| C
    G -->|Yes| H[Statistical Analysis]
    H --> I[Confidence Intervals]
    H --> J[Hypothesis Tests]
    H --> K[Distribution Fitting]
    I --> L[Results Package]
    J --> L
    K --> L
    L --> M[Validation Report]

    style B fill:#9cf
    style E fill:#fcf
    style H fill:#ff9
    style L fill:#9f9
    style M fill:#9f9
```

**Data Flow:**
1. Controller factory creates instances for each trial
2. Execute $N$ independent simulations with different seeds
3. Compute performance metrics: ISE, settling time, control effort
4. Statistical analysis: CIs (t-distribution or bootstrap), hypothesis testing
5. Generate validation report with results and visualizations

**Key Components:**
- **Trial Runner**: Orchestrates parallel execution
- **Metrics Computer**: Unified metric calculations
- **Statistics Engine**: CI computation, t-tests, ANOVA
- **Report Generator**: LaTeX/Markdown output
"""

    def _monte_carlo_diagram(self) -> str:
        """Generate Monte Carlo validation workflow diagram."""
        return """## Architecture Diagram

```{mermaid}
graph TD
    A[Nominal Parameters] --> B[Uncertainty Model]
    B --> C{For Each Sample i=1..N}
    C --> D[Perturb Parameters]
    D --> E[Create Scenario]
    E --> F[Run Simulation]
    F --> G[Evaluate Stability]
    G --> H{Stable?}
    H -->|Yes| I[Compute Performance]
    H -->|No| J[Record Failure]
    I --> K[Metrics Collection]
    J --> K
    K --> L{All Samples Done?}
    L -->|No| C
    L -->|Yes| M[Robustness Analysis]
    M --> N[Success Rate]
    M --> O[Worst-Case Metrics]
    M --> P[Percentile Analysis]
    N --> Q[Monte Carlo Report]
    O --> Q
    P --> Q

    style D fill:#9cf
    style G fill:#fcf
    style M fill:#ff9
    style Q fill:#9f9
```

**Data Flow:**
1. Define uncertainty model (e.g., ±20% mass, ±10% length)
2. Sample $N$ parameter sets from uncertainty distribution
3. For each sample: simulate system and evaluate stability
4. Collect metrics for successful trials
5. Robustness analysis: success rate, worst-case, percentiles
6. Generate report with uncertainty quantification

**Sampling Methods:**
- **Uniform**: $\\theta \\sim \\mathcal{U}(\\theta_0(1-\\epsilon), \\theta_0(1+\\epsilon))$
- **Gaussian**: $\\theta \\sim \\mathcal{N}(\\theta_0, \\sigma^2)$
- **Latin Hypercube**: Stratified sampling for high dimensions
"""

    def _validation_metrics_diagram(self) -> str:
        """Generate validation metrics computation diagram."""
        return """## Architecture Diagram

```{mermaid}
graph TD
    A[Simulation Result] --> B[Time Vector t]
    A --> C[State Trajectory x_t_]
    A --> D[Control Signal u_t_]

    B --> E[Control Metrics]
    C --> E
    E --> F[ISE]
    E --> G[ITAE]
    E --> H[RMS Control]

    B --> I[Stability Metrics]
    C --> I
    I --> J[Overshoot]
    I --> K[Settling Time]
    I --> L[Damping Ratio]

    B --> M[Constraint Metrics]
    D --> M
    M --> N[Saturation Severity]
    M --> O[Violation Count]
    M --> P[Peak Violation]

    F --> Q[Metrics Dictionary]
    G --> Q
    H --> Q
    J --> Q
    K --> Q
    L --> Q
    N --> Q
    O --> Q
    P --> Q
    Q --> R[Validation Result]

    style E fill:#9cf
    style I fill:#fcf
    style M fill:#ff9
    style Q fill:#f9f
    style R fill:#9f9
```

**Data Flow:**
1. Extract time, state, control trajectories from simulation
2. **Control Metrics Module**: Compute ISE, ITAE, RMS
3. **Stability Metrics Module**: Compute overshoot, settling time, damping
4. **Constraint Metrics Module**: Compute violations and severity
5. Aggregate all metrics into unified dictionary
6. Return structured validation result

**Metric Categories:**
- **Control Performance**: Tracking accuracy, convergence speed
- **Stability Analysis**: Overshoot, damping characteristics
- **Constraint Satisfaction**: Actuator limits, physical constraints
"""

    def _validation_benchmark_diagram(self) -> str:
        """Generate validation benchmark comparison diagram."""
        return """## Architecture Diagram

```{mermaid}
graph TD
    A[Controller A] --> B[Benchmark Suite]
    C[Controller B] --> B
    D[Controller C] --> B

    B --> E[Scenario 1: Stabilization]
    B --> F[Scenario 2: Tracking]
    B --> G[Scenario 3: Disturbance Rejection]

    E --> H[Multi-Trial Runner]
    F --> H
    G --> H

    H --> I{For Each Controller×Scenario}
    I --> J[N Trials]
    J --> K[Metrics Collection]
    K --> L{All Combinations Done?}
    L -->|No| I
    L -->|Yes| M[Comparative Analysis]

    M --> N[ANOVA]
    M --> O[Pairwise t-tests]
    M --> P[Pareto Ranking]

    N --> Q[Statistical Report]
    O --> Q
    P --> Q
    Q --> R[Benchmark Comparison]

    style B fill:#9cf
    style H fill:#fcf
    style M fill:#ff9
    style Q fill:#f9f
    style R fill:#9f9
```

**Data Flow:**
1. Define benchmark suite with multiple scenarios
2. For each controller × scenario combination:
   - Execute $N$ trials with different seeds
   - Collect performance metrics
3. Comparative statistical analysis:
   - **ANOVA**: Test for significant differences
   - **Pairwise tests**: Identify specific differences (with Bonferroni correction)
   - **Pareto ranking**: Multi-objective dominance analysis
4. Generate comprehensive benchmark report

**Output Includes:**
- Statistical significance (p-values)
- Effect sizes (Cohen's d)
- Confidence intervals (95%)
- Performance rankings with uncertainty
"""

    # ===================================================================================
    # Benchmarking Framework Example Methods (Week 7 Phase 1)
    # ===================================================================================

    def _statistical_benchmarks_examples(self, high_priority: bool) -> str:
        """Generate statistical benchmarking usage examples."""
        return """## Usage Examples

### Basic Statistical Benchmarking

```python
from src.benchmarks.statistical_benchmarks_v2 import run_trials
from src.controllers.factory import create_smc_for_pso, SMCType

# Define controller factory
def controller_factory():
    return create_smc_for_pso(
        SMCType.CLASSICAL,
        gains=[10, 8, 15, 12, 50, 5],
        max_force=100.0
    )

# Configure benchmarking
from src.config import load_config
config = load_config("config.yaml")

# Run trials with statistical analysis
metrics_list, ci_results = run_trials(
    controller_factory,
    config,
    n_trials=30,
    confidence_level=0.95
)

# Access results
print(f"Mean ISE: {ci_results['ise']['mean']:.4f}")
print(f"95% CI: [{ci_results['ise']['ci_lower']:.4f}, {ci_results['ise']['ci_upper']:.4f}]")
print(f"Std Dev: {ci_results['ise']['std']:.4f}")
```

### Advanced: Bootstrap Confidence Intervals

```python
from src.benchmarks.statistical_benchmarks_v2 import run_trials_with_advanced_statistics

# Run with bootstrap CI (non-parametric, no normality assumption)
metrics_list, analysis = run_trials_with_advanced_statistics(
    controller_factory,
    config,
    n_trials=50,
    confidence_level=0.99,
    use_bootstrap=True,
    n_bootstrap=10000
)

# Bootstrap results more robust for non-normal distributions
print(f"Bootstrap 99% CI for settling time:")
print(f"  [{analysis['settling_time']['bootstrap_ci'][0]:.3f}, "
      f"{analysis['settling_time']['bootstrap_ci'][1]:.3f}]")
```

### Controller Comparison with Hypothesis Testing

```python
from src.benchmarks.statistical_benchmarks_v2 import compare_controllers

# Define two controllers
def classical_factory():
    return create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5])

def adaptive_factory():
    return create_smc_for_pso(SMCType.ADAPTIVE, [10, 8, 15, 12, 0.5])

# Statistical comparison
comparison = compare_controllers(
    controller_a_factory=classical_factory,
    controller_b_factory=adaptive_factory,
    config=config,
    n_trials=40
)

# Interpret results
for metric, result in comparison.items():
    print(f"\n{metric.upper()}:")
    print(f"  Classical: {result['mean_a']:.4f} ± {result['std_a']:.4f}")
    print(f"  Adaptive:  {result['mean_b']:.4f} ± {result['std_b']:.4f}")
    print(f"  p-value:   {result['p_value']:.4e}")

    if result['p_value'] < 0.05:
        better = 'Classical' if result['mean_a'] < result['mean_b'] else 'Adaptive'
        print(f"  → {better} is significantly better (p < 0.05)")
    else:
        print(f"  → No significant difference (p ≥ 0.05)")
```

### Batch Benchmarking Multiple Controllers

```python
from src.benchmarks.core import run_multiple_trials

controllers = {
    'Classical': lambda: create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5]),
    'Adaptive': lambda: create_smc_for_pso(SMCType.ADAPTIVE, [10, 8, 15, 12, 0.5]),
    'STA': lambda: create_smc_for_pso(SMCType.SUPER_TWISTING, [25, 10, 15, 12, 20, 15]),
    'Hybrid': lambda: create_smc_for_pso(SMCType.HYBRID, [15, 12, 18, 15])
}

results = {}
for name, factory in controllers.items():
    metrics_list, ci_results = run_trials(factory, config, n_trials=30)
    results[name] = ci_results

# Compare ISE across all controllers
import pandas as pd
comparison_df = pd.DataFrame({
    name: {
        'ISE': r['ise']['mean'],
        'ISE_CI': f"[{r['ise']['ci_lower']:.3f}, {r['ise']['ci_upper']:.3f}]",
        'Settling Time': r['settling_time']['mean']
    }
    for name, r in results.items()
}).T

print(comparison_df)
```

**See:** {doc}`../../../benchmarking_workflows/statistical_analysis_guide`
"""

    def _monte_carlo_examples(self, high_priority: bool) -> str:
        """Generate Monte Carlo validation usage examples."""
        return """## Usage Examples

### Basic Monte Carlo Robustness Analysis

```python
from src.analysis.validation.monte_carlo import MonteCarloValidator
from src.controllers.factory import create_smc_for_pso, SMCType

# Define controller
controller_factory = lambda: create_smc_for_pso(
    SMCType.CLASSICAL,
    gains=[10, 8, 15, 12, 50, 5],
    max_force=100.0
)

# Configure uncertainty model (±20% on masses, ±10% on lengths)
uncertainty = {
    'cart_mass': {'type': 'uniform', 'range': (-0.2, 0.2)},
    'pole1_mass': {'type': 'uniform', 'range': (-0.2, 0.2)},
    'pole2_mass': {'type': 'uniform', 'range': (-0.2, 0.2)},
    'pole1_length': {'type': 'uniform', 'range': (-0.1, 0.1)},
    'pole2_length': {'type': 'uniform', 'range': (-0.1, 0.1)},
}

# Run Monte Carlo validation
validator = MonteCarloValidator(
    controller_factory=controller_factory,
    uncertainty_model=uncertainty,
    n_samples=100,
    seed=42
)

results = validator.run()

# Analyze robustness
print(f"Success Rate: {results['success_rate']*100:.1f}%")
print(f"Mean ISE: {results['mean_ise']:.4f}")
print(f"Worst-case ISE: {results['worst_case_ise']:.4f}")
print(f"95th Percentile ISE: {results['percentile_95_ise']:.4f}")
```

### Gaussian Uncertainty with Correlation

```python
import numpy as np

# Define correlated uncertainties (masses tend to vary together)
mean_params = np.array([1.0, 0.1, 0.05])  # cart, pole1, pole2 masses
cov_matrix = np.array([
    [0.04, 0.01, 0.005],   # cart mass variance and covariances
    [0.01, 0.004, 0.002],  # pole1 mass
    [0.005, 0.002, 0.001]  # pole2 mass
])

# Gaussian Monte Carlo
validator = MonteCarloValidator(
    controller_factory=controller_factory,
    uncertainty_model={
        'masses': {
            'type': 'gaussian',
            'mean': mean_params,
            'cov': cov_matrix
        }
    },
    n_samples=200
)

results = validator.run()

# Visualize uncertainty propagation
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.subplot(1, 2, 1)
plt.hist(results['ise_samples'], bins=30, alpha=0.7, edgecolor='black')
plt.xlabel('ISE')
plt.ylabel('Frequency')
plt.title('Performance Distribution under Uncertainty')

plt.subplot(1, 2, 2)
plt.scatter(results['param_samples'][:, 0], results['ise_samples'], alpha=0.5)
plt.xlabel('Cart Mass Perturbation')
plt.ylabel('ISE')
plt.title('Sensitivity to Cart Mass')
plt.tight_layout()
plt.show()
```

### Latin Hypercube Sampling for High-Dimensional Uncertainty

```python
from src.analysis.validation.monte_carlo import LatinHypercubeSampler

# High-dimensional uncertainty (all 8 physics parameters)
uncertainty_full = {
    'cart_mass': (-0.2, 0.2),
    'pole1_mass': (-0.2, 0.2),
    'pole2_mass': (-0.2, 0.2),
    'pole1_length': (-0.1, 0.1),
    'pole2_length': (-0.1, 0.1),
    'friction_cart': (-0.3, 0.3),
    'friction_pole1': (-0.3, 0.3),
    'friction_pole2': (-0.3, 0.3),
}

# Latin Hypercube Sampling (more efficient than random for high dimensions)
sampler = LatinHypercubeSampler(uncertainty_full, n_samples=150, seed=42)
param_samples = sampler.generate()

# Run validation with LHS samples
validator = MonteCarloValidator(
    controller_factory=controller_factory,
    param_samples=param_samples  # Pre-generated samples
)

results = validator.run()

# Analyze which parameters drive failures
failure_params = param_samples[~results['stability_mask']]
print(f"Failure modes analysis:")
print(f"  Cart mass range in failures: [{failure_params[:, 0].min():.3f}, {failure_params[:, 0].max():.3f}]")
print(f"  Pole1 length range in failures: [{failure_params[:, 3].min():.3f}, {failure_params[:, 3].max():.3f}]")
```

### Robustness Comparison Across Controllers

```python
controllers = {
    'Classical': lambda: create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5]),
    'Adaptive': lambda: create_smc_for_pso(SMCType.ADAPTIVE, [10, 8, 15, 12, 0.5]),
}

uncertainty = {
    'cart_mass': {'type': 'uniform', 'range': (-0.3, 0.3)},  # Aggressive uncertainty
    'pole1_mass': {'type': 'uniform', 'range': (-0.3, 0.3)},
}

robustness_results = {}
for name, factory in controllers.items():
    validator = MonteCarloValidator(factory, uncertainty, n_samples=200)
    robustness_results[name] = validator.run()

# Compare success rates
for name, res in robustness_results.items():
    print(f"{name}:")
    print(f"  Success Rate: {res['success_rate']*100:.1f}%")
    print(f"  Mean ISE (successful): {res['mean_ise']:.4f}")
    print(f"  Worst-case ISE: {res['worst_case_ise']:.4f}")
```

**See:** {doc}`../../../validation/monte_carlo_robustness_guide`
"""

    def _validation_metrics_examples(self, high_priority: bool) -> str:
        """Generate validation metrics usage examples."""
        return """## Usage Examples

### Compute All Metrics for a Simulation

```python
from src.benchmarks.metrics import compute_all_metrics
from src.simulation.engines.simulation_runner import run_simulation
from src.controllers.factory import create_smc_for_pso, SMCType
from src.plant.models.simplified import SimplifiedDynamics

# Run simulation
controller = create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5])
dynamics = SimplifiedDynamics()

result = run_simulation(
    controller=controller,
    dynamics_model=dynamics,
    initial_state=[0.1, 0.05, 0, 0, 0, 0],
    sim_time=10.0,
    dt=0.01
)

# Compute comprehensive metrics
metrics = compute_all_metrics(
    t=result.time,
    x=result.states,
    u=result.control,
    max_force=100.0,
    include_advanced=True
)

# Access metrics
print("Control Performance:")
print(f"  ISE: {metrics['ise']:.4f}")
print(f"  ITAE: {metrics['itae']:.4f}")
print(f"  RMS Control: {metrics['rms_control']:.4f}")

print("\nStability Analysis:")
print(f"  Settling Time: {metrics['settling_time']:.3f}s")
print(f"  Overshoot: {metrics['overshoot']:.2f}%")
print(f"  Damping Ratio: {metrics['damping_ratio']:.3f}")

print("\nConstraint Violations:")
print(f"  Saturation Count: {metrics['saturation_count']}")
print(f"  Saturation Severity: {metrics['saturation_severity']:.4f}")
```

### Individual Metric Computation

```python
from src.benchmarks.metrics.control_metrics import compute_ise, compute_itae
from src.benchmarks.metrics.stability_metrics import compute_overshoot, compute_settling_time
from src.benchmarks.metrics.constraint_metrics import count_control_violations

# Control metrics
ise = compute_ise(result.time, result.states)
itae = compute_itae(result.time, result.states)

# Stability metrics
overshoot = compute_overshoot(result.states[:, 0])  # First angle
settling_time = compute_settling_time(result.time, result.states, threshold=0.02)

# Constraint violations
violations, severity, peak = count_control_violations(result.control, max_force=100.0)

print(f"ISE: {ise:.4f}, ITAE: {itae:.4f}")
print(f"Overshoot: {overshoot:.2f}%, Settling: {settling_time:.3f}s")
print(f"Violations: {violations}, Severity: {severity:.4f}, Peak: {peak:.2f}")
```

### Custom Metric: Chattering Index

```python
import numpy as np

def compute_chattering_index(u, dt):
    \"\"\"Quantify control chattering.\"\"\"
    # Total variation of control signal
    tv = np.sum(np.abs(np.diff(u))) * dt
    return tv

chattering = compute_chattering_index(result.control, dt=0.01)
print(f"Chattering Index: {chattering:.4f}")

# Compare chattering across controllers
controllers = {
    'Classical': create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5]),
    'STA': create_smc_for_pso(SMCType.SUPER_TWISTING, [25, 10, 15, 12, 20, 15]),
}

chattering_results = {}
for name, ctrl in controllers.items():
    result = run_simulation(ctrl, dynamics, [0.1, 0.05, 0, 0, 0, 0], 10.0, 0.01)
    chattering_results[name] = compute_chattering_index(result.control, 0.01)

for name, ci in chattering_results.items():
    print(f"{name} Chattering: {ci:.4f}")
```

### Energy-Based Metrics

```python
# Track energy conservation (for unforced natural dynamics)
def compute_energy_drift(result, dynamics):
    \"\"\"Measure energy drift as validation check.\"\"\"
    energies = [dynamics.compute_total_energy(x) for x in result.states]
    initial_energy = energies[0]
    drift = np.abs(np.array(energies) - initial_energy) / initial_energy * 100
    max_drift = np.max(drift)
    mean_drift = np.mean(drift)
    return {'max_drift_%': max_drift, 'mean_drift_%': mean_drift}

energy_metrics = compute_energy_drift(result, dynamics)
print(f"Energy drift: {energy_metrics['max_drift_%']:.3f}% (max), "
      f"{energy_metrics['mean_drift_%']:.3f}% (mean)")
```

### Batch Metric Computation for Trials

```python
from src.benchmarks.metrics import compute_all_metrics

# Compute metrics for multiple trials
trials_results = []  # List of simulation results from multiple runs

metrics_collection = []
for result in trials_results:
    metrics = compute_all_metrics(result.time, result.states, result.control, 100.0)
    metrics_collection.append(metrics)

# Aggregate statistics
import pandas as pd
df = pd.DataFrame(metrics_collection)

print("Metric Statistics Across Trials:")
print(df[['ise', 'settling_time', 'overshoot', 'rms_control']].describe())
```

**See:** {doc}`../../../performance_metrics/metric_definitions`
"""

    def _validation_benchmark_examples(self, high_priority: bool) -> str:
        """Generate validation benchmark usage examples."""
        return """## Usage Examples

### Basic Multi-Controller Benchmark

```python
from src.analysis.validation.benchmarking import run_benchmark_suite
from src.controllers.factory import create_smc_for_pso, SMCType

# Define controllers to compare
controllers = {
    'Classical SMC': lambda: create_smc_for_pso(SMCType.CLASSICAL, [10, 8, 15, 12, 50, 5]),
    'Adaptive SMC': lambda: create_smc_for_pso(SMCType.ADAPTIVE, [10, 8, 15, 12, 0.5]),
    'Super-Twisting': lambda: create_smc_for_pso(SMCType.SUPER_TWISTING, [25, 10, 15, 12, 20, 15]),
    'Hybrid': lambda: create_smc_for_pso(SMCType.HYBRID, [15, 12, 18, 15])
}

# Run benchmark
results = run_benchmark_suite(
    controllers=controllers,
    n_trials=30,
    scenarios=['stabilization', 'tracking', 'disturbance']
)

# Generate comparison report
results.generate_report('benchmark_report.pdf')

# Access statistical comparison
print(results.summary_table())
```

### ANOVA for Multi-Controller Comparison

```python
from src.analysis.validation.statistical_tests import run_anova

# Extract ISE values for each controller
ise_data = {
    name: [trial['ise'] for trial in results[name]]
    for name in controllers.keys()
}

# Run ANOVA
anova_result = run_anova(ise_data)

print(f"ANOVA F-statistic: {anova_result['F']:.3f}")
print(f"p-value: {anova_result['p_value']:.4e}")

if anova_result['p_value'] < 0.05:
    print("Significant differences detected between controllers (p < 0.05)")

    # Post-hoc pairwise comparisons
    from src.analysis.validation.statistical_tests import pairwise_t_tests

    pairwise_results = pairwise_t_tests(
        ise_data,
        correction='bonferroni'  # Conservative correction for multiple comparisons
    )

    print("\nPairwise Comparisons (Bonferroni corrected):")
    for (ctrl_a, ctrl_b), p_val in pairwise_results.items():
        sig = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else "ns"
        print(f"  {ctrl_a} vs {ctrl_b}: p = {p_val:.4e} {sig}")
```

### Pareto Ranking for Multi-Objective Comparison

```python
from src.analysis.validation.benchmarking import compute_pareto_ranking

# Define multiple objectives (minimize all)
objectives = ['ise', 'settling_time', 'rms_control', 'chattering_index']

# Compute Pareto frontier
pareto_ranking = compute_pareto_ranking(
    results,
    objectives=objectives,
    direction='minimize'  # All objectives to be minimized
)

print("Pareto Ranking (Tier 1 = Non-dominated):")
for tier, controllers in pareto_ranking.items():
    print(f"  Tier {tier}: {', '.join(controllers)}")

# Visualize Pareto frontier (2D projection)
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
for name, data in results.items():
    ise_mean = np.mean([t['ise'] for t in data])
    settling_mean = np.mean([t['settling_time'] for t in data])

    tier = [k for k, v in pareto_ranking.items() if name in v][0]
    marker = 'o' if tier == 1 else 's'

    plt.scatter(ise_mean, settling_mean, label=name, marker=marker, s=100)

plt.xlabel('Mean ISE')
plt.ylabel('Mean Settling Time (s)')
plt.title('Pareto Frontier: ISE vs Settling Time')
plt.legend()
plt.grid(True)
plt.show()
```

### Scenario-Based Benchmarking

```python
from src.analysis.validation.benchmarking import BenchmarkScenario

# Define custom scenarios
scenarios = [
    BenchmarkScenario(
        name='Stabilization',
        initial_state=[0.1, 0.05, 0, 0, 0, 0],
        duration=10.0,
        performance_weights={'ise': 0.6, 'settling_time': 0.4}
    ),
    BenchmarkScenario(
        name='Large Disturbance',
        initial_state=[0.3, 0.2, 0, 0, 0, 0],
        duration=15.0,
        performance_weights={'ise': 0.4, 'overshoot': 0.3, 'settling_time': 0.3}
    ),
    BenchmarkScenario(
        name='Tracking',
        reference_trajectory='sinusoidal',
        duration=20.0,
        performance_weights={'tracking_error': 0.8, 'control_effort': 0.2}
    )
]

# Run scenario-based benchmark
scenario_results = {}
for scenario in scenarios:
    scenario_results[scenario.name] = run_benchmark_suite(
        controllers=controllers,
        scenario=scenario,
        n_trials=25
    )

# Aggregate scenario performance
for scenario_name, res in scenario_results.items():
    print(f"\n{scenario_name} Results:")
    for ctrl_name, data in res.items():
        weighted_score = scenario.compute_weighted_score(data)
        print(f"  {ctrl_name}: Score = {weighted_score:.4f}")
```

### Reproducibility and Archival

```python
from src.analysis.validation.benchmarking import BenchmarkArchive

# Archive full benchmark configuration and results
archive = BenchmarkArchive('benchmark_20251004.h5')

archive.save(
    controllers=controllers,
    results=results,
    config={'n_trials': 30, 'dt': 0.01, 'duration': 10.0},
    metadata={
        'date': '2025-10-04',
        'author': 'Researcher',
        'purpose': 'Controller comparison for publication',
        'software_versions': {
            'numpy': np.__version__,
            'scipy': scipy.__version__
        }
    }
)

# Later: Reload exact benchmark
archive_loaded = BenchmarkArchive('benchmark_20251004.h5')
results_reloaded = archive_loaded.load()

# Verify reproducibility
assert np.allclose(
    results['Classical SMC'][0]['ise'],
    results_reloaded['Classical SMC'][0]['ise']
)
```

**See:** {doc}`../../../benchmarking/reproducible_validation_workflow`
"""

    def _insert_explanation(self, content: str, explanation: Dict) -> str:
        """Insert line-by-line explanation after method source code."""
        method_name = explanation['method_name']

        # Find the method's source code block - look for the literalinclude directive
        # Pattern: finds the opening ```, then the literalinclude block, then the closing ```
        pattern = rf'```{{literalinclude}}[^\`]+:pyobject:[^\`]*{method_name}[^\`]+```'
        match = re.search(pattern, content)

        if match:
            # Insert after the closing ``` of the source code block
            insert_pos = match.end()
            # Check if explanation already exists
            check_str = content[insert_pos:insert_pos+200]
            if "Line-by-Line Explanation" in check_str:
                return content  # Already has explanation
            return content[:insert_pos] + f"\n\n{explanation['explanation']}\n" + content[insert_pos:]

        return content

    def _add_architecture_diagrams(self, content: str, source_path: Path, high_priority: bool) -> str:
        """Add Mermaid architecture diagrams."""
        if not high_priority:
            return content

        diagram = self._generate_diagram(source_path)
        if not diagram:
            return content

        # Insert after Module Overview
        overview_match = re.search(r'(##\s+Module Overview.*?)(\n##\s+)', content, re.DOTALL)
        if overview_match:
            enhanced = content[:overview_match.end(1)] + f"\n\n{diagram}\n" + content[overview_match.end(1):]
            return enhanced

        return content

    def _generate_diagram(self, source_path: Path) -> str:
        """Generate Mermaid diagram based on file type."""
        path_str = str(source_path).lower()

        if 'classical' in path_str and 'controller' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[State Input] --> B[Sliding Surface]
    B --> C{Surface Value s}
    C --> D[Equivalent Control]
    C --> E[Switching Control]
    A --> D
    D --> F[Control Combiner]
    E --> F
    F --> G[Saturation]
    G --> H[Control Output u]

    style C fill:#ff9
    style F fill:#9f9
    style G fill:#f99
```

**Data Flow:**
1. State → Sliding Surface Computation
2. Surface Value → Equivalent & Switching Control
3. Control Components → Combination & Saturation
4. Final Control → Actuator
"""

        elif 'adaptive' in path_str and 'controller' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[State Input] --> B[Sliding Surface]
    B --> C{Surface Value s}
    C --> D[Adaptation Law]
    D --> E[Adaptive Gain K_t_]
    C --> F[Switching Control]
    E --> F
    F --> G[Saturation]
    G --> H[Control Output u]
    C --> I[Uncertainty Estimator]
    I --> D

    style C fill:#ff9
    style D fill:#9cf
    style E fill:#f9f
    style G fill:#f99
```

**Data Flow:**
1. State → Sliding Surface Computation
2. Surface Value → Adaptation Law + Switching Control
3. Online Gain Adaptation: K̇ = γ|s| - σ(K - K₀)
4. Adaptive Switching → Saturation → Control Output
"""

        elif ('super_twisting' in path_str or 'sta' in path_str) and 'controller' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[State Input] --> B[Sliding Surface]
    B --> C{Surface Value s}
    C --> D[Proportional Term]
    C --> E[Integral Term]
    D --> F["u₁ = -K₁|s|^1/2_sign_s_"]
    E --> G["u̇₂ = -K₂sign_s_"]
    G --> H[Integrator]
    H --> I[u₂]
    F --> J[Control Combiner]
    I --> J
    J --> K[Saturation]
    K --> L[Control Output u]

    style C fill:#ff9
    style D fill:#9cf
    style E fill:#fcf
    style J fill:#9f9
    style K fill:#f99
```

**Data Flow:**
1. State → Sliding Surface Computation
2. Surface → Proportional Term (fractional power)
3. Surface → Integral Term (continuous integration)
4. Continuous Control → Chattering-Free Output
"""

        elif 'hybrid' in path_str and 'controller' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[State Input] --> B[Sliding Surface]
    B --> C{Surface Value s}
    A --> D[Model Confidence]
    D --> E{Switching Logic}
    C --> F[Equivalent Control]
    C --> G[Super-Twisting Control]
    E -->|High Confidence| F
    E -->|Low Confidence| G
    F --> H[Transition Filter]
    G --> H
    H --> I[Saturation]
    I --> J[Control Output u]
    C --> K[Performance Monitor]
    K --> E

    style C fill:#ff9
    style E fill:#f9f
    style F fill:#9cf
    style G fill:#fcf
    style H fill:#cfc
    style I fill:#f99
```

**Data Flow:**
1. State → Sliding Surface + Model Confidence
2. Performance Monitoring → Mode Switching Decision
3. High Confidence → Equivalent Control (model-based)
4. Low Confidence → Super-Twisting (robust)
5. Smooth Transition → Final Control Output
"""

        elif 'pso' in path_str and 'optim' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[Parameter Bounds] --> B[Initialize Swarm]
    B --> C[Particle Population]
    C --> D{For Each Particle}
    D --> E[Create Controller]
    E --> F[Run Simulation]
    F --> G[Compute Cost]
    G --> H[Update Personal Best]
    H --> I{All Particles Done?}
    I -->|No| D
    I -->|Yes| J[Update Global Best]
    J --> K{Convergence?}
    K -->|No| L[Update Velocities]
    L --> M[Update Positions]
    M --> D
    K -->|Yes| N[Return Optimal Gains]

    style C fill:#9cf
    style G fill:#ff9
    style J fill:#f9f
    style N fill:#9f9
```

**Data Flow:**
1. Initialize swarm in parameter space
2. Evaluate fitness via closed-loop simulation
3. Update particle velocities: v = wv + c₁(p-x) + c₂(g-x)
4. Converge to optimal controller gains
5. Return best solution with performance metrics
"""

        elif 'simulation' in path_str or 'runner' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[Initial State x_0_] --> B[Simulation Loop]
    B --> C[Controller Compute]
    C --> D[Control Signal u]
    D --> E[Dynamics Model]
    E --> F[State Derivative ẋ]
    F --> G{Integration Method}
    G -->|Euler| H[x_k+1_ = x_k_ + Δt·ẋ]
    G -->|RK4| I[4-stage Runge-Kutta]
    G -->|RK45| J[Adaptive Step Size]
    H --> K[Next State]
    I --> K
    J --> K
    K --> L{Time < T_max_?}
    L -->|Yes| B
    L -->|No| M[Simulation Result]

    style C fill:#9cf
    style E fill:#fcf
    style G fill:#ff9
    style M fill:#9f9
```

**Data Flow:**
1. Initialize state and time
2. Compute control action from controller
3. Evaluate system dynamics: ẋ = f(x, u, t)
4. Integrate using numerical method (Euler/RK4/RK45)
5. Update state and repeat until termination
"""

        elif 'dynamics' in path_str and 'model' in path_str:
            return """## Architecture Diagram

```{mermaid}
graph TD
    A[State x, Control u] --> B[Mass Matrix M_q_]
    A --> C[Coriolis Matrix C_q,q̇_]
    A --> D[Gravity Vector G_q_]
    A --> E[Input Matrix B]
    B --> F[Invert M]
    C --> G[Compute Forces]
    D --> G
    E --> G
    F --> H[M^-1_]
    G --> I[Total Force F]
    H --> J[Solve: ẍ = M^-1__F-Cq̇-G_+Bu_]
    I --> J
    J --> K[State Derivative ẋ]
    K --> L[Return: θ̈_1_, θ̈_2_, ẍ]

    style B fill:#9cf
    style C fill:#fcf
    style D fill:#ff9
    style J fill:#f9f
    style L fill:#9f9
```

**Data Flow:**
1. Extract generalized coordinates q = [x, θ₁, θ₂]
2. Compute configuration-dependent matrices M, C, G
3. Apply control input u via input matrix B
4. Solve second-order dynamics: Mq̈ + Cq̇ + G = Bu
5. Return accelerations [ẍ, θ̈₁, θ̈₂] for integration
"""

        elif 'statistical_benchmarks' in path_str:
            return self._statistical_benchmarks_diagram()
        elif 'monte_carlo' in path_str:
            return self._monte_carlo_diagram()
        elif 'validation' in path_str and 'metrics' in path_str:
            return self._validation_metrics_diagram()
        elif 'validation' in path_str and 'benchmark' in path_str:
            return self._validation_benchmark_diagram()

        return ""

    def _add_performance_notes(self, content: str, source_path: Path) -> str:
        """Add performance analysis section."""
        # Check if already has performance section
        if re.search(r'##\s+Performance', content):
            return content

        perf = self._generate_performance_notes(source_path)
        if not perf:
            return content

        # Insert before Dependencies
        deps_match = re.search(r'(\n---\n##\s+Dependencies)', content)
        if deps_match:
            enhanced = content[:deps_match.start()] + f"\n\n{perf}\n" + content[deps_match.start():]
            return enhanced

        return content

    def _generate_performance_notes(self, source_path: Path) -> str:
        """Generate performance notes based on file type."""
        path_str = str(source_path).lower()

        if 'controller' in path_str:
            return """## Performance Characteristics

### Computational Complexity
- **Time Complexity**: O(1) per control step
- **Space Complexity**: O(1) for state storage
- **Typical Execution Time**: 0.05-0.1ms per iteration

### Real-Time Suitability
- ✅ Suitable for control loops up to 1kHz
- ✅ Deterministic execution time
- ✅ No dynamic memory allocation in control loop

### Optimization Tips
- Pre-compute constant matrices in initialization
- Use Numba JIT compilation for batch simulations
- Vectorize for parallel parameter sweeps

**Benchmark Results:**
- Single simulation (5s): ~500ms
- Batch simulation (100 runs): ~2s (with Numba)
- PSO optimization (30 particles, 50 iterations): ~3-5 minutes
"""

        return ""

    def enhance_module(self, module_path: Path, config: EnhancementConfig):
        """Enhance all files in a module."""
        doc_files = list(module_path.rglob('*.md'))
        doc_files = [f for f in doc_files if f.name != 'index.md']

        print(f"\nEnhancing module: {module_path.name} ({len(doc_files)} files)")

        for doc_file in doc_files:
            self.enhance_file(doc_file, config)

    def print_stats(self):
        """Print enhancement statistics."""
        print(f"\n{'='*80}")
        print("ENHANCEMENT SUMMARY")
        print(f"{'='*80}")
        print(f"[SUCCESS] Enhanced: {self.stats['enhanced']}")
        print(f"[WARNING] Skipped: {self.stats['skipped']}")
        print(f"[ERROR] Errors: {self.stats['errors']}")
        print(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(description='Enhance API documentation')
    parser.add_argument('--file', type=str, help='Single file to enhance')
    parser.add_argument('--module', type=str, help='Module to enhance (e.g., controllers)')
    parser.add_argument('--all', action='store_true', help='Enhance all files')
    parser.add_argument('--priority', choices=['high', 'medium', 'low'], default='medium',
                        help='Enhancement priority level')
    parser.add_argument('--dry-run', action='store_true', help='Preview without writing')

    args = parser.parse_args()

    # Paths
    project_root = Path(__file__).parent.parent.parent
    docs_root = project_root / 'docs' / 'reference'
    src_root = project_root / 'src'

    # Configuration
    config = EnhancementConfig(priority=args.priority)
    enhancer = APIDocEnhancer(docs_root, src_root, dry_run=args.dry_run)

    # Execute enhancement
    if args.file:
        file_path = project_root / args.file
        enhancer.enhance_file(file_path, config)
    elif args.module:
        module_path = docs_root / args.module
        enhancer.enhance_module(module_path, config)
    elif args.all:
        for module_dir in docs_root.iterdir():
            if module_dir.is_dir():
                enhancer.enhance_module(module_dir, config)
    else:
        print("Please specify --file, --module, or --all")
        return

    enhancer.print_stats()


if __name__ == '__main__':
    main()
