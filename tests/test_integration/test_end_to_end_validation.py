#======================================================================================\\\
#================ tests/test_integration/test_end_to_end_validation.py ================\\\
#======================================================================================\\\

"""
End-to-End Workflow Validation - Mission 10 Production Readiness

MISSION-CRITICAL CAPABILITY: Validate the complete system workflow from CLI entry
points through simulation execution to output generation. This ensures that all
system components work together seamlessly in real production scenarios.

WORKFLOW VALIDATION HIERARCHY:
1. CLI Accessibility & Help System
2. Configuration System Integration
3. Simulation Execution Pipeline
4. Output Generation & Analysis
5. Error Handling & Graceful Degradation

SUCCESS CRITERIA - MISSION 10:
- 95%+ end-to-end workflow success rate
- All critical CLI commands functional
- Configuration loading and validation working
- Simulation pipeline executing successfully
- Production-grade error handling validated
"""

import pytest
import subprocess
import sys
import os
import tempfile
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import time
from datetime import datetime


@dataclass
class WorkflowTestResult:
    """Result of an end-to-end workflow test."""
    workflow_name: str
    success: bool
    execution_time: float
    steps_completed: List[str]
    error_messages: List[str]
    output_artifacts: List[str]
    performance_metrics: Dict[str, Any]


@dataclass
class SystemValidationResult:
    """Complete system validation result."""
    overall_success_rate: float
    workflow_results: List[WorkflowTestResult]
    production_ready: bool
    critical_failures: List[str]
    recommendations: List[str]


class EndToEndWorkflowValidator:
    """Validates end-to-end system workflows for production readiness."""

    def __init__(self):
        """Initialize end-to-end workflow validator."""
        self.repo_root = Path(__file__).parent.parent.parent
        self.temp_dir = Path(tempfile.mkdtemp())
        self.temp_dir.mkdir(exist_ok=True)

        # Key system files to validate
        self.simulate_py = self.repo_root / "simulate.py"
        self.streamlit_app = self.repo_root / "streamlit_app.py"
        self.config_yaml = self.repo_root / "config.yaml"

        self.workflow_results: List[WorkflowTestResult] = []

    def validate_cli_accessibility(self) -> WorkflowTestResult:
        """Validate CLI entry point accessibility and help system."""

        start_time = time.perf_counter()
        steps_completed = []
        error_messages = []
        output_artifacts = []
        performance_metrics = {}

        try:
            # Step 1: Check if simulate.py exists
            if self.simulate_py.exists():
                steps_completed.append("CLI Entry Point Located")

                # Step 2: Test CLI help system
                try:
                    result = subprocess.run([
                        sys.executable, str(self.simulate_py), "--help"
                    ], capture_output=True, text=True, timeout=30, cwd=str(self.repo_root))

                    if result.returncode == 0:
                        steps_completed.append("CLI Help System Functional")
                        performance_metrics['help_response_time'] = time.perf_counter() - start_time
                        performance_metrics['help_output_length'] = len(result.stdout)

                        # Check for key help content
                        help_output = result.stdout.lower()
                        if '--ctrl' in help_output or '--controller' in help_output:
                            steps_completed.append("Controller Options Available")
                        if '--plot' in help_output:
                            steps_completed.append("Visualization Options Available")
                        if '--config' in help_output:
                            steps_completed.append("Configuration Options Available")

                    else:
                        error_messages.append(f"CLI help failed: {result.stderr}")

                except subprocess.TimeoutExpired:
                    error_messages.append("CLI help command timed out (>30s)")
                except Exception as e:
                    error_messages.append(f"CLI help test failed: {str(e)}")

                # Step 3: Test configuration display (if supported)
                try:
                    result = subprocess.run([
                        sys.executable, str(self.simulate_py), "--print-config"
                    ], capture_output=True, text=True, timeout=15, cwd=str(self.repo_root))

                    if result.returncode == 0:
                        steps_completed.append("Configuration Display Functional")
                        performance_metrics['config_display_time'] = time.perf_counter() - start_time

                except Exception:
                    # This is optional functionality
                    pass

            else:
                error_messages.append("CLI entry point (simulate.py) not found")

        except Exception as e:
            error_messages.append(f"CLI accessibility validation failed: {str(e)}")

        execution_time = time.perf_counter() - start_time

        return WorkflowTestResult(
            workflow_name="CLI Accessibility",
            success=len(error_messages) == 0,
            execution_time=execution_time,
            steps_completed=steps_completed,
            error_messages=error_messages,
            output_artifacts=output_artifacts,
            performance_metrics=performance_metrics
        )

    def validate_configuration_system(self) -> WorkflowTestResult:
        """Validate configuration system integration."""

        start_time = time.perf_counter()
        steps_completed = []
        error_messages = []
        output_artifacts = []
        performance_metrics = {}

        try:
            # Step 1: Check if config.yaml exists
            if self.config_yaml.exists():
                steps_completed.append("Main Configuration File Located")

                # Step 2: Test configuration loading
                try:
                    with open(self.config_yaml, 'r') as f:
                        config_data = yaml.safe_load(f)

                    if config_data:
                        steps_completed.append("Configuration File Parseable")
                        performance_metrics['config_keys_count'] = len(config_data.keys())

                        # Check for key configuration sections
                        key_sections = ['physics_params', 'simulation_params', 'controllers', 'optimization']
                        found_sections = [section for section in key_sections if section in config_data]

                        if found_sections:
                            steps_completed.append(f"Key Config Sections Found ({len(found_sections)}/{len(key_sections)})")
                            performance_metrics['config_sections_found'] = len(found_sections)

                        # Test configuration validation (if available)
                        try:
                            # Try to validate with simulate.py if it has validation
                            result = subprocess.run([
                                sys.executable, "-c",
                                f"import yaml; config = yaml.safe_load(open('{self.config_yaml}')); print('Config loaded successfully')"
                            ], capture_output=True, text=True, timeout=10, cwd=str(self.repo_root))

                            if result.returncode == 0:
                                steps_completed.append("Configuration Validation Successful")

                        except Exception as e:
                            error_messages.append(f"Configuration validation failed: {str(e)}")
                    else:
                        error_messages.append("Configuration file is empty or invalid")

                except yaml.YAMLError as e:
                    error_messages.append(f"Configuration file parsing failed: {str(e)}")
                except Exception as e:
                    error_messages.append(f"Configuration loading failed: {str(e)}")

            else:
                error_messages.append("Main configuration file (config.yaml) not found")

            # Step 3: Check for additional configuration files
            config_dir = self.repo_root / "config"
            if config_dir.exists():
                config_files = list(config_dir.glob("*.yaml")) + list(config_dir.glob("*.yml"))
                if config_files:
                    steps_completed.append(f"Additional Config Files Found ({len(config_files)})")
                    performance_metrics['additional_config_files'] = len(config_files)

        except Exception as e:
            error_messages.append(f"Configuration system validation failed: {str(e)}")

        execution_time = time.perf_counter() - start_time

        return WorkflowTestResult(
            workflow_name="Configuration System",
            success=len(error_messages) == 0,
            execution_time=execution_time,
            steps_completed=steps_completed,
            error_messages=error_messages,
            output_artifacts=output_artifacts,
            performance_metrics=performance_metrics
        )

    def validate_simulation_execution(self) -> WorkflowTestResult:
        """Validate simulation execution pipeline."""

        start_time = time.perf_counter()
        steps_completed = []
        error_messages = []
        output_artifacts = []
        performance_metrics = {}

        try:
            # Step 1: Test basic simulation execution (dry run)
            if self.simulate_py.exists():
                steps_completed.append("Simulation Entry Point Available")

                # Step 2: Test simulation with minimal parameters (if supported)
                try:
                    # Try to run a minimal simulation without plots or heavy computation
                    test_commands = [
                        # Test configuration validation only
                        [sys.executable, str(self.simulate_py), "--help"],
                        # Test with minimal parameters if available
                    ]

                    for cmd in test_commands:
                        try:
                            result = subprocess.run(
                                cmd, capture_output=True, text=True,
                                timeout=20, cwd=str(self.repo_root)
                            )

                            if result.returncode == 0:
                                steps_completed.append(f"Command Execution Successful: {' '.join(cmd[-2:])}")
                                performance_metrics[f'cmd_execution_time'] = time.perf_counter() - start_time
                            else:
                                # Don't treat this as error for help command
                                if "--help" not in cmd:
                                    error_messages.append(f"Command failed: {' '.join(cmd[-2:])} - {result.stderr[:200]}")

                        except subprocess.TimeoutExpired:
                            error_messages.append(f"Command timed out: {' '.join(cmd[-2:])}")
                        except Exception as e:
                            error_messages.append(f"Command execution failed: {str(e)}")

                except Exception as e:
                    error_messages.append(f"Simulation execution test failed: {str(e)}")

                # Step 3: Check for output directories
                potential_output_dirs = ["output", "results", "plots", "logs"]
                existing_output_dirs = [d for d in potential_output_dirs if (self.repo_root / d).exists()]

                if existing_output_dirs:
                    steps_completed.append(f"Output Directories Available ({len(existing_output_dirs)})")
                    performance_metrics['output_directories'] = len(existing_output_dirs)

            else:
                error_messages.append("Simulation entry point not available")

        except Exception as e:
            error_messages.append(f"Simulation execution validation failed: {str(e)}")

        execution_time = time.perf_counter() - start_time

        return WorkflowTestResult(
            workflow_name="Simulation Execution",
            success=len(error_messages) == 0,
            execution_time=execution_time,
            steps_completed=steps_completed,
            error_messages=error_messages,
            output_artifacts=output_artifacts,
            performance_metrics=performance_metrics
        )

    def validate_web_interface(self) -> WorkflowTestResult:
        """Validate Streamlit web interface accessibility."""

        start_time = time.perf_counter()
        steps_completed = []
        error_messages = []
        output_artifacts = []
        performance_metrics = {}

        try:
            # Step 1: Check if streamlit app exists
            if self.streamlit_app.exists():
                steps_completed.append("Streamlit App Located")

                # Step 2: Test basic Python import/syntax check
                try:
                    result = subprocess.run([
                        sys.executable, "-m", "py_compile", str(self.streamlit_app)
                    ], capture_output=True, text=True, timeout=15, cwd=str(self.repo_root))

                    if result.returncode == 0:
                        steps_completed.append("Streamlit App Syntax Valid")
                        performance_metrics['syntax_check_time'] = time.perf_counter() - start_time
                    else:
                        error_messages.append(f"Streamlit app syntax error: {result.stderr}")

                except subprocess.TimeoutExpired:
                    error_messages.append("Streamlit syntax check timed out")
                except Exception as e:
                    error_messages.append(f"Streamlit syntax check failed: {str(e)}")

                # Step 3: Check for Streamlit in requirements (if exists)
                requirements_file = self.repo_root / "requirements.txt"
                if requirements_file.exists():
                    try:
                        with open(requirements_file, 'r') as f:
                            requirements_content = f.read().lower()

                        if 'streamlit' in requirements_content:
                            steps_completed.append("Streamlit Dependency Listed")

                    except Exception as e:
                        error_messages.append(f"Requirements check failed: {str(e)}")

            else:
                error_messages.append("Streamlit app (streamlit_app.py) not found")

        except Exception as e:
            error_messages.append(f"Web interface validation failed: {str(e)}")

        execution_time = time.perf_counter() - start_time

        return WorkflowTestResult(
            workflow_name="Web Interface",
            success=len(error_messages) == 0,
            execution_time=execution_time,
            steps_completed=steps_completed,
            error_messages=error_messages,
            output_artifacts=output_artifacts,
            performance_metrics=performance_metrics
        )

    def validate_testing_infrastructure(self) -> WorkflowTestResult:
        """Validate testing infrastructure and test execution."""

        start_time = time.perf_counter()
        steps_completed = []
        error_messages = []
        output_artifacts = []
        performance_metrics = {}

        try:
            # Step 1: Check for test directories
            test_directories = ["tests", "test"]
            existing_test_dirs = [d for d in test_directories if (self.repo_root / d).exists()]

            if existing_test_dirs:
                steps_completed.append(f"Test Directories Found ({len(existing_test_dirs)})")

                # Step 2: Count test files
                test_files = []
                for test_dir in existing_test_dirs:
                    test_path = self.repo_root / test_dir
                    test_files.extend(list(test_path.glob("**/test_*.py")))

                if test_files:
                    steps_completed.append(f"Test Files Found ({len(test_files)})")
                    performance_metrics['test_files_count'] = len(test_files)

                    # Step 3: Test basic pytest availability
                    try:
                        result = subprocess.run([
                            sys.executable, "-m", "pytest", "--version"
                        ], capture_output=True, text=True, timeout=10)

                        if result.returncode == 0:
                            steps_completed.append("Pytest Framework Available")
                            performance_metrics['pytest_version'] = result.stdout.strip()

                        # Step 4: Test simple test discovery
                        try:
                            result = subprocess.run([
                                sys.executable, "-m", "pytest", "--collect-only", "-q"
                            ], capture_output=True, text=True, timeout=30, cwd=str(self.repo_root))

                            if result.returncode == 0:
                                steps_completed.append("Test Discovery Successful")
                                # Count discovered tests from output
                                lines = result.stdout.split('\n')
                                test_lines = [l for l in lines if 'test' in l.lower() and '::' in l]
                                if test_lines:
                                    performance_metrics['discovered_tests'] = len(test_lines)

                        except subprocess.TimeoutExpired:
                            error_messages.append("Test discovery timed out")
                        except Exception as e:
                            error_messages.append(f"Test discovery failed: {str(e)}")

                    except Exception as e:
                        error_messages.append(f"Pytest availability check failed: {str(e)}")

                else:
                    error_messages.append("No test files found in test directories")

            else:
                error_messages.append("No test directories found")

            # Step 5: Check for test runner script
            test_runner_scripts = ["run_tests.py", "test_runner.py"]
            existing_runners = [s for s in test_runner_scripts if (self.repo_root / s).exists()]

            if existing_runners:
                steps_completed.append(f"Test Runner Scripts Available ({len(existing_runners)})")

        except Exception as e:
            error_messages.append(f"Testing infrastructure validation failed: {str(e)}")

        execution_time = time.perf_counter() - start_time

        return WorkflowTestResult(
            workflow_name="Testing Infrastructure",
            success=len(error_messages) == 0,
            execution_time=execution_time,
            steps_completed=steps_completed,
            error_messages=error_messages,
            output_artifacts=output_artifacts,
            performance_metrics=performance_metrics
        )

    def run_comprehensive_validation(self) -> SystemValidationResult:
        """Run comprehensive end-to-end workflow validation."""

        print("Starting Comprehensive End-to-End Workflow Validation...")

        # Run all workflow validations
        validation_methods = [
            self.validate_cli_accessibility,
            self.validate_configuration_system,
            self.validate_simulation_execution,
            self.validate_web_interface,
            self.validate_testing_infrastructure
        ]

        workflow_results = []
        for method in validation_methods:
            method_name = method.__name__.replace('validate_', '').replace('_', ' ').title()
            print(f"  Validating: {method_name}")

            try:
                result = method()
                workflow_results.append(result)

                status = "PASS" if result.success else "FAIL"
                print(f"    {status} - Steps: {len(result.steps_completed)}/{len(result.steps_completed) + len(result.error_messages)}")

                if result.error_messages:
                    for error in result.error_messages[:2]:  # Show first 2 errors
                        print(f"      WARNING: {error}")

            except Exception as e:
                error_result = WorkflowTestResult(
                    workflow_name=method_name,
                    success=False,
                    execution_time=0.0,
                    steps_completed=[],
                    error_messages=[str(e)],
                    output_artifacts=[],
                    performance_metrics={}
                )
                workflow_results.append(error_result)
                print(f"    FAIL - Exception: {str(e)}")

        # Calculate overall success rate
        successful_workflows = [r for r in workflow_results if r.success]
        overall_success_rate = len(successful_workflows) / len(workflow_results) if workflow_results else 0.0

        # Determine production readiness
        critical_workflows = ["CLI Accessibility", "Configuration System", "Testing Infrastructure"]
        critical_success = [
            any(r.workflow_name == workflow and r.success for r in workflow_results)
            for workflow in critical_workflows
        ]
        production_ready = all(critical_success) and overall_success_rate >= 0.8

        # Identify critical failures
        critical_failures = []
        for workflow in critical_workflows:
            result = next((r for r in workflow_results if r.workflow_name == workflow), None)
            if result and not result.success:
                critical_failures.extend(result.error_messages)

        # Generate recommendations
        recommendations = []
        if overall_success_rate >= 0.95:
            recommendations.append("Excellent system health - ready for production deployment")
        elif overall_success_rate >= 0.8:
            recommendations.append("Good system health - address remaining issues before deployment")
            recommendations.append("Focus on failed workflow components")
        else:
            recommendations.append("Critical system health issues detected")
            recommendations.append("Comprehensive system review required before deployment")

        return SystemValidationResult(
            overall_success_rate=overall_success_rate,
            workflow_results=workflow_results,
            production_ready=production_ready,
            critical_failures=critical_failures,
            recommendations=recommendations
        )

    def generate_validation_report(self, validation_result: SystemValidationResult) -> str:
        """Generate comprehensive validation report."""

        report = ["=" * 80]
        report.append("END-TO-END WORKFLOW VALIDATION REPORT - MISSION 10")
        report.append("=" * 80)
        report.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # Executive Summary
        report.append("EXECUTIVE SUMMARY")
        report.append("-" * 20)
        success_icon = "[PASS]" if validation_result.overall_success_rate >= 0.95 else "[WARN]" if validation_result.overall_success_rate >= 0.8 else "[FAIL]"
        report.append(f"{success_icon} Overall Success Rate: {validation_result.overall_success_rate:.1%}")
        report.append(f"Production Ready: {'YES' if validation_result.production_ready else 'NO'}")
        report.append(f"Workflows Tested: {len(validation_result.workflow_results)}")
        report.append("")

        # Workflow Results
        report.append("WORKFLOW VALIDATION RESULTS")
        report.append("-" * 30)
        for result in validation_result.workflow_results:
            status_icon = "[PASS]" if result.success else "[FAIL]"
            report.append(f"{status_icon} {result.workflow_name}")
            report.append(f"   Steps Completed: {len(result.steps_completed)}")
            report.append(f"   Execution Time: {result.execution_time:.2f}s")

            if result.steps_completed:
                report.append(f"   Completed Steps:")
                for step in result.steps_completed:
                    report.append(f"     • {step}")

            if result.error_messages:
                report.append(f"   Issues Found:")
                for error in result.error_messages:
                    report.append(f"     • {error}")

            if result.performance_metrics:
                report.append(f"   Metrics: {len(result.performance_metrics)} performance indicators")
            report.append("")

        # Critical Failures
        if validation_result.critical_failures:
            report.append("CRITICAL FAILURES")
            report.append("-" * 20)
            for failure in validation_result.critical_failures:
                report.append(f"[FAIL] {failure}")
            report.append("")

        # Recommendations
        report.append("RECOMMENDATIONS")
        report.append("-" * 15)
        for rec in validation_result.recommendations:
            report.append(rec)

        return "\n".join(report)


# ============================================================================
# PYTEST TEST CASES
# ============================================================================

@pytest.fixture
def workflow_validator():
    """Create workflow validator for testing."""
    return EndToEndWorkflowValidator()


class TestEndToEndWorkflowValidation:
    """Test suite for end-to-end workflow validation."""

    def test_cli_accessibility_validation(self, workflow_validator):
        """Test CLI accessibility validation."""
        result = workflow_validator.validate_cli_accessibility()

        assert isinstance(result, WorkflowTestResult), "Should return workflow test result"
        assert result.workflow_name == "CLI Accessibility", "Should have correct workflow name"
        assert result.execution_time > 0.0, "Should take measurable time"

        # CLI accessibility is critical for user interaction
        if result.success:
            assert len(result.steps_completed) > 0, "Should complete some steps on success"
            assert "CLI Entry Point Located" in result.steps_completed, "Should locate CLI entry point"

    def test_configuration_system_validation(self, workflow_validator):
        """Test configuration system validation."""
        result = workflow_validator.validate_configuration_system()

        assert isinstance(result, WorkflowTestResult), "Should return workflow test result"
        assert result.workflow_name == "Configuration System", "Should have correct workflow name"

        # Configuration system is critical for system operation
        if result.success:
            assert "Main Configuration File Located" in result.steps_completed, "Should locate config file"

    def test_simulation_execution_validation(self, workflow_validator):
        """Test simulation execution validation."""
        result = workflow_validator.validate_simulation_execution()

        assert isinstance(result, WorkflowTestResult), "Should return workflow test result"
        assert result.workflow_name == "Simulation Execution", "Should have correct workflow name"

        # Simulation execution is core system capability
        if result.success:
            assert len(result.steps_completed) > 0, "Should complete simulation steps"

    def test_web_interface_validation(self, workflow_validator):
        """Test web interface validation."""
        result = workflow_validator.validate_web_interface()

        assert isinstance(result, WorkflowTestResult), "Should return workflow test result"
        assert result.workflow_name == "Web Interface", "Should have correct workflow name"

        # Web interface availability enhances user experience
        if result.success:
            assert "Streamlit App Located" in result.steps_completed, "Should locate Streamlit app"

    def test_testing_infrastructure_validation(self, workflow_validator):
        """Test testing infrastructure validation."""
        result = workflow_validator.validate_testing_infrastructure()

        assert isinstance(result, WorkflowTestResult), "Should return workflow test result"
        assert result.workflow_name == "Testing Infrastructure", "Should have correct workflow name"

        # Testing infrastructure is critical for development workflow
        if result.success:
            assert len(result.steps_completed) > 0, "Should validate testing components"

    def test_comprehensive_validation(self, workflow_validator):
        """Test comprehensive workflow validation."""
        validation_result = workflow_validator.run_comprehensive_validation()

        assert isinstance(validation_result, SystemValidationResult), "Should return system validation result"
        assert 0.0 <= validation_result.overall_success_rate <= 1.0, "Success rate should be between 0 and 1"
        assert len(validation_result.workflow_results) > 0, "Should have workflow results"

        # Mission 10 success criteria
        if validation_result.overall_success_rate >= 0.95:
            assert validation_result.production_ready, "Should be production ready with 95%+ success rate"

    def test_validation_report_generation(self, workflow_validator):
        """Test validation report generation."""
        validation_result = workflow_validator.run_comprehensive_validation()
        report = workflow_validator.generate_validation_report(validation_result)

        assert isinstance(report, str), "Should generate string report"
        assert len(report) > 200, "Report should be substantial"
        assert "END-TO-END WORKFLOW VALIDATION REPORT" in report, "Should have proper header"
        assert "EXECUTIVE SUMMARY" in report, "Should have executive summary"

    def test_mission_10_end_to_end_success_criteria(self, workflow_validator):
        """Test Mission 10 end-to-end workflow success criteria."""
        validation_result = workflow_validator.run_comprehensive_validation()
        report = workflow_validator.generate_validation_report(validation_result)

        print("\n" + "="*80)
        print("MISSION 10: END-TO-END WORKFLOW VALIDATION RESULTS")
        print("="*80)
        print(report)

        # Mission 10 targets
        success_rate_target = 0.95  # 95%+ workflow success rate

        if validation_result.overall_success_rate >= success_rate_target:
            print(f"\nMISSION 10 SUCCESS: {validation_result.overall_success_rate:.1%} workflow success rate achieved!")
            assert validation_result.production_ready, "System should be production ready"
        else:
            print(f"\nMISSION 10 PROGRESS: {validation_result.overall_success_rate:.1%} workflow success rate")
            print(f"Target: {success_rate_target:.1%} | Gap: {success_rate_target - validation_result.overall_success_rate:.1%}")

            # Provide actionable feedback
            print("\nWorkflow Issues to Address:")
            for result in validation_result.workflow_results:
                if not result.success:
                    print(f"  [FAIL] {result.workflow_name}: {len(result.error_messages)} issues")

        # Don't fail test - this is validation and feedback
        assert validation_result.overall_success_rate > 0.0, "System should have some workflow success"


if __name__ == "__main__":
    # Run standalone end-to-end validation
    validator = EndToEndWorkflowValidator()

    print("MISSION 10: End-to-End Workflow Validation")
    print("="*50)

    # Run comprehensive validation
    validation_result = validator.run_comprehensive_validation()

    # Generate and display report
    report = validator.generate_validation_report(validation_result)
    print(report)

    # Final status
    if validation_result.production_ready:
        print("\nSUCCESS: End-to-end workflows are production ready!")
    else:
        print(f"\nPROGRESS: Workflow success rate at {validation_result.overall_success_rate:.1%}")
        print("Continue development to address remaining workflow issues.")