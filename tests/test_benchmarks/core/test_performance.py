#==========================================================================================\\\
#======================= tests/test_benchmarks/test_performance.py =======================\\\
#==========================================================================================\\\
"""
This file is auto‑generated by refactor_tests.sh.
It combines the following original test modules: test_performance.py, test_performance_regression.py.
Each section is delimited by BEGIN/END markers.

"""
import numpy as np
import pytest

from src.controllers.classic_smc import ClassicalSMC
from src.controllers.sta_smc import SuperTwistingSMC
from src.core.dynamics import DoubleInvertedPendulum
from src.core.vector_sim import simulate_system_batch

try:
    # Prefer the public factory
    from src.controllers.factory import create_controller
except Exception as e:
    create_controller = None

# Try to import the public batch API; otherwise fall back to the numba kernel used internally.
_simulate_fallback = None
try:
    from src.core.vector_sim import simulate_system_batch as _simulate_batch
except Exception:  # pragma: no cover - fallback path for older layouts
    from src.core.vector_sim import _simulate_batch_numba_full as _simulate_fallback  # type: ignore

# --------------------------------------------------------------------------------------|
# -----------------------------------Helpers--------------------------------------------|
# --------------------------------------------------------------------------------------|
CTRL_NAMES = ["classical_smc", "sta_smc", "adaptive_smc"]

def _default_gains_for(ctrl_name, config):
    """Fetch reasonable default gains from the config, with safe fallbacks."""
    key = ctrl_name.lower()
    try:
        # Many repos keep defaults under controller_defaults.<name>.gains
        gains = config.controller_defaults[key]["gains"]
        return np.asarray(gains, dtype=float)
    except Exception:
        # Fallbacks chosen to match numba kernels in vector_sim
        if key == "classical_smc":
            # [k1, k2, lam1, lam2, K_switch, kd]
            return np.array([10.0, 8.0, 2.0, 2.0, 50.0, 1.0], dtype=float)
        if key == "sta_smc":
            # [alg_gain_K1, alg_gain_K2, surf_gain_k1, surf_gain_k2, surf_lam1, surf_lam2]
            return np.array([2.0, 1.0, 5.0, 5.0, 3.0, 3.0], dtype=float)
        if key == "adaptive_smc":
            # [k1, k2, lam1, lam2, gamma, rate_weight, leak_rate, K_init]
            return np.array([5.0, 5.0, 2.0, 2.0, 1.0, 1.0, 0.01, 2.0], dtype=float)
        raise ValueError(f"Unknown controller '{ctrl_name}'")

def _controller_mode(ctrl_name: str) -> int:
    key = ctrl_name.lower()
    if key == "classical_smc":
        return 0
    if key == "sta_smc":
        return 1
    if key == "adaptive_smc":
        return 2
    raise ValueError(key)

# --------------------------------------------------------------------------------------
# Benchmarks
# --------------------------------------------------------------------------------------

@pytest.mark.benchmark(group="controller.compute_control")
@pytest.mark.parametrize("ctrl_name", CTRL_NAMES)
def test_controller_compute_speed(ctrl_name, config, benchmark):
    """
    Measure raw compute cost of each controller's compute_control on a static state.
    The benchmark intentionally uses a constant state and stable per-call args to
    isolate the cost of the control law itself (no integration, minimal Python noise).
    """
    if create_controller is None:
        pytest.skip("Controller factory not available to instantiate controllers.")

    gains = _default_gains_for(ctrl_name, config)
    controller = create_controller(ctrl_name, gains=gains, config=config)

    # Initialize controller's transient state
    try:
        last_u, = controller.initialize_state()
    except Exception:
        # Some implementations might return a scalar
        last_u = getattr(controller, "last_u", 0.0)
    try:
        history = controller.initialize_history()
    except Exception:
        history = {}

    # Static state near upright — shape (6,)
    state = np.array([0.0, 0.05, -0.04, 0.0, 0.02, -0.01], dtype=float)

    # Use the benchmark fixture to time just the compute_control call
    result = benchmark(controller.compute_control, state, last_u, history)

    # Sanity: ensure we got something that looks like a control update tuple
    # (no performance assertion here; pytest-benchmark handles stats/compare)
    assert isinstance(result, (tuple, list)) and len(result) >= 1


@pytest.mark.benchmark(group="simulation.throughput")
@pytest.mark.parametrize("ctrl_name", CTRL_NAMES)
def test_full_simulation_throughput(ctrl_name, config, full_dynamics, benchmark):
    """
    Time the end-to-end batch simulation for each controller over 1s of sim time
    with 50 particles. Uses the Numba batch kernel if available.
    """
    # Simulation shape
    B = 50
    dt = float(config.simulation.dt)
    duration = 1.0
    N = int(np.ceil(duration / dt))

    # Gains per particle
    g = _default_gains_for(ctrl_name, config)
    gains_b = np.tile(g, (B, 1))

    # Initial states per particle (slight randomization for realism)
    rng = np.random.default_rng(0)
    x0 = np.zeros((B, 6), dtype=float)
    x0[:, 1] = rng.normal(0.05, 0.01, size=B)  # theta1
    x0[:, 2] = rng.normal(-0.04, 0.01, size=B) # theta2

    # Controller mode for the numba kernel
    mode = _controller_mode(ctrl_name)

    # Choose the callable to benchmark
    if _simulate_fallback is not None:
        # Internal kernel signature: (gains_b, x0_b, N, dt, params, u_max, controller_mode)
        params = full_dynamics.config  # FullDIPParams instance
        try:
            u_max = getattr(config.controllers[ctrl_name], "max_force", None)
        except Exception:
            u_max = None
        if u_max is None:
            u_max = 150.0

        def _call():
            return _simulate_fallback(gains_b, x0, N, dt, params, float(u_max), int(mode))
        result = benchmark(_call)
        # Ensure arrays returned (x, u, sigma, ctrl_states)
        assert isinstance(result, tuple) and len(result) == 4
    else:
        # Public API path: use the high‑level batch simulator
        # Construct a controller factory for this controller
        def controller_factory(particle_gains):
            return create_controller(ctrl_name, config=config, gains=particle_gains)

        def _call():
            return _simulate_batch(
                controller_factory=controller_factory,
                particles=gains_b,
                initial_state=x0,
                sim_time=duration,
                dt=dt,
                u_max=float(getattr(config.controllers[ctrl_name], "max_force", 150.0)),
            )
        result = benchmark(_call)
        assert result is not None


THRESH_CONV_TIME = 0.5  # seconds

def _batch_convergence_time(
    controller_cls,
    physics_params,
    gains,
    dt=0.001,
    initial_state=None,
    batch_size=50,
    std_angle=0.05,
    t_max=2.0,
    use_ueq=False,
    **kwargs,
):
    """
    Runs a batch of simulations and returns the time it took for the worst-case
    particle to converge below the sigma threshold.
    """
    np.random.seed(42)  # For reproducibility

    # The factory function that the batch simulator needs
    def controller_factory(particle_gains):
        """Create a controller instance with all required constructor arguments."""
        kwargs_controller = {}
        # Optionally provide a dynamics model (UEQ) if requested
        if use_ueq:
            kwargs_controller['dynamics_model'] = DoubleInvertedPendulum(physics_params)
        # ClassicalSMC does not accept a dt argument but requires max_force and boundary_layer
        if controller_cls is ClassicalSMC:
            return controller_cls(
                gains=particle_gains,
                max_force=150.0,
                boundary_layer=0.01,
                **kwargs_controller,
            )
        else:
            # SuperTwistingSMC (and other controllers) require dt and accept max_force and boundary_layer
            return controller_cls(
                gains=particle_gains,
                dt=dt,
                max_force=150.0,
                boundary_layer=0.01,
                **kwargs_controller,
            )

    # All particles (the "batch") will use the same gain set for this test
    particle_gains = np.tile(np.asarray(gains), (batch_size, 1))

    # --- Create a cloud of initial states with noise (The AI's good idea) ---
    if initial_state is None:
        central_state = np.array([0.0, 0.2, 0.1, 0.0, 0.0, 0.0])
    else:
        central_state = np.asarray(initial_state)

    # Create a batch of initial states by adding noise to the central state
    initial_states_batch = np.tile(central_state, (batch_size, 1))
    initial_states_batch[:, 1:3] += np.random.normal(0, std_angle, (batch_size, 2))

    # --- CORRECTED: Call the batch simulator with the right arguments ---
    # Use the updated simulate_system_batch with early termination. Passing
    # convergence_tol and grace_period dramatically reduces the simulation
    # time for benchmark tests by stopping integration once all particles
    # converge on the sliding surface. The grace_period matches the
    # convergence criteria used when computing conv_time below.
    t, states_b, controls_b, sigma_b = simulate_system_batch(
        controller_factory=controller_factory,
        particles=particle_gains,
        initial_state=initial_states_batch,  # Pass the batch of states
        dt=dt,
        sim_time=t_max,
        convergence_tol=1e-3,
        grace_period=0.1,
    )

    # --- CORRECTED: Analyze the results with the correct shape ---
    # sigma_b shape is (batch_size, num_steps)
    grace_period_steps = int(0.1 / dt)

    # Find the max sigma value across the entire batch at each time step
    max_sigma_per_step = np.max(np.abs(sigma_b), axis=0)

    # Find the first time step (after a grace period) where all particles have converged
    converged_indices = np.where(max_sigma_per_step[grace_period_steps:] < 1e-3)[0]

    if len(converged_indices) > 0:
        # Get the index of the first convergence point
        first_convergence_index = converged_indices[0] + grace_period_steps
        conv_time = t[first_convergence_index]
    else:
        conv_time = t_max

    rms_per_batch = np.sqrt(np.mean(controls_b**2, axis=1))
    avg_rms_u = np.mean(rms_per_batch)
    return conv_time, avg_rms_u

# --- The pytest benchmark tests remain the same ---
#@pytest.mark.usefixtures("long_simulation_config")
@pytest.mark.benchmark(group="controller_convergence")
def test_classical_smc_convergence(benchmark, physics_cfg):
    result = benchmark.pedantic(
        _batch_convergence_time,
        kwargs=dict(
            controller_cls=ClassicalSMC,
            physics_params=physics_cfg,
            gains=[10.0, 8.0, 5.0, 4.0, 50.0, 1.0],
        ),
        iterations=5,
        rounds=3,
    )
    conv_time, _ = result
    assert conv_time < THRESH_CONV_TIME

@pytest.mark.parametrize("use_ueq", [False, True])
#@pytest.mark.usefixtures("long_simulation_config")
@pytest.mark.benchmark(group="controller_convergence")
def test_sta_smc_convergence(benchmark, physics_cfg, use_ueq):
    """
    Super-Twisting SMC should converge quickly without large σ overshoot
    with corrected sign conventions and validated gains.
    """
    result = benchmark.pedantic(
        _batch_convergence_time,
        kwargs=dict(
            controller_cls=SuperTwistingSMC,
            physics_params=physics_cfg,
            # These are the validated gains for the corrected controller
            gains=[1.18495, 47.7040, 1.0807, 7.4019, 46.9200, 0.6699],
            # Ensure fast convergence from a consistent, perturbed start
            initial_state=np.array([0.0, 0.1, 0.1, 0.0, 0.0, 0.0]),
            use_ueq=use_ueq,
        ),
        iterations=5,
        rounds=3,
    )
    conv_time, _ = result
    assert conv_time < THRESH_CONV_TIME
#=========================================================================\\\   
