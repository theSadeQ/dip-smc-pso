# Simulation Result Validation Methodology

**Document Status:** Phase 3.3 Completion - Monte Carlo and Statistical Testing Framework
**Last Updated:** 2025-10-07
**Part of:** MCP-Orchestrated Documentation Enhancement Workflow

## Executive Summary

This document provides comprehensive validation methodologies for control system simulation results, covering Monte Carlo analysis, cross-validation protocols, statistical testing frameworks, and benchmark comparisons. These methods ensure scientific rigor and reproducibility in controller performance evaluation.

**Key Validation Capabilities:**
- **Monte Carlo Simulation:** LHS, Sobol, Halton sampling with convergence analysis
- **Cross-Validation:** K-fold, time series, Monte Carlo CV for generalization assessment
- **Statistical Testing:** Normality, stationarity, hypothesis testing with power analysis
- **Benchmark Comparison:** Multi-method statistical comparison with effect size analysis

**Implementation:** `src/analysis/validation/` (4 modules, 3,777 total lines)

---

## Table of Contents

1. [Monte Carlo Simulation Methodology](#1-monte-carlo-simulation-methodology)
2. [Cross-Validation Protocols](#2-cross-validation-protocols)
3. [Statistical Testing Framework](#3-statistical-testing-framework)
4. [Benchmark Comparison Methodology](#4-benchmark-comparison-methodology)
5. [Uncertainty Quantification](#5-uncertainty-quantification)
6. [Integration with Control Systems](#6-integration-with-control-systems)
7. [Best Practices and Pitfalls](#7-best-practices-and-pitfalls)

---

## 1. Monte Carlo Simulation Methodology

### 1.1 Overview

Monte Carlo simulation propagates uncertainty through control system models to quantify performance variability and validate stability claims under realistic conditions.

**Implementation:** `src/analysis/validation/monte_carlo.py` (1,007 lines)

### 1.2 Sampling Strategies

#### 1.2.1 Random Sampling (Baseline)

**Description:** Standard pseudorandom sampling from specified distributions.

**Use Cases:**
- Quick uncertainty propagation
- Initial sensitivity screening
- Non-critical applications

**Mathematical Foundation:**

For parameter θ with distribution F(θ), generate samples:

```
θᵢ ~ F(θ), i = 1, ..., N
```

**Convergence Rate:** O(N^(-1/2)) - slow but unbiased

**Implementation Example:**
```python
from src.analysis.validation.monte_carlo import MonteCarloConfig, MonteCarloAnalyzer

config = MonteCarloConfig(
    n_samples=1000,
    sampling_method="random",
    random_seed=42
)
analyzer = MonteCarloAnalyzer(config)
```

**Advantages:**
- Simple to implement
- Unbiased estimator
- Parallelizable

**Limitations:**
- Slow convergence
- Poor space coverage
- High variance for same N

---

#### 1.2.2 Latin Hypercube Sampling (LHS)

**Description:** Stratified sampling ensuring uniform coverage of parameter space.

**Use Cases:**
- **Recommended for most control applications**
- Parameter sensitivity analysis
- Design space exploration
- Limited computational budget

**Mathematical Foundation:**

Partition each parameter dimension into N intervals of equal probability. Sample one point from each interval with random permutation across dimensions:

```
For parameter θⱼ ∈ [a, b]:
Intervals: [a + (i-1)(b-a)/N, a + i(b-a)/N], i = 1,...,N
Sample: θⱼ,ᵢ ~ Uniform(interval_i)
Apply random permutation π across dimensions
```

**Convergence Rate:** O(N^(-1)) - faster than random sampling

**Implementation Example:**
```python
config = MonteCarloConfig(
    n_samples=500,  # Can use fewer samples than random
    sampling_method="latin_hypercube",
    random_seed=42
)
analyzer = MonteCarloAnalyzer(config)
```

**Advantages:**
- Better space coverage than random sampling
- Faster convergence
- Efficient for sensitivity analysis
- Works well with 100-1000 samples

**When to Use:**
- Computational budget limited (expensive simulations)
- Need uniform coverage of uncertainty space
- Sensitivity analysis required
- 5-20 uncertain parameters

---

#### 1.2.3 Sobol Sequence (Quasi-Monte Carlo)

**Description:** Low-discrepancy sequence for deterministic space-filling sampling.

**Use Cases:**
- High-dimensional parameter spaces (>10 dimensions)
- Global sensitivity analysis (Sobol indices)
- Expensive simulations requiring maximum efficiency
- Publication-quality uncertainty quantification

**Mathematical Foundation:**

Sobol sequences are quasi-random sequences with low discrepancy D(N):

```
D(N) = O((log N)^d / N)  where d = dimension
```

Much better than random sampling: D(N) = O(N^(-1/2))

**Convergence Rate:** O(N^(-1) log^d N) - best for high dimensions

**Implementation Example:**
```python
config = MonteCarloConfig(
    n_samples=1024,  # Typically use powers of 2
    sampling_method="sobol",
    sensitivity_analysis=True,
    sensitivity_method="sobol"
)
analyzer = MonteCarloAnalyzer(config)
```

**Advantages:**
- Deterministic and reproducible
- Excellent space-filling properties
- Best for high-dimensional problems
- Enables Sobol sensitivity indices

**Limitations:**
- Implementation complexity
- Requires specific sample sizes (powers of 2)
- May not work well with adaptive sampling

**When to Use:**
- >10 uncertain parameters
- Need global sensitivity analysis
- Computational budget allows 1000+ samples
- Deterministic reproducibility required

---

#### 1.2.4 Halton Sequence

**Description:** Another quasi-random low-discrepancy sequence based on prime numbers.

**Use Cases:**
- Alternative to Sobol for moderate dimensions (5-15)
- Sequential simulation (easy to extend sample size)
- Real-time uncertainty propagation

**Mathematical Foundation:**

Halton sequence uses van der Corput sequences with different prime bases:

```
For dimension j with prime pⱼ:
xⱼ,ᵢ = vdC(i, pⱼ)

where vdC(n, p) = Σₖ aₖp^(-k-1)
and n = Σₖ aₖp^k (base-p expansion)
```

**Convergence Rate:** O(N^(-1) log N) - slightly worse than Sobol

**Implementation Example:**
```python
config = MonteCarloConfig(
    n_samples=1000,
    sampling_method="halton"
)
analyzer = MonteCarloAnalyzer(config)
```

**Advantages:**
- Easy to extend sample size incrementally
- Simpler implementation than Sobol
- Good for sequential analysis

**Limitations:**
- Correlation issues in higher dimensions
- Slightly worse convergence than Sobol

**When to Use:**
- Sequential/adaptive sampling needed
- 5-15 dimensions
- Simpler alternative to Sobol

---

### 1.3 Variance Reduction Techniques

#### 1.3.1 Antithetic Variates

**Description:** Generate paired samples with negative correlation to reduce variance.

**Mathematical Principle:**

For estimator θ̂ = (Y₁ + Y₂)/2 where Y₁, Y₂ are antithetic:

```
Var(θ̂) = [Var(Y₁) + Var(Y₂) + 2Cov(Y₁, Y₂)] / 4

If Cov(Y₁, Y₂) < 0, then Var(θ̂) < Var(Ȳ)
```

**Implementation:**
```python
config = MonteCarloConfig(
    n_samples=1000,
    antithetic_variates=True
)
```

**Example:** For uniform θ ~ U(0,1), use pairs (θ, 1-θ)

**Variance Reduction:** Typically 50-90% for monotonic functions

---

#### 1.3.2 Control Variates

**Description:** Use known expectation of correlated variable to reduce variance.

**Mathematical Principle:**

```
θ̂_CV = θ̂ + c(Ẑ - E[Z])

where Z is control variate with known E[Z]
```

Optimal coefficient:
```
c* = -Cov(θ̂, Ẑ) / Var(Ẑ)
```

Variance reduction:
```
Var(θ̂_CV) = Var(θ̂)(1 - ρ²)

where ρ = Corr(θ̂, Ẑ)
```

**Use Cases:**
- Linear approximation available
- Similar problem solved analytically
- High correlation with known quantity

---

### 1.4 Convergence Criteria

#### 1.4.1 Running Mean Stability

**Criterion:** Stop when running mean stabilizes within tolerance.

**Mathematical Test:**

```
|μ̂ₙ - μ̂ₙ₋ₖ| / |μ̂ₙ| < ε

where:
μ̂ₙ = (1/n)Σᵢ₌₁ⁿ Xᵢ
k = convergence_window
ε = convergence_tolerance
```

**Implementation:**
```python
config = MonteCarloConfig(
    convergence_tolerance=0.01,  # 1% relative change
    convergence_window=50,
    min_samples=100,
    max_samples=10000
)
```

**Interpretation:**
- Converged: Mean stable to within 1% over last 50 samples
- Not converged: Increase max_samples

---

#### 1.4.2 Confidence Interval Width

**Criterion:** Stop when confidence interval is sufficiently narrow.

**Mathematical Test:**

```
CI_width = 2 × t_{α/2,n-1} × (s/√n) < δ

where:
t_{α/2,n-1} = t-distribution critical value
s = sample standard deviation
δ = desired precision
```

**Target Precision:**
- Safety-critical: δ < 0.05 × |μ̂| (5% of mean)
- Performance analysis: δ < 0.10 × |μ̂| (10% of mean)
- Initial screening: δ < 0.20 × |μ̂| (20% of mean)

---

### 1.5 Bootstrap Analysis

**Description:** Resampling method for estimating sampling distribution and confidence intervals without distributional assumptions.

**Mathematical Foundation:**

From sample {x₁, ..., xₙ}, generate B bootstrap samples:

```
X*ᵇ = {x*₁, ..., x*ₙ} sampled with replacement

Compute statistic: θ̂*ᵇ = g(X*ᵇ)

Bootstrap CI (percentile method):
[θ̂*_{α/2}, θ̂*_{1-α/2}]
```

**Implementation:**
```python
config = MonteCarloConfig(
    bootstrap_samples=1000,
    bootstrap_confidence_level=0.95
)

result = analyzer.validate(data)
bootstrap_ci = result.data['bootstrap_analysis']['mean_confidence_interval']
```

**Use Cases:**
- Non-normal distributions
- Small sample sizes
- Complex statistics (e.g., ratio of means)
- Unknown analytical CI

**Bootstrap CI Types:**
1. **Percentile:** [θ̂*_{α/2}, θ̂*_{1-α/2}]
2. **Basic:** [2θ̂ - θ̂*_{1-α/2}, 2θ̂ - θ̂*_{α/2}]
3. **BCa (Bias-Corrected Accelerated):** Adjusts for bias and skewness

---

### 1.6 Sensitivity Analysis

#### 1.6.1 One-at-a-Time (OAT) Sensitivity

**Description:** Vary each parameter individually while holding others constant.

**Sensitivity Measure:**

```
S_i = ∂f/∂θᵢ ≈ [f(θ + Δθᵢ) - f(θ)] / Δθᵢ
```

**Implementation:**
```python
config = MonteCarloConfig(
    sensitivity_analysis=True,
    sensitivity_method="simple"  # One-at-a-time
)
```

**Advantages:**
- Intuitive interpretation
- Low computational cost
- Good for screening

**Limitations:**
- Ignores parameter interactions
- Local (not global) sensitivity
- Misses nonlinear effects

**When to Use:**
- Initial parameter screening
- Linear/weakly nonlinear systems
- Limited computational budget

---

#### 1.6.2 Sobol Sensitivity Indices

**Description:** Variance-based global sensitivity analysis decomposing output variance into parameter contributions.

**Mathematical Foundation:**

Total variance decomposition:

```
Var(Y) = Σᵢ Vᵢ + Σᵢ<ⱼ Vᵢⱼ + ... + V₁₂...ₙ

where:
Vᵢ = Var[E(Y|Xᵢ)] = variance due to Xᵢ alone
Vᵢⱼ = Var[E(Y|Xᵢ,Xⱼ)] - Vᵢ - Vⱼ = interaction effect
```

**Sobol Indices:**

First-order (main effect):
```
Sᵢ = Vᵢ / Var(Y)
```

Total-order (including interactions):
```
STᵢ = [Var(Y) - Var[E(Y|X₋ᵢ)]] / Var(Y)
```

**Interpretation:**
- Sᵢ: Fraction of variance explained by Xᵢ alone
- STᵢ - Sᵢ: Interaction effects involving Xᵢ
- Σᵢ Sᵢ ≠ 1 if interactions present

**Use Cases:**
- Nonlinear systems
- Parameter interactions expected
- Need global sensitivity
- >5 parameters to screen

**Computational Cost:**
- Basic Sobol: O(N × (d+2)) simulations
- Second-order: O(N × d²) simulations

**When to Use:**
- Need rigorous global sensitivity
- Can afford 1000+ simulations
- Parameter ranking for optimization
- Publication-quality analysis

---

### 1.7 Distribution Fitting

**Description:** Identify best-fit probability distribution for simulation output.

**Supported Distributions:**
1. **Normal:** Gaussian distribution
2. **Lognormal:** Positive skewed data
3. **Exponential:** Failure times, waiting times
4. **Gamma:** Positive continuous data
5. **Beta:** Bounded data [0,1]

**Goodness-of-Fit Tests:**

**Kolmogorov-Smirnov (K-S):**
```
D = sup_x |F̂(x) - F₀(x)|

where F̂ = empirical CDF, F₀ = hypothesized CDF
```

**Anderson-Darling:**
```
A² = -n - (1/n)Σᵢ (2i-1)[ln F₀(Xᵢ) + ln(1-F₀(Xₙ₊₁₋ᵢ))]
```

More sensitive to tail behavior than K-S.

**Model Selection:**

Akaike Information Criterion (AIC):
```
AIC = 2k - 2ln(L̂)

where k = number of parameters, L̂ = likelihood
```

**Best fit:** Minimum AIC

**Implementation:**
```python
result = analyzer.validate(data)
dist_analysis = result.data['distribution_analysis']

best_fit = dist_analysis['best_fit']  # e.g., "lognormal"
ks_stat = dist_analysis['distribution_fits'][best_fit]['ks_statistic']
p_value = dist_analysis['distribution_fits'][best_fit]['p_value']
```

**Interpretation:**
- p_value > 0.05: Cannot reject hypothesized distribution
- AIC comparison: Lower is better (difference >10 is strong evidence)

---

## 2. Cross-Validation Protocols

### 2.1 Overview

Cross-validation assesses generalization performance and prevents overfitting in controller tuning and model selection.

**Implementation:** `src/analysis/validation/cross_validation.py` (920 lines)

**Key Question:** Will this controller perform well on unseen scenarios?

---

### 2.2 K-Fold Cross-Validation

**Description:** Split data into K equal folds; train on K-1, test on 1, repeat K times.

**Mathematical Framework:**

```
For k = 1, ..., K:
  Train on D \ Dₖ
  Test on Dₖ
  Compute score Sₖ

CV score = (1/K) Σₖ Sₖ
CV std = √[(1/K) Σₖ (Sₖ - S̄)²]
```

**Implementation:**
```python
from src.analysis.validation.cross_validation import CrossValidationConfig, CrossValidator

config = CrossValidationConfig(
    cv_method="k_fold",
    n_splits=5,
    shuffle=True,
    random_state=42
)
validator = CrossValidator(config)
```

**Choosing K:**
- **K=5:** Standard choice, good bias-variance tradeoff
- **K=10:** More stable, higher variance
- **K=N (LOO):** Unbiased but high variance, expensive

**Bias-Variance Tradeoff:**
- **Small K:** Low computational cost, high bias (small training sets)
- **Large K:** Low bias, high variance (similar training sets)

**When to Use:**
- General-purpose validation
- i.i.d. data (not time series)
- Sufficient data (N > 50)
- Hyperparameter tuning

**Advantages:**
- Simple and interpretable
- Efficient (all data used for training and testing)
- Low variance estimates

**Limitations:**
- Assumes i.i.d. data
- Not suitable for time series
- May leak temporal information

---

### 2.3 Time Series Cross-Validation

**Description:** Respects temporal order; train on past, test on future.

**Mathematical Framework:**

```
Split 1: Train [1, n₁]      Test [n₁+1, n₁+t]
Split 2: Train [1, n₂]      Test [n₂+1, n₂+t]
...
Split K: Train [1, nₖ]      Test [nₖ+1, nₖ+t]

where n₁ < n₂ < ... < nₖ
```

**Implementation:**
```python
config = CrossValidationConfig(
    cv_method="time_series",
    n_splits=5,
    max_train_size=None,  # Use all past data
    test_size=None,       # Auto-determined
    gap=0                 # No gap between train/test
)
validator = CrossValidator(config)
```

**Parameters:**
- **gap:** Forecast horizon (e.g., gap=10 for 10-step-ahead prediction)
- **max_train_size:** Limit training window (e.g., sliding window)
- **test_size:** Fixed test set size

**Use Cases for Control Systems:**
1. **Controller Performance Prediction:**
   - Train on historical trajectories
   - Test on future scenarios
   - Gap = control horizon

2. **Adaptive Controller Validation:**
   - Sliding window training
   - Evaluate online adaptation

3. **Disturbance Rejection:**
   - Train on nominal conditions
   - Test on perturbed scenarios

**When to Use:**
- Time series data (trajectories, adaptive control)
- Temporal dependencies important
- Evaluate forecasting ability
- Controller adaptation analysis

**Advantages:**
- Respects temporal order
- Realistic evaluation
- Prevents temporal leakage

**Limitations:**
- Fewer training samples than K-fold
- High variance if limited data
- Correlated test sets

---

### 2.4 Stratified K-Fold

**Description:** Maintain class/value distribution in each fold.

**Use Cases:**
- Imbalanced scenarios (e.g., rare fault conditions)
- Discrete operating regimes
- Classification tasks

**Implementation:**
```python
config = CrossValidationConfig(
    cv_method="stratified",
    n_splits=5
)
```

**When to Use:**
- Imbalanced operating conditions
- Rare events (faults, constraints)
- Ensure representative test sets

---

### 2.5 Monte Carlo Cross-Validation

**Description:** Random train-test splits repeated many times.

**Mathematical Framework:**

```
For iteration i = 1, ..., N:
  Random split: 80% train, 20% test
  Train model and compute score Sᵢ

CV score = mean(S)
CV std = std(S)
```

**Implementation:**
```python
config = CrossValidationConfig(
    cv_method="monte_carlo",
    n_repetitions=100,
    test_ratio=0.2,
    random_state=42
)
```

**Advantages:**
- Flexible train/test ratio
- More iterations than K-fold
- Lower variance estimates

**When to Use:**
- Large datasets
- Need precise CV estimates
- Computational budget allows

---

### 2.6 Nested Cross-Validation

**Description:** Outer CV for performance estimation, inner CV for hyperparameter tuning.

**Purpose:** Obtain unbiased performance estimate when hyperparameters are tuned.

**Mathematical Framework:**

```
Outer CV (performance estimation):
  For k = 1, ..., K_outer:
    Train_outer = D \ Dₖ

    Inner CV (hyperparameter selection):
      For j = 1, ..., K_inner:
        Train_inner ⊂ Train_outer
        Tune hyperparameters

    Select best hyperparameters
    Evaluate on Dₖ
```

**Implementation:**
```python
config = CrossValidationConfig(
    cv_method="k_fold",
    n_splits=5,              # Outer CV
    enable_nested_cv=True,
    inner_cv_splits=3        # Inner CV
)
```

**Computational Cost:**
- K_outer × K_inner × n_hyperparameter_configs evaluations
- Example: 5 × 3 × 10 = 150 model trains

**When to Use:**
- Hyperparameter tuning required
- Need unbiased performance estimate
- Avoid optimistic bias

**Warning:** High computational cost - use for final validation only.

---

### 2.7 Bias-Variance Decomposition

**Description:** Decompose prediction error into bias², variance, and irreducible noise.

**Mathematical Foundation:**

Expected prediction error:

```
E[(y - f̂(x))²] = Bias²(f̂) + Var(f̂) + σ²

where:
Bias(f̂) = E[f̂(x)] - f(x)
Var(f̂) = E[(f̂(x) - E[f̂(x)])²]
σ² = irreducible noise
```

**Implementation:**
```python
result = validator.validate(
    data,
    models=[model],
    prediction_function=predict_fn
)
bv_analysis = result.data['bias_variance_analysis']

bias_squared = bv_analysis[model_name]['bias_squared']
variance = bv_analysis[model_name]['variance']
```

**Interpretation:**

**High Bias (Underfitting):**
- Model too simple
- Missing important features
- **Solution:** Increase model complexity

**High Variance (Overfitting):**
- Model too complex
- Overfitting training data
- **Solution:** Regularization, more data, simpler model

**Bias-Variance Tradeoff:**
```
Total Error = Bias² + Variance + Noise

Optimal complexity minimizes total error
```

**Control System Context:**
- **Bias:** Systematic controller error (e.g., tracking offset)
- **Variance:** Performance variability across scenarios
- **Target:** Low bias AND low variance

---

### 2.8 Learning Curves

**Description:** Plot performance vs. training set size to diagnose learning behavior.

**Mathematical Framework:**

```
For n = [n₁, n₂, ..., nₖ]:
  Train on first n samples
  Evaluate on validation set
  Record train and test scores
```

**Interpretation:**

**Convergence Patterns:**

1. **High Bias (Underfitting):**
   ```
   Train error: High, plateaus quickly
   Test error:  High, parallel to train error
   Gap:         Small
   ```
   **Diagnosis:** Model capacity insufficient
   **Solution:** More complex model, more features

2. **High Variance (Overfitting):**
   ```
   Train error: Low, continues decreasing
   Test error:  High, large gap from train
   Gap:         Large
   ```
   **Diagnosis:** Model memorizing training data
   **Solution:** More data, regularization, simpler model

3. **Good Fit:**
   ```
   Train error: Low, stable
   Test error:  Low, converging to train error
   Gap:         Small and decreasing
   ```
   **Diagnosis:** Optimal model complexity
   **Action:**  Production ready

**Implementation:**
```python
result = validator.validate(data, models=[model])
lc = result.data['learning_curve_analysis'][model_name]

train_sizes = lc['train_sizes']
train_scores = lc['train_scores']
test_scores = lc['test_scores']
```

**When to Use:**
- Diagnose underfitting vs. overfitting
- Determine if more data would help
- Select appropriate model complexity
- Convince stakeholders of data needs

---

## 3. Statistical Testing Framework

### 3.1 Overview

Statistical tests validate assumptions and quantify confidence in analysis results.

**Implementation:** `src/analysis/validation/statistical_tests.py` (905 lines)

**Core Questions:**
1. Are my data normally distributed?
2. Is my time series stationary?
3. Is the difference statistically significant?
4. How large is the effect?

---

### 3.2 Normality Tests

**Purpose:** Many statistical tests assume normality. Verify this assumption.

**Implementation:** `src/analysis/validation/statistical_tests.py` (lines 217-291)

---

#### 3.2.1 Shapiro-Wilk Test

**Description:** Most powerful normality test for small-to-moderate samples.

**Null Hypothesis:** Data come from normal distribution

**Test Statistic:**

```
W = (Σᵢ aᵢx₍ᵢ₎)² / Σᵢ(xᵢ - x̄)²

where x₍ᵢ₎ are order statistics, aᵢ are weights
```

**Implementation:**
```python
from src.analysis.validation.statistical_tests import StatisticalTestSuite

suite = StatisticalTestSuite()
result = suite.validate(data, test_types=['normality_tests'])

shapiro = result.data['normality_tests']['shapiro_wilk']
print(f"W = {shapiro['statistic']:.4f}, p = {shapiro['p_value']:.4f}")
```

**Decision Rule:**
- p > 0.05: Cannot reject normality (data may be normal)
- p < 0.05: Reject normality (data not normal)

**Sample Size:**
- **Recommended:** 10 ≤ n ≤ 5000
- **Limitation:** Not applicable for n > 5000

**Power:** Highest among normality tests for small samples

---

#### 3.2.2 Anderson-Darling Test

**Description:** Emphasizes tails more than K-S test.

**Test Statistic:**

```
A² = -n - (1/n)Σᵢ (2i-1)[ln Φ(zᵢ) + ln(1-Φ(zₙ₊₁₋ᵢ))]

where zᵢ = (xᵢ - μ̂)/σ̂, Φ = standard normal CDF
```

**Critical Values (α = 0.05):**
- A² < 0.787: Cannot reject normality
- A² > 0.787: Reject normality

**Advantages:**
- More sensitive to tail deviations
- Works for larger samples
- Distribution-specific critical values

---

#### 3.2.3 Kolmogorov-Smirnov Test

**Description:** Tests if empirical CDF matches hypothesized distribution.

**Test Statistic:**

```
D = sup_x |F̂ₙ(x) - F₀(x)|

where F̂ₙ = empirical CDF, F₀ = normal CDF
```

**Decision Rule:**

```
Reject H₀ if D > critical value(α, n)

Critical value ≈ 1.36/√n for α=0.05
```

**Limitations:**
- Less powerful than Shapiro-Wilk
- Conservative (may not reject false H₀)
- Sensitive to ties in data

---

#### 3.2.4 D'Agostino-Pearson Test

**Description:** Tests skewness and kurtosis jointly.

**Test Statistic:**

```
K² = Z₁² + Z₂²  ~ χ²(2)

where Z₁ = skewness test, Z₂ = kurtosis test
```

**Advantages:**
- Works for larger samples (n > 20)
- Identifies specific departures (skewness vs. kurtosis)

---

**Normality Test Decision Tree:**

```
Sample Size?
│
├─ n < 50:      Use Shapiro-Wilk (most powerful)
├─ 50 ≤ n ≤ 300: Use Shapiro-Wilk or Anderson-Darling
├─ n > 300:      Use D'Agostino-Pearson
└─ Very large:   Use QQ-plot (visual) + A-D test
```

**If Non-Normal:**
1. Check for outliers
2. Consider transformation (log, Box-Cox)
3. Use non-parametric tests
4. Use bootstrap for confidence intervals

---

### 3.3 Stationarity Tests

**Purpose:** Time series analysis assumes stationarity. Non-stationary data can lead to spurious conclusions.

**Definition:** A time series is stationary if:
1. Constant mean: E[Xₜ] = μ for all t
2. Constant variance: Var(Xₜ) = σ² for all t
3. Autocovariance depends only on lag: Cov(Xₜ, Xₜ₊ₖ) = γₖ

---

#### 3.3.1 Augmented Dickey-Fuller (ADF) Test

**Description:** Tests for unit root (non-stationarity).

**Null Hypothesis:** Series has unit root (non-stationary)

**Test Regression:**

```
Δyₜ = α + βt + γyₜ₋₁ + ΣⱼδⱼΔyₜ₋ⱼ + εₜ

H₀: γ = 0 (unit root)
H₁: γ < 0 (stationary)
```

**Implementation:**
```python
result = suite.validate(data, test_types=['stationarity_tests'])
adf = result.data['stationarity_tests']['augmented_dickey_fuller']

print(f"ADF statistic: {adf['test_statistic']:.4f}")
print(f"Conclusion: {adf['conclusion']}")
```

**Decision Rule:**
- ADF statistic < critical value: Reject H₀ (stationary)
- ADF statistic > critical value: Cannot reject H₀ (non-stationary)

**Critical Values (α = 0.05):**
- No trend: -2.86
- With trend: -3.41

**If Non-Stationary:**
1. Difference the series: ∇Xₜ = Xₜ - Xₜ₋₁
2. Remove trend: detrend(X)
3. Use non-stationary methods (e.g., cointegration)

---

#### 3.3.2 KPSS Test

**Description:** Tests for stationarity (opposite of ADF).

**Null Hypothesis:** Series is stationary

**Note:** KPSS and ADF are complementary:
- ADF: H₀ = non-stationary
- KPSS: H₀ = stationary

**Best Practice:** Use both tests:

| ADF         | KPSS        | Conclusion                  |
|-------------|-------------|-----------------------------|
| Reject H₀   | Don't reject| **Stationary**             |
| Don't reject| Reject H₀   | **Non-stationary**         |
| Reject H₀   | Reject H₀   | Trending stationary        |
| Don't reject| Don't reject| Inconclusive (more data)   |

---

### 3.4 Hypothesis Testing

**Purpose:** Quantify evidence for/against research claims.

**Framework:**

1. State hypotheses:
   - H₀ (null): No effect / No difference
   - H₁ (alternative): Effect exists

2. Choose test statistic and significance level α (typically 0.05)

3. Compute p-value: P(observe data or more extreme | H₀ true)

4. Decision:
   - p < α: Reject H₀ (statistically significant)
   - p ≥ α: Cannot reject H₀ (not significant)

---

#### 3.4.1 One-Sample Tests

**Purpose:** Test if population mean equals hypothesized value.

**t-Test:**

```
H₀: μ = μ₀
H₁: μ ≠ μ₀ (two-sided)

Test statistic: t = (x̄ - μ₀) / (s/√n) ~ t(n-1)
```

**Implementation:**
```python
result = suite.validate(data, test_types=['hypothesis_tests'])
ttest = result.data['hypothesis_tests']['one_sample']['t_test_zero_mean']

if ttest['p_value'] < 0.05:
    print(f"Mean significantly different from 0 (p={ttest['p_value']:.4f})")
```

**Example (Control System):**
- H₀: Tracking error mean = 0
- H₁: Systematic bias exists
- Reject H₀ → Controller has systematic bias

---

#### 3.4.2 Two-Sample Tests

**Purpose:** Compare two controllers/methods.

**Independent t-Test (Welch's):**

```
H₀: μ₁ = μ₂
H₁: μ₁ ≠ μ₂

Test statistic: t = (x̄₁ - x̄₂) / √(s₁²/n₁ + s₂²/n₂)
```

**Paired t-Test:**

Use when same scenarios tested with both methods:

```
H₀: μ_diff = 0
H₁: μ_diff ≠ μ₀

Test statistic: t = d̄ / (s_d/√n)
where d̄ = mean difference, s_d = std of differences
```

**Implementation:**
```python
config = StatisticalTestConfig(
    use_paired_tests=True,  # Use paired if same scenarios
    significance_level=0.05
)
suite = StatisticalTestSuite(config)
```

**Mann-Whitney U Test (Non-parametric):**

Use when normality assumption violated:

```
H₀: Distributions are equal
H₁: Distributions differ

Test based on rank sums
```

**When to Use:**
- **t-test:** Normal data, continuous outcomes
- **Mann-Whitney:** Non-normal data, ordinal data, outliers
- **Paired:** Same scenarios, within-subject comparisons

---

#### 3.4.3 Multiple Comparison Corrections

**Problem:** Testing multiple hypotheses increases false positive rate.

**Example:** 20 comparisons at α=0.05 → expect 1 false positive

**Family-Wise Error Rate (FWER):**

Probability of at least one false positive:

```
FWER = 1 - (1-α)ᵐ ≈ mα for small α

where m = number of tests
```

**Bonferroni Correction:**

```
α_corrected = α / m

Most conservative - reject if p < α/m
```

**Holm-Bonferroni (Sequential):**

1. Order p-values: p₍₁₎ ≤ p₍₂₎ ≤ ... ≤ p₍ₘ₎
2. Reject H₍ᵢ₎ if p₍ᵢ₎ < α/(m-i+1)
3. Stop at first non-rejection

**Benjamini-Hochberg (FDR Control):**

Controls False Discovery Rate instead of FWER (less conservative):

1. Order p-values
2. Find largest i where p₍ᵢ₎ ≤ (i/m)α
3. Reject all H₍₁₎, ..., H₍ᵢ₎

**Implementation:**
```python
config = StatisticalTestConfig(
    multiple_comparisons_correction="bonferroni"  # or "holm", "fdr_bh"
)
```

**Choosing Correction:**
- **Bonferroni:** Very conservative, exploratory phase
- **Holm:** Less conservative, more power
- **FDR (B-H):** Large-scale testing (e.g., 100+ tests), discovery-focused
- **None:** Pre-planned single comparison

---

### 3.5 Power Analysis

**Purpose:** Determine sample size needed to detect an effect of given size with desired probability.

**Key Concepts:**

1. **Power (1-β):** Probability of correctly rejecting false H₀
   - Standard: 0.80 (80% power)
   - Rigorous: 0.90 or 0.95

2. **Effect Size:** Magnitude of difference
   - Small: Hard to detect
   - Large: Easy to detect

3. **Significance Level (α):** Type I error rate
   - Standard: 0.05
   - Stringent: 0.01

4. **Sample Size (n):** Number of observations

**Relationship:**
```
↑ Effect size  → ↑ Power (easier to detect)
↑ Sample size  → ↑ Power (more data)
↑ α (less stringent) → ↑ Power (more false positives)
```

**Sample Size Formula (two-sample t-test):**

```
n = 2(z₁₋α/₂ + z₁₋β)² / δ²

where δ = effect size (Cohen's d)
```

**Implementation:**
```python
result = suite.validate(data, test_types=['power_analysis'])
power_analysis = result.data['power_analysis']

print(f"Current power: {power_analysis['estimated_power']:.2f}")
print(f"Recommended N: {power_analysis['recommended_sample_size']}")

if not power_analysis['power_adequate']:
    print("⚠ Insufficient power - need more data")
```

**Interpretation:**

| Current N | Power | Action                           |
|-----------|-------|----------------------------------|
| 30        | 0.45  | Need 2x more data (underpowered)|
| 50        | 0.75  | Marginal - collect more if possible|
| 100       | 0.85  | Adequate power ✓                |
| 200       | 0.95  | Excellent power ✓✓              |

**Control System Context:**

- **Before Experiments:** Determine required test scenarios
- **During Analysis:** Check if differences are detectable
- **Publication:** Report achieved power to avoid false negatives

---

### 3.6 Effect Size Analysis

**Purpose:** Quantify practical significance (not just statistical significance).

**Key Insight:** With large n, tiny differences become statistically significant but may not be practically important.

---

#### 3.6.1 Cohen's d

**Definition:** Standardized mean difference

```
d = (μ₁ - μ₂) / σ_pooled

where σ_pooled = √[(s₁² + s₂²) / 2]
```

**Interpretation (Cohen's conventions):**

| |d| | Magnitude  | Interpretation              |
|-----|------------|------------------------------|
| 0.0-0.2 | Negligible | Not worth pursuing          |
| 0.2-0.5 | Small      | Detectable, minor practical value|
| 0.5-0.8 | Medium     | Noticeable, moderate importance|
| >0.8    | Large      | Obvious, high practical value|

**Implementation:**
```python
result = suite.validate(
    data,
    compare_groups=[group2],
    test_types=['effect_size_analysis']
)

effect_size = result.data['effect_size_analysis']
cohens_d = effect_size['cohens_d_group_0']['value']
interpretation = effect_size['cohens_d_group_0']['interpretation']
```

**Control System Example:**

Comparing two controllers:
- Controller A: mean settling time = 2.5s, std = 0.5s
- Controller B: mean settling time = 2.0s, std = 0.4s

```
d = (2.5 - 2.0) / √[(0.5² + 0.4²)/2] = 1.11 → Large effect
```

**Conclusion:** 0.5s improvement is **large** effect - substantial practical benefit.

---

#### 3.6.2 Glass's Δ

**Definition:** Standardize by control group std only

```
Δ = (μ_treatment - μ_control) / σ_control
```

**Use When:** Control group is the established baseline, treatment may have different variance.

---

#### 3.6.3 Hedges' g

**Definition:** Bias-corrected Cohen's d for small samples

```
g = d × [1 - 3/(4n - 9)]

Correction factor ≈ 1 for n > 50
```

**Use When:** Small samples (n < 20)

---

**Effect Size Decision Tree:**

```
What are you comparing?
│
├─ Two groups (between-subjects)
│  ├─ Equal variances expected: Cohen's d
│  ├─ Control is reference: Glass's Δ
│  └─ Small sample: Hedges' g
│
├─ Paired observations (within-subjects)
│  └─ Use Cohen's d on differences
│
└─ Multiple groups
   └─ Use η² (eta-squared) or ω² (omega-squared)
```

---

## 4. Benchmark Comparison Methodology

### 4.1 Overview

Rigorous statistical comparison of multiple controllers/methods.

**Implementation:** `src/analysis/validation/benchmarking.py` (841 lines)

**Workflow:**
1. Define test scenarios
2. Run all methods on same scenarios
3. Statistical comparison
4. Ranking and recommendations

---

### 4.2 Performance Metrics Selection

**Key Principle:** Select metrics aligned with application requirements.

**Standard Control Metrics:**

| Metric            | Definition                  | Use Case                     |
|-------------------|-----------------------------|------------------------------|
| Rise Time         | 10% → 90% of steady-state  | Responsiveness               |
| Settling Time     | Within ±2% of steady-state | Stability                    |
| Overshoot         | Peak - steady-state        | Safety, constraints          |
| Steady-State Error| |target - final|            | Accuracy                     |
| RMS Tracking Error| √(mean(e²))                | Overall performance          |
| Control Effort    | Σ|u|                       | Actuator wear, energy        |
| Disturbance Rejection| RMSE with disturbance   | Robustness                   |

**Implementation:**
```python
from src.analysis.validation.benchmarking import BenchmarkConfig, BenchmarkSuite

config = BenchmarkConfig(
    metrics_to_compare=["settling_time", "overshoot", "control_effort"],
    primary_metric="settling_time"
)
```

**Multi-Objective Ranking:**

When multiple metrics matter, use weighted score:

```
Score = Σᵢ wᵢ × (normalized_metricᵢ)

Normalization: [0,1] where 1 is best
```

---

### 4.3 Statistical Significance Testing

**Workflow:**

1. **Run Trials:** Each method × each scenario × multiple trials
2. **Collect Scores:** Distribution of performance for each method
3. **Pairwise Tests:** Compare each pair statistically
4. **Multiple Comparison Correction:** Adjust for many tests
5. **Ranking:** Order methods by performance

**Implementation:**
```python
benchmark = BenchmarkSuite(config)

result = benchmark.validate(
    data=None,
    methods=[controller_A, controller_B, controller_C],
    simulation_function=run_simulation,
    test_cases=[scenario1, scenario2, scenario3]
)

sig_tests = result.data['statistical_significance_testing']
ranking = result.data['ranking_analysis']['final_ranking']
```

**Interpreting Results:**

```python
for comparison, test in sig_tests['corrected_tests'].items():
    if test['corrected_significant']:
        print(f"{comparison}: Significant difference (p={test['corrected_p_value']:.4f})")
    else:
        print(f"{comparison}: No significant difference")
```

---

### 4.4 Robustness Comparison

**Robustness Measures:**

1. **Coefficient of Variation (CV):**
   ```
   CV = σ / |μ|

   Lower CV = more robust (consistent performance)
   ```

2. **Interquartile Range (IQR):**
   ```
   IQR = Q₃ - Q₁

   Smaller IQR = less variability
   ```

3. **Worst-Case Performance:**
   ```
   Worst-case = 95th percentile of error

   Critical for safety-critical systems
   ```

**Implementation:**
```python
robustness = result.data['robustness_comparison']

for method_name, metrics in robustness['robustness_metrics'].items():
    cv = metrics['settling_time']['coefficient_of_variation']
    score = metrics['settling_time']['robustness_score']
    print(f"{method_name}: CV={cv:.3f}, Robustness Score={score:.3f}")
```

**Robustness Ranking:**
```python
ranking = robustness['robustness_ranking']
print(f"Most robust: {ranking[0][0]} (score={ranking[0][1]:.3f})")
```

---

### 4.5 Efficiency Comparison

**Computational Efficiency Metrics:**

1. **Mean Computation Time:** Average time per simulation
2. **Peak Memory Usage:** Maximum memory required
3. **Scalability:** Time vs. problem size

**Implementation:**
```python
config = BenchmarkConfig(
    measure_computation_time=True,
    measure_memory_usage=True
)
```

**Trade-off Analysis:**

```
Performance vs. Efficiency:

High Performance, High Cost: Advanced control (MPC, adaptive)
High Performance, Low Cost:  Well-tuned classical control (ideal!)
Low Performance, Low Cost:   Simple control (acceptable for some)
Low Performance, High Cost:  Poorly designed (avoid!)
```

**Pareto Front:**

Identify non-dominated solutions:
- Method A: 2.0s settling time, 10ms compute time
- Method B: 1.8s settling time, 50ms compute time ← Pareto optimal
- Method C: 2.5s settling time, 60ms compute time ← Dominated by A

---

### 4.6 Ranking Methodologies

#### 4.6.1 Single-Metric Ranking

**Simplest:** Rank by primary metric mean.

```python
# example-metadata:
# runnable: false

ranking = result.data['performance_comparison']['method_ranking']
# [(method1, score1), (method2, score2), ...]
```

---

#### 4.6.2 Borda Count (Multi-Metric)

**Description:** Aggregate rankings across multiple metrics.

**Algorithm:**

1. Rank methods for each metric
2. Assign points: 1st place = n-1 points, 2nd = n-2, ..., last = 0
3. Sum points across all metrics
4. Final ranking by total points

**Example:**

| Method | Metric A Rank | Metric B Rank | Metric C Rank | Total |
|--------|---------------|---------------|---------------|-------|
| A      | 1 (2pts)      | 2 (1pt)       | 1 (2pts)      | 5     |
| B      | 2 (1pt)       | 1 (2pts)      | 3 (0pts)      | 3     |
| C      | 3 (0pts)      | 3 (0pts)      | 2 (1pt)       | 1     |

**Winner:** Method A (highest Borda count)

**Implementation:**
```python
ranking = result.data['ranking_analysis']
borda_scores = ranking['borda_scores']
final_ranking = ranking['final_ranking']
```

---

#### 4.6.3 Weighted Score

**Description:** Weight metrics by importance.

```
Score = Σᵢ wᵢ × normalized_metricᵢ

where Σᵢ wᵢ = 1
```

**Example:**

Safety-critical application:
- w_settling = 0.3
- w_overshoot = 0.5 (most important - avoid constraint violation)
- w_control_effort = 0.2

**Implementation:**
```python
weights = {'settling_time': 0.3, 'overshoot': 0.5, 'control_effort': 0.2}

# Compute weighted scores
for method_name, method_data in performance_data.items():
    score = sum(weights[m] * normalize(method_data[m]) for m in weights)
```

---

## 5. Uncertainty Quantification

### 5.1 Overview

Quantify uncertainty in performance predictions and stability guarantees.

**Sources of Uncertainty:**
1. **Aleatory (Stochastic):** Inherent randomness (disturbances, noise)
2. **Epistemic (Knowledge):** Model uncertainty, parameter uncertainty

---

### 5.2 Confidence Intervals

**Parametric CI (Normal Assumption):**

```
CI = x̄ ± t_{α/2,n-1} × (s/√n)

where t_{α/2,n-1} = Student's t critical value
```

**Bootstrap CI (Non-Parametric):**

1. Resample data with replacement B times
2. Compute statistic for each bootstrap sample
3. CI = [θ̂*_{α/2}, θ̂*_{1-α/2}] (percentile method)

**Interpretation:**

"95% CI [2.1, 2.5]": We are 95% confident true mean settling time is between 2.1s and 2.5s.

**Control System Context:**

- **Narrow CI:** Predictable performance, safe for deployment
- **Wide CI:** High uncertainty, need more data or better control

**Safety Margin:**

For safety-critical applications:
```
Worst-case bound = Upper CI + k×σ

where k = 3 for 99.7% coverage (3-sigma)
```

---

### 5.3 Distribution Fitting and Goodness-of-Fit

**Why Fit Distributions?**

1. Predict probabilities: P(settling time > 3s) = ?
2. Generate synthetic scenarios
3. Formal probabilistic guarantees

**Fitted Distribution Usage:**

```python
# After fitting (e.g., lognormal)
from scipy import stats

dist_params = result.data['distribution_analysis']['distribution_fits']['lognormal']['parameters']
dist = stats.lognorm(*dist_params)

# Probability of exceeding threshold
prob_exceed = 1 - dist.cdf(threshold)
print(f"P(settling time > 3s) = {prob_exceed:.4f}")

# 95th percentile
percentile_95 = dist.ppf(0.95)
print(f"95% of scenarios have settling time < {percentile_95:.2f}s")
```

---

### 5.4 Risk Analysis for Safety-Critical Systems

**Value at Risk (VaR):**

```
VaR_α = inf{x : P(X ≤ x) ≥ α}

Example: VaR₀.₀₅ = worst-case performance for bottom 5% of scenarios
```

**Conditional Value at Risk (CVaR / Expected Shortfall):**

```
CVaR_α = E[X | X ≤ VaR_α]

Average performance in worst α% of cases
```

**Implementation:**
```python
risk_analysis = result.data['risk_analysis']

var_5 = risk_analysis['value_at_risk']['var_5']    # 5th percentile
cvar_5 = risk_analysis['conditional_value_at_risk']['cvar_5']

print(f"Worst 5% scenarios: VaR={var_5:.2f}, CVaR={cvar_5:.2f}")
```

**Safety Validation:**

For a controller to be "safe", require:
```
P(performance < acceptable_threshold) < ε

where ε = acceptable failure rate (e.g., 10⁻⁶ for aviation)
```

**Extreme Value Analysis:**

For rare but catastrophic events:
```python
extreme = risk_analysis['extreme_value_analysis']

# 100-year return level
return_100 = extreme['return_levels']['100_year']
print(f"Once-in-100-scenarios worst-case: {return_100:.2f}")
```

---

## 6. Integration with Control Systems

### 6.1 Validation Workflow for New Controllers

**Standard Validation Protocol:**

```
1. **Unit Testing:**
   ├─ Verify implementation correctness
   └─ Test edge cases (saturation, singular configurations)

2. **Monte Carlo Validation:**
   ├─ Parameter uncertainty propagation
   ├─ LHS sampling (500-1000 samples)
   ├─ Check convergence
   └─ Distribution fitting

3. **Cross-Validation:**
   ├─ Time series CV (respect temporal order)
   ├─ Test on held-out scenarios
   └─ Bias-variance analysis

4. **Statistical Testing:**
   ├─ Normality of residuals
   ├─ Stationarity of performance
   └─ Hypothesis tests vs. baseline

5. **Benchmark Comparison:**
   ├─ Compare with state-of-the-art
   ├─ Statistical significance + effect size
   └─ Robustness comparison

6. **Uncertainty Quantification:**
   ├─ Confidence intervals
   ├─ Risk analysis (VaR, CVaR)
   └─ Extreme value analysis

7. **Production Validation:**
   ├─ Hardware-in-the-loop testing
   ├─ Field trials
   └─ Long-term monitoring
```

---

### 6.2 PSO Hyperparameter Validation

**Challenge:** Ensure PSO-tuned parameters generalize beyond training scenarios.

**Validation Approach:**

1. **Training Scenarios:** Nominal operating conditions
2. **Validation Scenarios:** Hold-out scenarios (10-20% of total)
3. **Test Scenarios:** Completely new conditions

**Cross-Validation for PSO:**

```python
from src.analysis.validation.cross_validation import CrossValidator

validator = CrossValidator(CrossValidationConfig(
    cv_method="monte_carlo",
    n_repetitions=50,
    test_ratio=0.2
))

# For each CV split:
#   - Optimize on training scenarios
#   - Evaluate on test scenarios
#   - Record generalization gap

cv_result = validator.validate(
    scenarios,
    models=[pso_optimized_controller],
    prediction_function=simulate_controller
)

generalization_gap = (
    cv_result.data['monte_carlo_validation']['training_score'] -
    cv_result.data['monte_carlo_validation']['test_score']
)

if generalization_gap > threshold:
    print("⚠ Overfitting detected - need more diverse training scenarios")
```

---

### 6.3 Adaptive Controller Validation

**Challenge:** Performance depends on online adaptation - cannot use standard CV.

**Time Series Validation Approach:**

```python
config = CrossValidationConfig(
    cv_method="time_series",
    n_splits=5,
    max_train_size=100,  # Limit adaptation window
    gap=10               # Predict 10 steps ahead
)

# Each fold:
#   - Controller adapts on [t₀, t₁]
#   - Performance evaluated on [t₁+gap, t₂]
```

**Metrics:**
- **Adaptation Speed:** Time to converge after disturbance
- **Tracking Performance:** RMSE during adaptation vs. after
- **Stability Margin:** Lyapunov function or gain margin

---

### 6.4 Real-Time Control Validation

**Timing Validation:**

```python
# example-metadata:
# runnable: false

config = BenchmarkConfig(
    measure_computation_time=True,
    n_trials=100
)

benchmark = BenchmarkSuite(config)
result = benchmark.validate(...)

comp_time = result.data['simulation_benchmarks'][scenario][method]['computational_analysis']

mean_time = comp_time['mean_computation_time']
std_time = comp_time['std_computation_time']
worst_case_time = comp_time['mean_computation_time'] + 3*comp_time['std_computation_time']

if worst_case_time < control_period:
    print("✓ Real-time feasible")
else:
    print("✗ Timing constraint violated")
```

**Jitter Analysis:**

```
Jitter = std(computation_time)

Low jitter (<10% of mean) → Predictable timing
High jitter (>30% of mean) → May cause control instability
```

---

## 7. Best Practices and Pitfalls

### 7.1 Common Pitfalls

#### Pitfall 1: Insufficient Sample Size

**Problem:** Low statistical power → cannot detect real differences.

**Solution:**
```python
# Always check power
result = suite.validate(data, test_types=['power_analysis'])
if not result.data['power_analysis']['power_adequate']:
    print(f"⚠ Need {result.data['power_analysis']['recommended_sample_size']} samples")
```

---

#### Pitfall 2: Multiple Testing Without Correction

**Problem:** 20 tests at α=0.05 → expect 1 false positive.

**Solution:**
```python
config = StatisticalTestConfig(
    multiple_comparisons_correction="holm"  # Use Holm or FDR
)
```

---

#### Pitfall 3: Confusing Statistical and Practical Significance

**Problem:** p < 0.05 doesn't mean the effect is important.

**Solution:**
```python
# Always report effect size
if test['p_value'] < 0.05:
    effect_size = compute_cohens_d(group1, group2)
    if abs(effect_size) < 0.2:
        print("⚠ Statistically significant but negligible effect")
```

---

#### Pitfall 4: Inappropriate Cross-Validation for Time Series

**Problem:** Using standard K-fold on time series leaks future information.

**Solution:**
```python
# For time series, ALWAYS use time series CV
config = CrossValidationConfig(
    cv_method="time_series"  # Not "k_fold"!
)
```

---

#### Pitfall 5: Ignoring Non-Normality

**Problem:** Using t-tests on non-normal data → invalid conclusions.

**Solution:**
```python
# Check normality first
normality = suite.validate(data, test_types=['normality_tests'])

if normality_rejected:
    # Use non-parametric test
    use_mann_whitney_u()  # Instead of t-test
    # OR transform data
    log_data = np.log(data)
    # OR use bootstrap CI
```

---

#### Pitfall 6: Overfitting in Parameter Tuning

**Problem:** PSO optimizes perfectly for training scenarios, fails on new ones.

**Solution:**
```python
# Use nested CV for unbiased evaluation
config = CrossValidationConfig(
    enable_nested_cv=True,
    n_splits=5,
    inner_cv_splits=3
)
```

---

### 7.2 Best Practices

#### Practice 1: Pre-Register Analysis Plan

**Before collecting data:**
1. State hypotheses
2. Choose metrics
3. Select statistical tests
4. Determine sample size (power analysis)

**Prevents:** p-hacking, HARKing (Hypothesizing After Results Known)

---

#### Practice 2: Report Effect Sizes

**Always report:**
- Test statistic
- p-value
- **Effect size** (Cohen's d, η², etc.)
- Confidence intervals

**Example:**
```
"Controller A achieved 20% faster settling time than B (2.0s vs. 2.5s),
t(58)=3.2, p=0.002, d=0.85 (large effect), 95% CI [0.2s, 0.8s]"
```

---

#### Practice 3: Visualize Uncertainty

**Don't just report means:**
```python
import matplotlib.pyplot as plt

# Show distribution, not just mean
plt.violinplot([data_A, data_B])
plt.boxplot([data_A, data_B])

# Show confidence intervals
plt.errorbar(x, means, yerr=confidence_intervals)
```

---

#### Practice 4: Use Multiple Validation Methods

**Triangulation:** Converging evidence from multiple methods increases confidence.

```python
# example-metadata:
# runnable: false

# Monte Carlo + Cross-Validation + Statistical Tests
mc_result = mc_analyzer.validate(...)
cv_result = cv_validator.validate(...)
stat_result = stat_suite.validate(...)

# If all agree → high confidence
# If diverge → investigate why
```

---

#### Practice 5: Document Assumptions

**Always state:**
- Distributional assumptions (normality, etc.)
- Independence assumptions
- Stationarity assumptions
- Validation of assumptions

**Example:**
```
"Assumption: Tracking errors are normally distributed.
Validation: Shapiro-Wilk test W=0.98, p=0.23 → cannot reject normality."
```

---

### 7.3 Checklist for Publication-Ready Validation

**Monte Carlo:**
- [ ] Appropriate sampling method for dimensionality
- [ ] Convergence analysis performed
- [ ] Sample size justified (power analysis)
- [ ] Sensitivity analysis included
- [ ] Random seed reported for reproducibility

**Cross-Validation:**
- [ ] Method appropriate for data structure (time series vs. i.i.d.)
- [ ] Sufficient folds/repetitions
- [ ] Metrics aligned with application
- [ ] Bias-variance analysis performed
- [ ] Overfitting assessed

**Statistical Testing:**
- [ ] Assumptions validated (normality, etc.)
- [ ] Appropriate test selected
- [ ] Multiple comparison correction applied
- [ ] Effect sizes reported
- [ ] Confidence intervals provided

**Benchmark Comparison:**
- [ ] Fair comparison (same scenarios, resources)
- [ ] Statistical significance tested
- [ ] Robustness compared
- [ ] Computational cost reported
- [ ] Limitations discussed

**Uncertainty Quantification:**
- [ ] Confidence intervals for all estimates
- [ ] Distribution fitting with goodness-of-fit tests
- [ ] Risk analysis for safety-critical applications
- [ ] Sensitivity to assumptions analyzed

**Reproducibility:**
- [ ] Random seeds documented
- [ ] Software versions listed
- [ ] Hyperparameters reported
- [ ] Data availability statement
- [ ] Code archived (e.g., GitHub, Zenodo)

---

## References

**Monte Carlo Methods:**
1. Kroese, D.P., et al. "Why the Monte Carlo method is so important today." WIREs Computational Statistics, 2014.
2. McKay, M.D., et al. "A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code." Technometrics, 1979. [Latin Hypercube Sampling]

**Cross-Validation:**
3. Hastie, T., Tibshirani, R., Friedman, J. "The Elements of Statistical Learning." Springer, 2009. [Chapter 7: Model Assessment and Selection]
4. Bergmeir, C., Benítez, J.M. "On the use of cross-validation for time series predictor evaluation." Information Sciences, 2012.

**Statistical Testing:**
5. Razali, N.M., Wah, Y.B. "Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests." Journal of Statistical Modeling and Analytics, 2011.
6. Benjamini, Y., Hochberg, Y. "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing." JRSS-B, 1995.

**Effect Size:**
7. Cohen, J. "Statistical Power Analysis for the Behavioral Sciences." 2nd ed., 1988.
8. Sullivan, G.M., Feinn, R. "Using Effect Size—or Why the P Value Is Not Enough." Journal of Graduate Medical Education, 2012.

**Uncertainty Quantification:**
9. Smith, R.C. "Uncertainty Quantification: Theory, Implementation, and Applications." SIAM, 2013.
10. Saltelli, A., et al. "Global Sensitivity Analysis: The Primer." Wiley, 2008.

**Control Systems:**
11. Åström, K.J., Murray, R.M. "Feedback Systems: An Introduction for Scientists and Engineers." Princeton, 2008.
12. Ljung, L. "System Identification: Theory for the User." Prentice Hall, 1999. [Cross-validation for model selection]

---

## Related Documentation

**Phase 2 (Theory Foundations):**
- [SMC Theory Complete](../theory/smc_theory_complete.md) - Control-theoretic foundations
- [PSO Optimization Complete](../theory/pso_optimization_complete.md) - Optimization theory
- [Lyapunov Stability Analysis](../theory/lyapunov_stability_analysis.md) - Stability guarantees

**Phase 3.1-3.2 (Performance Analysis):**
- [Controller Performance Benchmarks](../benchmarks/controller_performance_benchmarks.md) - Benchmark results
- [Performance Benchmarking Guide](../testing/guides/performance_benchmarking.md) - Testing protocols

**API References:**
- [Monte Carlo Analyzer](../reference/analysis/validation_monte_carlo.md)
- [Cross Validator](../reference/analysis/validation_cross_validation.md)
- [Statistical Test Suite](../reference/analysis/validation_statistical_tests.md)
- [Benchmark Suite](../reference/analysis/validation_benchmarking.md)

---

**Document Metadata:**
- **Implementation:** `src/analysis/validation/` (4 modules, 3,777 lines)
- **Examples:** See `docs/validation/validation_examples.md`
- **Workflow:** See `docs/validation/validation_workflow.md`
- **API Reference:** See `docs/validation/api_reference.md`
- **Version:** Phase 3.3 Completion
- **Last Updated:** 2025-10-07
