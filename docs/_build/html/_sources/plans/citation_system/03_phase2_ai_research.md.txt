# Citation System - Phase 2: AI Research Automation

**Document Version:** 1.0.0
**Created:** 2025-01-15
**Status:** Planning Phase
**Estimated Duration:** Week 3-4 (25-35 hours)



## Phase Overview

**Objective:** Deploy AI-powered research agents to find academic references for 500+ extracted claims.

**Input:** `artifacts/claims_inventory.json` (from Phase 1)
**Output:** 150-200 verified BibTeX entries with claim→citation mapping



## Technology Stack

### API Integrations

#### 1. Semantic Scholar API

- **Rate Limit:** 100 requests per 5 minutes
- **Coverage:** Computer science, control theory, robotics
- **Features:** Citation counts, influential citations, related papers
- **Endpoint:** `https://api.semanticscholar.org/graph/v1/paper/search`

#### 2. ArXiv API

- **Rate Limit:** 3 requests per second
- **Coverage:** Physics, mathematics, control systems
- **Features:** Full-text access, version tracking
- **Endpoint:** `http://export.arxiv.org/api/query`

#### 3. CrossRef API

- **Rate Limit:** None (polite usage recommended)
- **Coverage:** DOI resolution, metadata retrieval
- **Features:** Journal articles, conference papers
- **Endpoint:** `https://api.crossref.org/works`



## Tool Development

### 1. API Clients (`scripts/citations/api_clients.py`)

**Estimated Effort:** 400 lines, 12 hours

```python
# example-metadata:
# runnable: false

class SemanticScholarClient:
    """Semantic Scholar API client with rate limiting."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = RateLimiter(max_requests=100, period=300)

    async def search_papers(
        self,
        query: str,
        fields: List[str],
        limit: int = 10
    ) -> List[Dict]:
        \"\"\"Search papers with exponential backoff.\"\"\"
        await self.rate_limiter.acquire()
        # Implementation with retry logic

class ArXivClient:
    \"\"\"ArXiv API client with rate limiting.\"\"\"

    def __init__(self):
        self.rate_limiter = RateLimiter(max_requests=3, period=1)

    async def search_papers(
        self,
        query: str,
        max_results: int = 10
    ) -> List[Dict]:
        \"\"\"Search ArXiv with rate limiting.\"\"\"
        await self.rate_limiter.acquire()
        # Implementation
```

**Features:**
- Exponential backoff for rate limit violations
- Async/await for parallel requests
- Response caching (60-minute TTL)
- Error handling and logging



### 2. Research Pipeline (`scripts/citations/research_pipeline.py`)

**Estimated Effort:** 500 lines, 15 hours

**Workflow:**
1. Load claims inventory
2. Generate search queries for each claim
3. Execute parallel API searches (respecting rate limits)
4. Rank results by relevance
5. Validate DOIs (HTTP 200 checks)
6. Generate claim→citation mapping

```python
# example-metadata:
# runnable: false

class ResearchPipeline:
    \"\"\"Automated research pipeline for citation discovery.\"\"\"

    def __init__(
        self,
        semantic_scholar: SemanticScholarClient,
        arxiv: ArXivClient,
        crossref: CrossRefClient
    ):
        self.clients = [semantic_scholar, arxiv, crossref]

    async def research_claim(
        self,
        claim: Claim
    ) -> List[Reference]:
        \"\"\"Research a single claim across all APIs.\"\"\"
        query = self.generate_query(claim)

        # Parallel search
        results = await asyncio.gather(*[
            client.search_papers(query)
            for client in self.clients
        ])

        # Rank and deduplicate
        ranked = self.rank_by_relevance(results)

        return ranked[:5]  # Top 5 per claim
```

**Ranking Algorithm:**
- **Citation count** (40% weight) - Academic impact
- **Venue quality** (30% weight) - Conference/journal tier
- **Recency** (20% weight) - Recent papers preferred
- **Keyword match** (10% weight) - Query relevance



### 3. BibTeX Generator (`scripts/citations/bibtex_generator.py`)

**Estimated Effort:** 250 lines, 8 hours

```python
# example-metadata:
# runnable: false

class BibTeXGenerator:
    \"\"\"Generate BibTeX entries from API results.\"\"\"

    def __init__(self, existing_bib: Path):
        self.existing = self.load_existing(existing_bib)

    def generate_entry(
        self,
        paper: Paper,
        citation_key: str
    ) -> str:
        \"\"\"Generate IEEE-style BibTeX entry.\"\"\"
        return f\"\"\"@{paper.entry_type}{{{citation_key},
    title = {{{paper.title}}},
    author = {{{paper.authors}}},
    year = {{{paper.year}}},
    doi = {{{paper.doi}}},
    {self.format_venue(paper)}
}}\"\"\"
```

**Features:**
- IEEE citation style enforcement
- Duplicate detection (DOI-based)
- Merge with existing 39 references
- Citation key generation (AuthorYear format)



## Deliverables

### 1. Research Results Database

**File:** `artifacts/research_results.json`

```json
{
  "claim_id": "CLAIM_001",
  "claim_text": "Super-twisting algorithm eliminates chattering",
  "references": [
    {
      "doi": "10.1109/TAC.2001.964620",
      "title": "Super-twisting algorithm for second-order sliding mode",
      "authors": ["Levant, A."],
      "year": 2001,
      "venue": "IEEE Transactions on Automatic Control",
      "citation_count": 2847,
      "relevance_score": 0.95
    }
  ]
}
```

### 2. Enhanced Bibliography

**File:** `docs/references/enhanced_bibliography.bib`

150-200 BibTeX entries in IEEE format

### 3. Citation Mapping

**File:** `artifacts/citation_mapping.json`

```json
{
  "CLAIM_001": ["Levant2001", "Moreno2012"],
  "CLAIM_002": ["Utkin1999", "Edwards1998"]
}
```

### 4. Research Quality Report

**File:** `artifacts/research_quality_report.json`

```json
{
  "total_claims": 512,
  "claims_with_citations": 438,
  "coverage_percentage": 85.5,
  "total_references": 187,
  "avg_citations_per_claim": 2.3,
  "doi_validation_success": 96.8
}
```



## Acceptance Criteria

| Metric | Target | Validation |
|--------|--------|------------|
| **References Generated** | 150-200 | Count unique DOIs |
| **Claim Coverage** | ≥85% | claims_with_citations / total_claims |
| **DOI Accessibility** | ≥95% | HTTP 200 for DOI resolution |
| **Duplicate Rate** | <5% | Overlap with existing 39 refs |
| **Avg. Citations/Reference** | >50 | Semantic Scholar citation counts |
| **Venue Quality** | >70% top-tier | IEEE/Springer/Elsevier conferences/journals |



## Execution Plan

### Week 3: API Integration (12 hours)

1. Implement `api_clients.py` with rate limiting
2. Test API connections and rate limits
3. Implement caching and error handling
4. Unit tests for API clients

### Week 4: Research Pipeline (13 hours)

1. Implement `research_pipeline.py`
2. Develop ranking algorithm
3. Implement DOI validation
4. Generate research results database

### Week 4: BibTeX Generation (10 hours)

1. Implement `bibtex_generator.py`
2. IEEE citation style formatting
3. Duplicate detection and merging
4. Generate enhanced bibliography



## Risk Management

| Risk | Mitigation |
|------|-----------|
| **API rate limits** | Exponential backoff, distributed execution |
| **Low-quality results** | Multi-source validation, manual review of top 50 |
| **Broken DOIs** | CrossRef validation, fallback to URL |
| **Citation style inconsistency** | Automated formatting, style guide validation |



## Dependencies

- **Phase 1 Complete:** `artifacts/claims_inventory.json` available
- **Python Libraries:** `aiohttp`, `bibtexparser`, `crossrefapi`, `arxiv`
- **API Keys:** Semantic Scholar API key (optional, higher rate limit)



## Next Phase

**Phase 3:** [Citation Integration](./04_phase3_citation_integration.md)



**Related Documents:**
- [Master Roadmap](./00_master_roadmap.md)
- [Phase 1: Claim Extraction](./02_phase1_claim_extraction.md)
