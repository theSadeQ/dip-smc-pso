#==========================================================================================\\\
#============= docs/factory/enhanced_pso_integration_guide.md =================\\\
#==========================================================================================\\\

# Enhanced PSO Integration Guide
## Complete Workflow Specifications for Controller Optimization

### Overview

This comprehensive guide documents the complete integration between the Enhanced Controller Factory and Particle Swarm Optimization (PSO) workflows for automated controller parameter tuning. The integration provides validated, thread-safe, mathematically rigorous optimization capabilities for all SMC controller variants.

## PSO Integration Architecture

### System Overview

```
                    ┌─────────────────────────────────────────┐
                    │           PSO Engine                    │
                    │  ┌─────────────┐ ┌─────────────────┐   │
                    │  │ Population  │ │ Velocity Update │   │
                    │  │ Management  │ │ & Position      │   │
                    │  └─────────────┘ └─────────────────┘   │
                    └──────────────┬──────────────────────────┘
                                   │ Candidate Gains
                                   ▼
                    ┌─────────────────────────────────────────┐
                    │        Factory-PSO Bridge              │
                    │  ┌─────────────┐ ┌─────────────────┐   │
                    │  │ Gain        │ │ Controller      │   │
                    │  │ Validation  │ │ Instantiation   │   │
                    │  └─────────────┘ └─────────────────┘   │
                    └──────────────┬──────────────────────────┘
                                   │ Controller Instance
                                   ▼
                    ┌─────────────────────────────────────────┐
                    │       Simulation & Evaluation          │
                    │  ┌─────────────┐ ┌─────────────────┐   │
                    │  │ DIP Plant   │ │ Performance     │   │
                    │  │ Simulation  │ │ Metrics         │   │
                    │  └─────────────┘ └─────────────────┘   │
                    └──────────────┬──────────────────────────┘
                                   │ Fitness Score
                                   ▼
                    ┌─────────────────────────────────────────┐
                    │         PSO Update                      │
                    │     (Fitness-Guided Search)             │
                    └─────────────────────────────────────────┘
```

### Core Integration Components

#### 1. PSOFactoryInterface

**Specialized interface providing PSO-optimized controller creation:**

```python
# example-metadata:
# runnable: false

class PSOFactoryInterface:
    """
    High-performance interface for PSO optimization workflows.

    Features:
    - Thread-safe parallel optimization
    - Automatic gain validation and bounds checking
    - Performance monitoring and diagnostics
    - Fallback mechanisms for invalid parameter sets
    """

    def __init__(self, controller_type: str, simulation_config: Any):
        self.controller_type = controller_type
        self.config = simulation_config
        self._initialize_pso_environment()

    def _initialize_pso_environment(self) -> None:
        """Setup PSO optimization environment with all requirements."""

        # Controller specifications
        self.registry_info = CONTROLLER_REGISTRY[self.controller_type]
        self.n_gains = self.registry_info['gain_count']
        self.default_gains = self.registry_info['default_gains']

        # PSO bounds (mathematically derived)
        self.bounds_lower, self.bounds_upper = get_gain_bounds_for_pso(
            SMCType(self.controller_type)
        )

        # Performance tracking
        self.metrics = {
            'total_evaluations': 0,
            'successful_evaluations': 0,
            'validation_failures': 0,
            'simulation_failures': 0,
            'best_fitness': float('inf'),
            'average_fitness': 0.0
        }

        # Thread safety
        self._evaluation_lock = threading.RLock()
```

#### 2. PSO Controller Wrapper

**Optimized wrapper providing PSO-compatible interface:**

```python
# example-metadata:
# runnable: false

class PSOControllerWrapper:
    """
    PSO-optimized controller wrapper with comprehensive validation.

    Provides:
    - Simplified control interface for fitness evaluation
    - Automatic gain validation with controller-specific rules
    - Performance monitoring and error handling
    - Thread-safe operation for parallel PSO
    """

    def __init__(self, controller: Any, controller_type: str, validation_config: Dict[str, Any]):
        self.controller = controller
        self.controller_type = controller_type
        self.validation_config = validation_config

        # PSO-required attributes
        self.n_gains = CONTROLLER_REGISTRY[controller_type]['gain_count']
        self.max_force = getattr(controller, 'max_force', 150.0)

        # Performance tracking
        self.control_calls = 0
        self.control_failures = 0
        self.last_control_time = 0.0

    def validate_gains(self, particles: np.ndarray) -> np.ndarray:
        """
        Vectorized gain validation for PSO particle swarms.

        Args:
            particles: Array of shape (n_particles, n_gains)

        Returns:
            Boolean mask indicating valid particles
        """
        if particles.ndim == 1:
            particles = particles.reshape(1, -1)

        valid_mask = np.ones(particles.shape[0], dtype=bool)

        # Basic validation
        for i, gains in enumerate(particles):
            try:
                # Check gain count
                if len(gains) != self.n_gains:
                    valid_mask[i] = False
                    continue

                # Check for finite positive values
                if not all(np.isfinite(g) and g > 0 for g in gains):
                    valid_mask[i] = False
                    continue

                # Controller-specific validation
                if not self._validate_controller_specific_constraints(gains):
                    valid_mask[i] = False
                    continue

            except Exception:
                valid_mask[i] = False

        return valid_mask

    def _validate_controller_specific_constraints(self, gains: List[float]) -> bool:
        """Apply mathematical constraints for each controller type."""

        if self.controller_type == 'classical_smc':
            # Classical SMC: All gains positive, reasonable ranges
            k1, k2, lam1, lam2, K, kd = gains
            return all(g > 0 for g in gains[:5]) and kd >= 0

        elif self.controller_type == 'sta_smc':
            # Super-Twisting: Critical stability condition K1 > K2
            K1, K2 = gains[0], gains[1]
            return K1 > K2 > 0 and all(g > 0 for g in gains[2:])

        elif self.controller_type == 'adaptive_smc':
            # Adaptive SMC: Adaptation rate bounds
            k1, k2, lam1, lam2, gamma = gains
            return all(g > 0 for g in gains[:4]) and 0.1 <= gamma <= 20.0

        elif self.controller_type == 'hybrid_adaptive_sta_smc':
            # Hybrid SMC: Surface parameters positive
            return all(g > 0 for g in gains)

        return True

    def compute_control(self, state: np.ndarray) -> np.ndarray:
        """
        PSO-compatible control computation with error handling.

        Args:
            state: System state vector [θ₁, θ₂, x, θ̇₁, θ̇₂, ẋ]

        Returns:
            Control output as numpy array
        """
        try:
            self.control_calls += 1
            start_time = time.time()

            # Validate input state
            if len(state) != 6:
                raise ValueError(f"Expected 6-element state, got {len(state)}")

            # Call underlying controller
            result = self.controller.compute_control(state, {}, {})

            # Extract control value
            if hasattr(result, 'u'):
                u = result.u
            elif isinstance(result, dict) and 'u' in result:
                u = result['u']
            else:
                u = result

            # Apply saturation and return as array
            u_sat = np.clip(float(u), -self.max_force, self.max_force)

            # Performance tracking
            self.last_control_time = time.time() - start_time

            return np.array([u_sat])

        except Exception as e:
            self.control_failures += 1
            # Return safe fallback control
            return np.array([0.0])
```

## PSO Optimization Workflows

### 1. Standard PSO Optimization Workflow

**Complete workflow for single-controller optimization:**

```python
def optimize_smc_controller_pso(
    controller_type: str,
    simulation_config: Any,
    pso_config: Dict[str, Any],
    optimization_objectives: List[str]
) -> Dict[str, Any]:
    """
    Complete PSO optimization workflow for SMC controllers.

    Args:
        controller_type: SMC controller type ('classical_smc', etc.)
        simulation_config: Plant and simulation parameters
        pso_config: PSO algorithm parameters
        optimization_objectives: List of objectives ['ise', 'overshoot', 'settling_time']

    Returns:
        Optimization results with best gains and performance metrics
    """

    # 1. Initialize PSO-Factory Interface
    pso_interface = PSOFactoryInterface(controller_type, simulation_config)

    # 2. Setup PSO Algorithm
    from pyswarms.single import GlobalBestPSO

    # PSO parameters with adaptive bounds
    bounds = (
        np.array(pso_interface.bounds_lower),
        np.array(pso_interface.bounds_upper)
    )

    optimizer = GlobalBestPSO(
        n_particles=pso_config.get('n_particles', 30),
        dimensions=pso_interface.n_gains,
        options={
            'c1': pso_config.get('c1', 2.0),  # Cognitive component
            'c2': pso_config.get('c2', 2.0),  # Social component
            'w': pso_config.get('w', 0.9)     # Inertia weight
        },
        bounds=bounds
    )

    # 3. Define Fitness Function
    def fitness_function(particles: np.ndarray) -> np.ndarray:
        """
        Vectorized fitness evaluation for PSO particles.

        Args:
            particles: Array of shape (n_particles, n_gains)

        Returns:
            Fitness scores for each particle
        """
        fitness_scores = []

        for gains in particles:
            try:
                # Create controller with current gains
                controller_factory = pso_interface.create_pso_controller_factory()
                controller = controller_factory(gains)

                # Validate gains
                if not controller.validate_gains(gains.reshape(1, -1))[0]:
                    fitness_scores.append(1000.0)  # Penalty for invalid gains
                    continue

                # Run simulation
                simulation_result = run_simulation_with_controller(
                    controller, simulation_config
                )

                # Compute multi-objective fitness
                fitness = compute_multi_objective_fitness(
                    simulation_result, optimization_objectives
                )

                fitness_scores.append(fitness)

            except Exception as e:
                # Penalty for simulation failures
                fitness_scores.append(1000.0)

        return np.array(fitness_scores)

    # 4. Run PSO Optimization
    best_cost, best_gains = optimizer.optimize(
        fitness_function,
        iters=pso_config.get('iters', 100),
        verbose=True
    )

    # 5. Validate and Return Results
    validation_result = validate_optimization_result(
        best_gains, best_cost, controller_type, simulation_config
    )

    return {
        'best_gains': best_gains.tolist(),
        'best_fitness': float(best_cost),
        'controller_type': controller_type,
        'optimization_history': optimizer.cost_history,
        'validation_result': validation_result,
        'pso_metrics': pso_interface.metrics
    }
```

### 2. Multi-Objective PSO Optimization

**Advanced workflow for simultaneous optimization of multiple objectives:**

```python
# example-metadata:
# runnable: false

def multi_objective_pso_optimization(
    controller_types: List[str],
    simulation_config: Any,
    objectives: Dict[str, float],  # {'ise': 0.4, 'overshoot': 0.3, 'energy': 0.3}
    pso_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Multi-objective PSO optimization across multiple controller types.

    Features:
    - Simultaneous optimization of multiple performance metrics
    - Pareto-optimal solution discovery
    - Controller comparison and ranking
    - Robust constraint handling
    """

    results = {}
    pareto_solutions = []

    for controller_type in controller_types:
        print(f"Optimizing {controller_type}...")

        # Single-objective optimization for baseline
        single_result = optimize_smc_controller_pso(
            controller_type, simulation_config, pso_config,
            list(objectives.keys())
        )

        results[controller_type] = single_result

        # Extract Pareto solutions
        pareto_solutions.extend(
            extract_pareto_solutions(single_result, objectives)
        )

    # Multi-objective analysis
    pareto_front = compute_pareto_front(pareto_solutions)
    controller_ranking = rank_controllers_by_objectives(results, objectives)

    return {
        'individual_results': results,
        'pareto_front': pareto_front,
        'controller_ranking': controller_ranking,
        'best_overall': select_best_overall_solution(results, objectives)
    }
```

### 3. Adaptive PSO with Dynamic Bounds

**Advanced PSO with self-tuning parameters:**

```python
# example-metadata:
# runnable: false

def adaptive_pso_optimization(
    controller_type: str,
    simulation_config: Any,
    adaptation_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Adaptive PSO with dynamic parameter adjustment.

    Features:
    - Dynamic PSO parameter adjustment based on convergence
    - Adaptive bounds tightening around promising regions
    - Early stopping with convergence detection
    - Exploration-exploitation balance optimization
    """

    # Initialize adaptive PSO
    adaptive_pso = AdaptivePSOOptimizer(
        controller_type=controller_type,
        config=adaptation_config
    )

    # Multi-stage optimization
    stages = [
        {'exploration_weight': 0.8, 'iterations': 50},   # Exploration phase
        {'exploration_weight': 0.5, 'iterations': 30},   # Balanced phase
        {'exploration_weight': 0.2, 'iterations': 20}    # Exploitation phase
    ]

    all_results = []

    for stage_idx, stage_config in enumerate(stages):
        print(f"PSO Stage {stage_idx + 1}: {stage_config}")

        # Adjust PSO parameters
        adaptive_pso.update_parameters(stage_config)

        # Run optimization stage
        stage_result = adaptive_pso.optimize_stage(
            simulation_config, stage_config['iterations']
        )

        all_results.append(stage_result)

        # Check for early convergence
        if adaptive_pso.check_convergence():
            print(f"Converged early at stage {stage_idx + 1}")
            break

    # Combine results
    final_result = adaptive_pso.combine_stage_results(all_results)

    return final_result
```

## Performance Optimization Strategies

### 1. Parallel PSO Evaluation

**Multi-threaded fitness evaluation for improved performance:**

```python
def parallel_fitness_evaluation(
    particles: np.ndarray,
    controller_factory: Callable,
    simulation_config: Any,
    n_threads: int = 4
) -> np.ndarray:
    """
    Parallel fitness evaluation using thread pool.

    Significantly improves PSO performance for expensive simulations.
    """

    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time

    def evaluate_single_particle(gains: np.ndarray) -> float:
        """Evaluate fitness for single particle."""
        try:
            controller = controller_factory(gains)
            result = run_simulation_with_controller(controller, simulation_config)
            return compute_fitness(result)
        except Exception:
            return 1000.0  # Penalty for failures

    # Parallel execution
    with ThreadPoolExecutor(max_workers=n_threads) as executor:
        future_to_idx = {
            executor.submit(evaluate_single_particle, particle): idx
            for idx, particle in enumerate(particles)
        }

        fitness_scores = np.zeros(len(particles))

        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                fitness_scores[idx] = future.result()
            except Exception:
                fitness_scores[idx] = 1000.0

    return fitness_scores
```

### 2. Cached Simulation Results

**Intelligent caching to avoid redundant simulations:**

```python
# example-metadata:
# runnable: false

class SimulationCache:
    """
    Intelligent caching system for PSO optimization.

    Features:
    - Hash-based lookup for identical gain sets
    - LRU eviction for memory management
    - Cache hit/miss statistics
    - Persistent storage for long-running optimizations
    """

    def __init__(self, max_size: int = 1000, tolerance: float = 1e-6):
        self.cache = {}
        self.max_size = max_size
        self.tolerance = tolerance
        self.hits = 0
        self.misses = 0

    def get_cache_key(self, gains: np.ndarray) -> str:
        """Generate consistent cache key for gain arrays."""
        rounded_gains = np.round(gains / self.tolerance) * self.tolerance
        return hash(tuple(rounded_gains))

    def get(self, gains: np.ndarray) -> Optional[float]:
        """Retrieve cached fitness if available."""
        key = self.get_cache_key(gains)
        if key in self.cache:
            self.hits += 1
            return self.cache[key]
        self.misses += 1
        return None

    def put(self, gains: np.ndarray, fitness: float) -> None:
        """Store fitness result in cache."""
        if len(self.cache) >= self.max_size:
            # Remove oldest entry (simple LRU)
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]

        key = self.get_cache_key(gains)
        self.cache[key] = fitness

    def get_statistics(self) -> Dict[str, Any]:
        """Return cache performance statistics."""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0

        return {
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.cache)
        }
```

### 3. GPU-Accelerated Simulation

**GPU acceleration for large-scale PSO optimization:**

```python
def gpu_accelerated_pso_evaluation(
    particles: np.ndarray,
    controller_factory: Callable,
    simulation_config: Any
) -> np.ndarray:
    """
    GPU-accelerated fitness evaluation using CuPy/Numba.

    For very large swarm sizes (>100 particles), GPU acceleration
    can provide significant speedup.
    """

    try:
        import cupy as cp
        import numba.cuda as cuda

        # Transfer data to GPU
        gpu_particles = cp.asarray(particles)

        # GPU kernel for parallel simulation
        @cuda.jit
        def evaluate_particles_kernel(particles, fitness_scores):
            idx = cuda.grid(1)
            if idx < particles.shape[0]:
                # GPU-accelerated simulation logic
                fitness_scores[idx] = gpu_simulate_controller(particles[idx])

        # Allocate GPU memory
        gpu_fitness = cp.zeros(len(particles))

        # Launch GPU kernel
        threads_per_block = 256
        blocks_per_grid = (len(particles) + threads_per_block - 1) // threads_per_block

        evaluate_particles_kernel[blocks_per_grid, threads_per_block](
            gpu_particles, gpu_fitness
        )

        # Transfer results back to CPU
        return cp.asnumpy(gpu_fitness)

    except ImportError:
        # Fallback to CPU evaluation
        return parallel_fitness_evaluation(particles, controller_factory, simulation_config)
```

## Integration Best Practices

### 1. Parameter Bounds Specification

**Mathematically-derived bounds for optimal PSO performance:**

```python
# example-metadata:
# runnable: false

def get_optimized_pso_bounds(controller_type: str, plant_params: Dict[str, Any]) -> Tuple[List[float], List[float]]:
    """
    Compute optimized PSO bounds based on plant parameters and control theory.

    Uses stability margins and performance requirements to derive tight bounds.
    """

    if controller_type == 'classical_smc':
        # Classical SMC bounds based on stability analysis
        # Pole placement considerations for closed-loop stability

        max_damping = plant_params.get('max_damping_requirement', 0.7)
        settling_time = plant_params.get('settling_time_requirement', 2.0)

        # Derive bounds from desired closed-loop characteristics
        lambda_min = 4.0 / settling_time  # Natural frequency requirement
        lambda_max = 20.0  # Upper bound to prevent excessive control effort

        k_min = lambda_min / 2.0  # Position gain lower bound
        k_max = lambda_max * 2.0  # Position gain upper bound

        K_min = estimate_min_switching_gain(plant_params)
        K_max = plant_params.get('max_force', 150.0) * 0.8  # Conservative upper bound

        bounds_lower = [k_min, k_min, lambda_min, lambda_min, K_min, 0.0]
        bounds_upper = [k_max, k_max, lambda_max, lambda_max, K_max, 10.0]

    elif controller_type == 'sta_smc':
        # Super-Twisting bounds with stability constraint K1 > K2

        # Lyapunov-based design bounds
        L0 = estimate_lipschitz_constant(plant_params)

        K1_min = math.sqrt(L0) * 1.1  # Safety margin
        K1_max = math.sqrt(L0) * 5.0  # Conservative upper bound

        K2_min = L0 / (2 * math.sqrt(L0 - K1_min**2)) * 1.1
        K2_max = K1_max * 0.8  # Ensure K1 > K2 constraint

        bounds_lower = [K1_min, K2_min, 2.0, 2.0, 5.0, 5.0]
        bounds_upper = [K1_max, K2_max, 30.0, 30.0, 20.0, 20.0]

    elif controller_type == 'adaptive_smc':
        # Adaptive SMC bounds based on adaptation rate limits

        # Stability-preserving adaptation rate bounds
        gamma_min = 0.1  # Minimum for reasonable adaptation speed
        gamma_max = estimate_max_adaptation_rate(plant_params)  # Stability limit

        bounds_lower = [2.0, 2.0, 5.0, 5.0, gamma_min]
        bounds_upper = [40.0, 40.0, 25.0, 25.0, gamma_max]

    else:  # hybrid_adaptive_sta_smc
        # Hybrid controller bounds (conservative surface parameters)
        bounds_lower = [2.0, 2.0, 5.0, 5.0]
        bounds_upper = [30.0, 30.0, 20.0, 20.0]

    return bounds_lower, bounds_upper

def estimate_min_switching_gain(plant_params: Dict[str, Any]) -> float:
    """Estimate minimum switching gain based on disturbance bounds."""

    # Extract disturbance characteristics
    max_model_uncertainty = plant_params.get('model_uncertainty', 0.2)
    max_external_disturbance = plant_params.get('external_disturbance', 5.0)
    safety_margin = plant_params.get('safety_margin', 1.5)

    # Conservative estimate
    return (max_model_uncertainty + max_external_disturbance) * safety_margin

def estimate_lipschitz_constant(plant_params: Dict[str, Any]) -> float:
    """Estimate Lipschitz constant for STA design."""

    # Based on system nonlinearity and uncertainty bounds
    max_nonlinearity = plant_params.get('max_nonlinearity', 10.0)
    uncertainty_bound = plant_params.get('uncertainty_bound', 5.0)

    return max_nonlinearity + uncertainty_bound

def estimate_max_adaptation_rate(plant_params: Dict[str, Any]) -> float:
    """Estimate maximum stable adaptation rate."""

    # Based on parameter variation speed and system bandwidth
    system_bandwidth = plant_params.get('system_bandwidth', 10.0)  # rad/s
    parameter_variation_rate = plant_params.get('parameter_variation_rate', 0.1)  # Hz

    # Conservative bound: adaptation much slower than system dynamics
    return min(system_bandwidth / 10.0, 1.0 / parameter_variation_rate)
```

### 2. Convergence Detection and Early Stopping

**Intelligent convergence detection for efficient optimization:**

```python
class PSO_ConvergenceDetector:
    """
    Advanced convergence detection for PSO optimization.

    Features:
    - Multiple convergence criteria
    - Statistical significance testing
    - Plateau detection
    - Diversity monitoring
    """

    def __init__(self, patience: int = 20, tolerance: float = 1e-6):
        self.patience = patience
        self.tolerance = tolerance
        self.fitness_history = []
        self.diversity_history = []
        self.best_fitness = float('inf')
        self.stagnation_count = 0

    def update(self, current_fitness: float, population_diversity: float) -> bool:
        """
        Update convergence detector with current optimization state.

        Returns:
            True if convergence detected, False otherwise
        """

        self.fitness_history.append(current_fitness)
        self.diversity_history.append(population_diversity)

        # Check for improvement
        if current_fitness < self.best_fitness - self.tolerance:
            self.best_fitness = current_fitness
            self.stagnation_count = 0
        else:
            self.stagnation_count += 1

        # Multiple convergence criteria
        return (
            self._check_fitness_plateau() or
            self._check_diversity_collapse() or
            self._check_statistical_convergence()
        )

    def _check_fitness_plateau(self) -> bool:
        """Check if fitness has plateaued."""
        return self.stagnation_count >= self.patience

    def _check_diversity_collapse(self) -> bool:
        """Check if population diversity has collapsed."""
        if len(self.diversity_history) < 10:
            return False

        recent_diversity = np.mean(self.diversity_history[-10:])
        return recent_diversity < 1e-8  # Very low diversity

    def _check_statistical_convergence(self) -> bool:
        """Check statistical significance of convergence."""
        if len(self.fitness_history) < 30:
            return False

        # Test if recent improvements are statistically significant
        recent_fitness = self.fitness_history[-15:]
        older_fitness = self.fitness_history[-30:-15]

        from scipy.stats import ttest_ind
        statistic, p_value = ttest_ind(recent_fitness, older_fitness)

        # If no significant difference, consider converged
        return p_value > 0.05
```

### 3. Robust Error Handling

**Comprehensive error handling for production PSO workflows:**

```python
def robust_pso_optimization(
    controller_type: str,
    simulation_config: Any,
    pso_config: Dict[str, Any],
    error_handling_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Production-ready PSO optimization with comprehensive error handling.

    Features:
    - Graceful degradation for simulation failures
    - Automatic retry mechanisms
    - Fallback strategies for numerical instabilities
    - Comprehensive logging and diagnostics
    """

    import logging
    import traceback
    from contextlib import contextmanager

    # Setup logging
    logger = logging.getLogger('PSO_Optimization')

    @contextmanager
    def error_context(operation_name: str):
        """Context manager for operation-specific error handling."""
        try:
            logger.info(f"Starting {operation_name}")
            yield
            logger.info(f"Completed {operation_name}")
        except Exception as e:
            logger.error(f"Error in {operation_name}: {e}")
            logger.debug(traceback.format_exc())
            raise

    try:
        with error_context("PSO Initialization"):
            # Initialize with validation
            pso_interface = PSOFactoryInterface(controller_type, simulation_config)

            # Validate PSO configuration
            validate_pso_configuration(pso_config, pso_interface.n_gains)

        with error_context("Fitness Function Setup"):
            # Create robust fitness function with fallbacks
            fitness_function = create_robust_fitness_function(
                pso_interface, simulation_config, error_handling_config
            )

        with error_context("PSO Execution"):
            # Run PSO with monitoring
            result = run_monitored_pso_optimization(
                fitness_function, pso_config, error_handling_config
            )

        with error_context("Result Validation"):
            # Validate optimization results
            validated_result = validate_and_refine_result(
                result, controller_type, simulation_config
            )

        return validated_result

    except Exception as e:
        logger.error(f"PSO optimization failed: {e}")

        # Attempt fallback optimization
        if error_handling_config.get('enable_fallback', True):
            logger.info("Attempting fallback optimization")
            return fallback_optimization_strategy(
                controller_type, simulation_config, pso_config
            )
        else:
            raise

def create_robust_fitness_function(
    pso_interface: PSOFactoryInterface,
    simulation_config: Any,
    error_config: Dict[str, Any]
) -> Callable:
    """Create fitness function with comprehensive error handling."""

    max_retries = error_config.get('max_retries', 3)
    timeout = error_config.get('simulation_timeout', 30.0)

    def robust_fitness(particles: np.ndarray) -> np.ndarray:
        """Robust fitness evaluation with retries and timeouts."""

        fitness_scores = []

        for particle in particles:
            best_score = float('inf')

            for retry in range(max_retries):
                try:
                    # Create controller with timeout
                    with timeout_context(timeout):
                        controller = pso_interface.create_controller(particle)

                        # Run simulation with monitoring
                        result = run_monitored_simulation(controller, simulation_config)

                        # Compute fitness
                        score = compute_robust_fitness(result, error_config)

                        best_score = min(best_score, score)
                        break  # Success, no need to retry

                except TimeoutError:
                    logger.warning(f"Simulation timeout for particle {particle}")
                    continue
                except Exception as e:
                    logger.warning(f"Simulation error (retry {retry}): {e}")
                    continue

            fitness_scores.append(best_score)

        return np.array(fitness_scores)

    return robust_fitness
```

## Production Deployment Considerations

### 1. Performance Monitoring

**Real-time monitoring for production PSO optimization:**

```python
class PSO_ProductionMonitor:
    """
    Production monitoring system for PSO optimization workflows.

    Features:
    - Real-time performance metrics
    - Resource utilization tracking
    - Optimization progress visualization
    - Alert system for anomalies
    """

    def __init__(self, monitoring_config: Dict[str, Any]):
        self.config = monitoring_config
        self.metrics = {
            'optimization_start_time': None,
            'total_evaluations': 0,
            'successful_evaluations': 0,
            'failed_evaluations': 0,
            'average_evaluation_time': 0.0,
            'peak_memory_usage': 0.0,
            'cpu_utilization': [],
            'convergence_rate': 0.0
        }

    def start_optimization(self):
        """Initialize monitoring for new optimization run."""
        self.metrics['optimization_start_time'] = time.time()

    def log_evaluation(self, success: bool, evaluation_time: float):
        """Log individual fitness evaluation."""
        self.metrics['total_evaluations'] += 1

        if success:
            self.metrics['successful_evaluations'] += 1
        else:
            self.metrics['failed_evaluations'] += 1

        # Update average evaluation time
        total_time = (self.metrics['average_evaluation_time'] *
                     (self.metrics['total_evaluations'] - 1) + evaluation_time)
        self.metrics['average_evaluation_time'] = total_time / self.metrics['total_evaluations']

    def check_resource_usage(self):
        """Monitor system resource usage."""
        import psutil

        # Memory usage
        memory_info = psutil.virtual_memory()
        self.metrics['peak_memory_usage'] = max(
            self.metrics['peak_memory_usage'],
            memory_info.percent
        )

        # CPU utilization
        cpu_percent = psutil.cpu_percent(interval=1)
        self.metrics['cpu_utilization'].append(cpu_percent)

        # Check for resource alerts
        if memory_info.percent > 90:
            logger.warning(f"High memory usage: {memory_info.percent}%")

        if cpu_percent > 95:
            logger.warning(f"High CPU usage: {cpu_percent}%")

    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive monitoring report."""

        if self.metrics['optimization_start_time'] is None:
            return {'status': 'not_started'}

        elapsed_time = time.time() - self.metrics['optimization_start_time']
        success_rate = (self.metrics['successful_evaluations'] /
                       self.metrics['total_evaluations'] * 100
                       if self.metrics['total_evaluations'] > 0 else 0)

        return {
            'elapsed_time': elapsed_time,
            'total_evaluations': self.metrics['total_evaluations'],
            'success_rate': success_rate,
            'average_evaluation_time': self.metrics['average_evaluation_time'],
            'evaluations_per_second': self.metrics['total_evaluations'] / elapsed_time,
            'peak_memory_usage': self.metrics['peak_memory_usage'],
            'average_cpu_usage': np.mean(self.metrics['cpu_utilization']),
            'status': 'running' if elapsed_time > 0 else 'completed'
        }
```

### 2. Configuration Management

**Centralized configuration system for PSO workflows:**

```python
# example-metadata:
# runnable: false

from dataclasses import dataclass
from typing import Optional
import yaml

@dataclass
class PSO_OptimizationConfig:
    """
    Complete configuration for PSO optimization workflows.

    Provides type-safe configuration with validation and defaults.
    """

    # Controller configuration
    controller_type: str
    controller_config: Dict[str, Any]

    # PSO algorithm parameters
    n_particles: int = 30
    max_iterations: int = 100
    c1: float = 2.0  # Cognitive component
    c2: float = 2.0  # Social component
    w: float = 0.9   # Inertia weight

    # Optimization objectives
    objectives: Dict[str, float] = None  # {'ise': 0.4, 'overshoot': 0.3, 'energy': 0.3}

    # Performance settings
    enable_parallel_evaluation: bool = True
    n_threads: int = 4
    enable_gpu_acceleration: bool = False

    # Caching and persistence
    enable_simulation_cache: bool = True
    cache_size: int = 1000
    save_intermediate_results: bool = True

    # Error handling
    max_retries: int = 3
    simulation_timeout: float = 30.0
    enable_fallback: bool = True

    # Convergence detection
    convergence_patience: int = 20
    convergence_tolerance: float = 1e-6
    enable_early_stopping: bool = True

    # Monitoring and logging
    enable_monitoring: bool = True
    log_level: str = 'INFO'
    save_optimization_history: bool = True

    def __post_init__(self):
        """Validate configuration after initialization."""

        # Set default objectives if not provided
        if self.objectives is None:
            self.objectives = {'ise': 0.5, 'overshoot': 0.3, 'settling_time': 0.2}

        # Validate objectives sum to 1.0
        if abs(sum(self.objectives.values()) - 1.0) > 1e-6:
            raise ValueError("Objective weights must sum to 1.0")

        # Validate PSO parameters
        if not (0 < self.c1 < 5 and 0 < self.c2 < 5):
            raise ValueError("PSO cognitive/social parameters must be in (0, 5)")

        if not (0 < self.w < 1):
            raise ValueError("PSO inertia weight must be in (0, 1)")

        # Validate controller type
        valid_types = ['classical_smc', 'sta_smc', 'adaptive_smc', 'hybrid_adaptive_sta_smc']
        if self.controller_type not in valid_types:
            raise ValueError(f"Controller type must be one of {valid_types}")

def load_pso_config_from_yaml(config_path: str) -> PSO_OptimizationConfig:
    """Load PSO configuration from YAML file with validation."""

    with open(config_path, 'r') as f:
        config_dict = yaml.safe_load(f)

    # Extract PSO-specific configuration
    pso_config = config_dict.get('pso_optimization', {})

    return PSO_OptimizationConfig(**pso_config)

def save_pso_config_to_yaml(config: PSO_OptimizationConfig, output_path: str) -> None:
    """Save PSO configuration to YAML file."""

    config_dict = {
        'pso_optimization': {
            'controller_type': config.controller_type,
            'controller_config': config.controller_config,
            'n_particles': config.n_particles,
            'max_iterations': config.max_iterations,
            'c1': config.c1,
            'c2': config.c2,
            'w': config.w,
            'objectives': config.objectives,
            # ... include all configuration fields
        }
    }

    with open(output_path, 'w') as f:
        yaml.dump(config_dict, f, default_flow_style=False, indent=2)
```

---

**Document Status**: Complete - Production Ready
**Last Updated**: September 28, 2024
**Integration Level**: Full Factory + PSO Workflow Support
**Performance**: Optimized for Production Deployment