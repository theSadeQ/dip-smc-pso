# PSO Integration Workflow and Best Practices ## Overview This document provides guidance for integrating the SMC Controller Factory with Particle Swarm Optimization (PSO) workflows. The integration enables automated controller parameter tuning while maintaining factory validation, thread safety, and performance requirements. ## PSO Integration Architecture ### High-Level PSO-Factory Integration Flow ```

┌─────────────────┐ ┌──────────────────┐ ┌─────────────────┐
│ PSO Engine │───▶│ Factory-PSO │───▶│ Controller │
│ Optimizer │ │ Interface │ │ Validation │
└─────────────────┘ └──────────────────┘ └─────────────────┘ │ │ │ │ ▼ │ │ ┌──────────────────┐ │ │ │ Parameter │ │ └─────────────▶│ Bounds & Specs │◄────────────┘ └──────────────────┘ │ ▼ ┌──────────────────┐ │ Fitness │ │ Evaluation │ └──────────────────┘
``` ### PSO-Compatible Controller Factory Interface ```python
# example-metadata:
# runnable: false class PSOFactoryInterface: """ Specialized interface for PSO optimization integration. Features: - Vectorized controller creation for swarm populations - Automatic parameter validation and bounds checking - Performance-optimized fitness evaluation - Thread-safe parallel optimization support """ def __init__(self, controller_type: str, plant_config: Any): self.controller_type = controller_type self.plant_config = plant_config self._setup_optimization_environment() def _setup_optimization_environment(self) -> None: """Initialize PSO optimization environment.""" # Get controller specifications self.gain_spec = SMC_GAIN_SPECS[SMCType(self.controller_type)] self.n_gains = self.gain_spec.n_gains self.bounds = self.gain_spec.gain_bounds # Performance monitoring self.evaluation_count = 0 self.successful_evaluations = 0 self.failed_evaluations = 0 # Thread-safe operations self._lock = threading.RLock() def create_pso_controller_factory(self) -> Callable[[GainsArray], PSOControllerWrapper]: """ Create PSO-optimized controller factory function. Returns: Factory function that takes gains and returns PSO-wrapped controller """ def controller_factory(gains: GainsArray) -> PSOControllerWrapper: """PSO controller factory with validation.""" with self._lock: self.evaluation_count += 1 try: # Validate gains if not self._validate_pso_gains(gains): self.failed_evaluations += 1 return self._create_fallback_controller(gains) # Create controller via factory controller = create_controller( controller_type=self.controller_type, config=self.plant_config, gains=gains ) # Wrap for PSO optimization wrapper = PSOControllerWrapper( controller=controller, controller_type=self.controller_type, validation_enabled=True ) # Add PSO-required attributes wrapper.n_gains = self.n_gains wrapper.controller_type = self.controller_type wrapper.max_force = getattr(controller, 'max_force', 150.0) self.successful_evaluations += 1 return wrapper except Exception as e: logger.warning(f"PSO controller creation failed: {e}") self.failed_evaluations += 1 return self._create_fallback_controller(gains) # Add PSO-required attributes to factory function controller_factory.n_gains = self.n_gains controller_factory.controller_type = self.controller_type controller_factory.bounds = self.bounds controller_factory.max_force = 150.0 return controller_factory def _validate_pso_gains(self, gains: GainsArray) -> bool: """Validate gains for PSO optimization.""" try: gains_array = np.asarray(gains) # Check dimensions if len(gains_array) != self.n_gains: return False # Check bounds for i, (gain, (min_val, max_val)) in enumerate(zip(gains_array, self.bounds)): if not (min_val <= gain <= max_val): return False # Check numerical validity if not np.all(np.isfinite(gains_array)): return False # Controller-specific validation return validate_smc_gains(SMCType(self.controller_type), gains_array) except Exception: return False def _create_fallback_controller(self, gains: GainsArray) -> PSOControllerWrapper: """Create fallback controller for invalid parameters.""" # Use default gains as fallback default_gains = get_default_gains(self.controller_type) try: controller = create_controller( controller_type=self.controller_type, config=self.plant_config, gains=default_gains ) wrapper = PSOControllerWrapper( controller=controller, controller_type=self.controller_type, validation_enabled=False # Disable validation for fallback ) wrapper.n_gains = self.n_gains wrapper.controller_type = self.controller_type wrapper.is_fallback = True return wrapper except Exception: # Emergency fallback - return minimal controller return self._create_emergency_fallback() def get_optimization_statistics(self) -> Dict[str, Any]: """Get PSO optimization statistics.""" with self._lock: success_rate = self.successful_evaluations / max(1, self.evaluation_count) return { 'total_evaluations': self.evaluation_count, 'successful_evaluations': self.successful_evaluations, 'failed_evaluations': self.failed_evaluations, 'success_rate': success_rate, 'optimization_health': 'GOOD' if success_rate > 0.8 else 'WARNING' if success_rate > 0.5 else 'POOR' }
``` ## Controller-Specific PSO Integration Patterns ### Classical SMC PSO Integration ```python

def setup_classical_smc_pso_optimization( plant_config: Any, optimization_config: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]: """ Setup PSO optimization for Classical SMC with enhanced performance. Classical SMC Parameters: [k1, k2, λ1, λ2, K, kd] - k1, k2: Pendulum surface gains (convergence rate) - λ1, λ2: Sliding coefficients (surface slope) - K: Switching gain (uncertainty rejection) - kd: Damping gain (chattering reduction) """ # Enhanced bounds for double-inverted pendulum bounds = { 'lower': [5.0, 5.0, 3.0, 3.0, 10.0, 1.0], # Conservative lower bounds 'upper': [50.0, 40.0, 30.0, 25.0, 80.0, 15.0] # Aggressive upper bounds } # PSO-specific configuration default_config = { 'swarm_size': 30, 'max_iterations': 100, 'cognitive_param': 2.0, # Personal best weight 'social_param': 2.0, # Global best weight 'inertia_weight': 0.9, # Exploration vs exploitation 'inertia_decay': 0.95, # Dynamic inertia reduction 'convergence_threshold': 1e-6, 'parallel_evaluation': True, 'thread_count': 4 } if optimization_config: default_config.update(optimization_config) # Create PSO factory interface pso_interface = PSOFactoryInterface('classical_smc', plant_config) controller_factory = pso_interface.create_pso_controller_factory() # Fitness function for classical SMC def fitness_function(gains: GainsArray) -> float: """ Multi-objective fitness function for Classical SMC optimization. Objectives: 1. Stabilization performance (primary) 2. Control effort minimization (secondary) 3. Chattering reduction (tertiary) """ try: controller = controller_factory(gains) # Test scenarios scenarios = [ ('small_disturbance', np.array([0.1, 0.05, 0.03, 0.0, 0.0, 0.0])), ('medium_angles', np.array([0.3, 0.4, 0.2, 0.1, 0.0, 0.0])), ('high_velocity', np.array([0.1, 0.1, 0.1, 1.5, 1.0, 0.8])) ] total_cost = 0.0 scenario_weights = [0.5, 0.3, 0.2] # Weight different scenarios for weight, (scenario_name, initial_state) in zip(scenario_weights, scenarios): scenario_cost = evaluate_scenario_performance( controller, initial_state, simulation_time=2.0 ) total_cost += weight * scenario_cost # Penalize extreme gains gain_penalty = compute_gain_penalty(gains, bounds) total_cost += 0.1 * gain_penalty return total_cost except Exception as e: logger.warning(f"Fitness evaluation failed: {e}") return 1000.0 # High penalty for failed evaluations return { 'controller_factory': controller_factory, 'fitness_function': fitness_function, 'bounds': bounds, 'pso_config': default_config, 'pso_interface': pso_interface, 'n_gains': 6, 'optimization_type': 'classical_smc_dip' } def evaluate_scenario_performance( controller: PSOControllerWrapper, initial_state: StateVector, simulation_time: float = 2.0, dt: float = 0.001
) -> float: """Evaluate controller performance for a specific scenario.""" from src.plant.models.simplified.dynamics import SimplifiedDIPDynamics # Create dynamics model dynamics = SimplifiedDIPDynamics() # Simulation parameters n_steps = int(simulation_time / dt) state = initial_state.copy() # Performance metrics position_errors = [] control_efforts = [] control_variations = [] previous_control = 0.0 for step in range(n_steps): try: # Compute control control = controller.compute_control(state) control_value = control[0] # Store metrics position_error = np.linalg.norm(state[:3]) # Position error magnitude position_errors.append(position_error) control_efforts.append(abs(control_value)) # Control variation (chattering metric) control_variation = abs(control_value - previous_control) control_variations.append(control_variation) previous_control = control_value # Simulate dynamics result = dynamics.compute_dynamics(state, control) if not result.success: break # Integrate state = state + dt * result.state_derivative # Stability check if np.any(np.abs(state) > 10.0): return 1000.0 # Instability penalty except Exception: return 1000.0 # Simulation failure penalty # Compute composite cost avg_position_error = np.mean(position_errors) avg_control_effort = np.mean(control_efforts) avg_control_variation = np.mean(control_variations) final_position_error = position_errors[-1] # Multi-objective cost function cost = ( 10.0 * avg_position_error + # Primary: tracking performance 0.1 * avg_control_effort + # Secondary: control effort 1.0 * avg_control_variation + # Tertiary: chattering 5.0 * final_position_error # Final: steady-state error ) return cost def compute_gain_penalty(gains: GainsArray, bounds: Dict[str, List[float]]) -> float: """Compute penalty for gains near boundaries.""" gains_array = np.asarray(gains) lower_bounds = np.array(bounds['lower']) upper_bounds = np.array(bounds['upper']) # Normalize gains to [0, 1] range normalized_gains = (gains_array - lower_bounds) / (upper_bounds - lower_bounds) # Penalty for gains too close to boundaries boundary_penalty = 0.0 for g in normalized_gains: if g < 0.1 or g > 0.9: # Within 10% of boundaries boundary_penalty += 1.0 return boundary_penalty
``` ### Adaptive SMC PSO Integration ```python
def setup_adaptive_smc_pso_optimization( plant_config: Any, adaptation_focused: bool = True
) -> Dict[str, Any]: """ Setup PSO optimization for Adaptive SMC with adaptation-focused tuning. Adaptive SMC Parameters: [k1, k2, λ1, λ2, γ] - k1, k2: Pendulum surface gains - λ1, λ2: Sliding coefficients - γ: Adaptation rate (critical parameter) """ # Adaptation-focused bounds if adaptation_focused: bounds = { 'lower': [8.0, 8.0, 5.0, 5.0, 1.0], # Higher surface gains 'upper': [60.0, 50.0, 40.0, 35.0, 8.0] # Conservative adaptation rate } else: bounds = { 'lower': [5.0, 5.0, 3.0, 3.0, 0.5], 'upper': [50.0, 40.0, 30.0, 25.0, 10.0] } # PSO configuration with emphasis on exploration for adaptation pso_config = { 'swarm_size': 40, # Larger swarm for adaptation exploration 'max_iterations': 150, # More iterations for convergence 'cognitive_param': 2.5, # Higher personal exploration 'social_param': 1.5, # Lower social influence 'inertia_weight': 0.9, 'inertia_decay': 0.98, # Slower decay for exploration 'adaptation_penalty_weight': 0.2, # Penalty for poor adaptation 'convergence_threshold': 5e-7 } # Create PSO interface pso_interface = PSOFactoryInterface('adaptive_smc', plant_config) controller_factory = pso_interface.create_pso_controller_factory() def adaptive_fitness_function(gains: GainsArray) -> float: """Fitness function emphasizing adaptation performance.""" try: controller = controller_factory(gains) # Extract adaptation rate for analysis gamma = gains[4] # Adaptive-specific test scenarios scenarios = [ ('adaptation_test_1', np.array([0.2, 0.3, 0.1, 0.0, 0.0, 0.0])), ('adaptation_test_2', np.array([0.4, 0.5, 0.3, 0.5, 0.3, 0.2])), ('parameter_change', np.array([0.1, 0.1, 0.1, 0.0, 0.0, 0.0])) # For adaptation testing ] total_cost = 0.0 for i, (scenario_name, initial_state) in enumerate(scenarios): if scenario_name == 'parameter_change': # Test adaptation to parameter changes cost = evaluate_adaptation_performance(controller, initial_state) else: # Standard performance evaluation cost = evaluate_scenario_performance(controller, initial_state, simulation_time=3.0) total_cost += cost # Adaptation rate penalty if gamma < 0.5 or gamma > 8.0: total_cost += 50.0 * abs(gamma - 3.0) # Penalty for extreme adaptation rates # Convergence bonus for reasonable adaptation rates if 1.0 <= gamma <= 5.0: total_cost *= 0.9 # 10% bonus for good adaptation rate return total_cost except Exception as e: logger.warning(f"Adaptive fitness evaluation failed: {e}") return 1500.0 return { 'controller_factory': controller_factory, 'fitness_function': adaptive_fitness_function, 'bounds': bounds, 'pso_config': pso_config, 'pso_interface': pso_interface, 'n_gains': 5, 'optimization_type': 'adaptive_smc_dip', 'special_features': ['adaptation_monitoring', 'parameter_change_testing'] } def evaluate_adaptation_performance( controller: PSOControllerWrapper, initial_state: StateVector, simulation_time: float = 4.0
) -> float: """Evaluate adaptation performance with parameter changes.""" from src.plant.models.simplified.dynamics import SimplifiedDIPDynamics dynamics = SimplifiedDIPDynamics() dt = 0.001 n_steps = int(simulation_time / dt) state = initial_state.copy() adaptation_errors = [] pre_change_errors = [] post_change_errors = [] change_step = n_steps // 2 # Parameter change at midpoint for step in range(n_steps): try: # Simulate parameter change at midpoint if step == change_step: # Introduce disturbance to test adaptation state += np.array([0.1, 0.1, 0.0, 0.0, 0.0, 0.0]) control = controller.compute_control(state) result = dynamics.compute_dynamics(state, control) if not result.success: break state = state + dt * result.state_derivative position_error = np.linalg.norm(state[:3]) if step < change_step: pre_change_errors.append(position_error) else: post_change_errors.append(position_error) adaptation_errors.append(position_error) if np.any(np.abs(state) > 8.0): return 2000.0 # Adaptation failure penalty except Exception: return 2000.0 # Analyze adaptation performance pre_change_avg = np.mean(pre_change_errors) if pre_change_errors else 1.0 post_change_avg = np.mean(post_change_errors) if post_change_errors else 1.0 # Adaptation quality metric adaptation_ratio = post_change_avg / (pre_change_avg + 1e-6) # Cost emphasizing good adaptation adaptation_cost = ( 5.0 * np.mean(adaptation_errors) + # Overall performance 10.0 * max(0, adaptation_ratio - 2.0) + # Penalty for poor adaptation 2.0 * post_change_avg # Post-change performance ) return adaptation_cost
``` ### Super-Twisting SMC PSO Integration ```python

def setup_super_twisting_smc_pso_optimization( plant_config: Any, high_performance_mode: bool = True
) -> Dict[str, Any]: """ Setup PSO optimization for Super-Twisting SMC. STA-SMC Parameters: [K1, K2, k1, k2, λ1, λ2] - K1, K2: Super-twisting algorithm gains (K1 > K2 typically) - k1, k2: Surface gains - λ1, λ2: Sliding coefficients """ # High-performance bounds for aggressive STA tuning if high_performance_mode: bounds = { 'lower': [15.0, 10.0, 8.0, 8.0, 5.0, 5.0], 'upper': [100.0, 70.0, 60.0, 50.0, 40.0, 35.0] } else: bounds = { 'lower': [10.0, 8.0, 5.0, 5.0, 3.0, 3.0], 'upper': [80.0, 60.0, 50.0, 40.0, 30.0, 25.0] } # STA-specific PSO configuration pso_config = { 'swarm_size': 35, 'max_iterations': 120, 'cognitive_param': 2.2, 'social_param': 1.8, 'inertia_weight': 0.85, 'inertia_decay': 0.97, 'sta_constraint_weight': 0.3, # Weight for STA-specific constraints 'convergence_threshold': 1e-6 } pso_interface = PSOFactoryInterface('sta_smc', plant_config) controller_factory = pso_interface.create_pso_controller_factory() def sta_fitness_function(gains: GainsArray) -> float: """Fitness function for Super-Twisting SMC optimization.""" try: controller = controller_factory(gains) K1, K2 = gains[0], gains[1] # STA constraint penalty sta_penalty = 0.0 if K1 <= K2: sta_penalty += 100.0 * (K2 - K1 + 1.0) # Strong penalty for K1 <= K2 # Test with challenging scenarios for STA performance scenarios = [ ('precision_tracking', np.array([0.05, 0.03, 0.02, 0.0, 0.0, 0.0])), ('large_disturbance', np.array([0.6, 0.8, 0.4, 0.3, 0.2, 0.1])), ('high_frequency', np.array([0.2, 0.2, 0.2, 2.0, 1.5, 1.0])) ] total_cost = 0.0 scenario_weights = [0.4, 0.4, 0.2] for weight, (scenario_name, initial_state) in zip(scenario_weights, scenarios): if scenario_name == 'precision_tracking': # STA excels at precision - test with tighter tolerance cost = evaluate_sta_precision_performance(controller, initial_state) elif scenario_name == 'large_disturbance': # Test robustness with large disturbances cost = evaluate_sta_robustness_performance(controller, initial_state) else: # Standard evaluation cost = evaluate_scenario_performance(controller, initial_state) total_cost += weight * cost total_cost += sta_penalty # Bonus for optimal K1/K2 ratio k_ratio = K1 / K2 if 1.2 <= k_ratio <= 2.0: # Optimal STA ratio range total_cost *= 0.95 # 5% bonus return total_cost except Exception as e: logger.warning(f"STA fitness evaluation failed: {e}") return 1800.0 return { 'controller_factory': controller_factory, 'fitness_function': sta_fitness_function, 'bounds': bounds, 'pso_config': pso_config, 'pso_interface': pso_interface, 'n_gains': 6, 'optimization_type': 'sta_smc_dip', 'special_features': ['finite_time_convergence', 'chattering_reduction', 'robustness_testing'] } def evaluate_sta_precision_performance( controller: PSOControllerWrapper, initial_state: StateVector, precision_threshold: float = 0.01
) -> float: """Evaluate STA precision performance with tight tolerances.""" from src.plant.models.simplified.dynamics import SimplifiedDIPDynamics dynamics = SimplifiedDIPDynamics() dt = 0.001 simulation_time = 3.0 n_steps = int(simulation_time / dt) state = initial_state.copy() precision_errors = [] convergence_time = None for step in range(n_steps): try: control = controller.compute_control(state) result = dynamics.compute_dynamics(state, control) if not result.success: break state = state + dt * result.state_derivative position_error = np.linalg.norm(state[:3]) precision_errors.append(position_error) # Check for convergence to precision threshold if convergence_time is None and position_error < precision_threshold: convergence_time = step * dt if np.any(np.abs(state) > 5.0): return 1500.0 # Precision failure except Exception: return 1500.0 # Precision-focused cost function avg_precision_error = np.mean(precision_errors) final_precision_error = precision_errors[-1] # Convergence time bonus convergence_bonus = 0.0 if convergence_time is not None: convergence_bonus = max(0, 2.0 - convergence_time) # Bonus for fast convergence precision_cost = ( 20.0 * avg_precision_error + 30.0 * final_precision_error - 5.0 * convergence_bonus ) return max(0.1, precision_cost) # Minimum positive cost def evaluate_sta_robustness_performance( controller: PSOControllerWrapper, initial_state: StateVector
) -> float: """Evaluate STA robustness with large disturbances.""" from src.plant.models.simplified.dynamics import SimplifiedDIPDynamics dynamics = SimplifiedDIPDynamics() dt = 0.001 simulation_time = 4.0 n_steps = int(simulation_time / dt) state = initial_state.copy() robustness_errors = [] max_recovery_time = 0.0 disturbance_steps = [n_steps // 4, n_steps // 2, 3 * n_steps // 4] for step in range(n_steps): try: # Apply disturbances at specific intervals if step in disturbance_steps: disturbance = np.array([0.1, 0.1, 0.05, 0.2, 0.1, 0.1]) state += disturbance control = controller.compute_control(state) result = dynamics.compute_dynamics(state, control) if not result.success: break state = state + dt * result.state_derivative position_error = np.linalg.norm(state[:3]) robustness_errors.append(position_error) if np.any(np.abs(state) > 8.0): return 2000.0 # Robustness failure except Exception: return 2000.0 # Robustness cost emphasizing disturbance rejection avg_robustness_error = np.mean(robustness_errors) max_error = np.max(robustness_errors) robustness_cost = ( 15.0 * avg_robustness_error + 10.0 * max_error + 5.0 * robustness_errors[-1] # Final steady-state error ) return robustness_cost
``` ## Advanced PSO Integration Features ### Parallel PSO Evaluation ```python
# example-metadata:
# runnable: false class ParallelPSOEvaluator: """ Thread-safe parallel evaluation system for PSO optimization. Features: - Multi-threaded fitness evaluation - Load balancing across CPU cores - Memory-efficient swarm processing - Progress monitoring and early termination """ def __init__( self, controller_factory: Callable, fitness_function: Callable, n_threads: int = 4, batch_size: int = 8 ): self.controller_factory = controller_factory self.fitness_function = fitness_function self.n_threads = n_threads self.batch_size = batch_size # Thread management self.thread_pool = ThreadPoolExecutor(max_workers=n_threads) self.evaluation_lock = threading.RLock() # Performance monitoring self.evaluation_times = [] self.success_count = 0 self.failure_count = 0 def evaluate_swarm_parallel( self, swarm_positions: np.ndarray, timeout_seconds: float = 30.0 ) -> List[float]: """ Evaluate entire swarm in parallel with timeout protection. Args: swarm_positions: Array of shape (swarm_size, n_dimensions) timeout_seconds: Maximum time for evaluation Returns: List of fitness values for each particle """ swarm_size = swarm_positions.shape[0] fitness_values = [float('inf')] * swarm_size # Submit evaluation tasks future_to_index = {} for i in range(swarm_size): future = self.thread_pool.submit( self._evaluate_particle_safe, swarm_positions[i], i ) future_to_index[future] = i # Collect results with timeout completed_count = 0 start_time = time.time() for future in as_completed(future_to_index, timeout=timeout_seconds): try: particle_index = future_to_index[future] fitness_value = future.result(timeout=1.0) # Individual timeout fitness_values[particle_index] = fitness_value with self.evaluation_lock: self.success_count += 1 completed_count += 1 except Exception as e: particle_index = future_to_index[future] logger.warning(f"Particle {particle_index} evaluation failed: {e}") with self.evaluation_lock: self.failure_count += 1 # Use penalty value for failed evaluations fitness_values[particle_index] = 5000.0 # Check for timeout if time.time() - start_time > timeout_seconds: logger.warning(f"Swarm evaluation timeout after {timeout_seconds}s") break # Cancel remaining futures for future in future_to_index: if not future.done(): future.cancel() return fitness_values def _evaluate_particle_safe(self, gains: GainsArray, particle_index: int) -> float: """Thread-safe particle evaluation with error handling.""" start_time = time.time() try: # Create controller controller = self.controller_factory(gains) # Evaluate fitness fitness_value = self.fitness_function(gains, controller) # Record evaluation time evaluation_time = time.time() - start_time with self.evaluation_lock: self.evaluation_times.append(evaluation_time) return fitness_value except Exception as e: logger.warning(f"Particle {particle_index} failed: {e}") return 3000.0 # High penalty for failures def get_evaluation_statistics(self) -> Dict[str, Any]: """Get parallel evaluation performance statistics.""" with self.evaluation_lock: total_evaluations = self.success_count + self.failure_count success_rate = self.success_count / max(1, total_evaluations) avg_time = np.mean(self.evaluation_times) if self.evaluation_times else 0.0 max_time = np.max(self.evaluation_times) if self.evaluation_times else 0.0 return { 'total_evaluations': total_evaluations, 'success_count': self.success_count, 'failure_count': self.failure_count, 'success_rate': success_rate, 'avg_evaluation_time': avg_time, 'max_evaluation_time': max_time, 'total_evaluation_time': sum(self.evaluation_times), 'parallel_efficiency': avg_time * self.n_threads / max(max_time, 0.001) } def cleanup(self): """Clean up thread pool resources.""" self.thread_pool.shutdown(wait=True)
``` ### PSO Progress Monitoring and Early Termination ```python
# example-metadata:

# runnable: false class PSOProgressMonitor: """ PSO optimization progress monitoring. Features: - Convergence detection - Performance tracking - Early termination criteria - Optimization health assessment """ def __init__( self, convergence_threshold: float = 1e-6, stagnation_threshold: int = 20, max_evaluation_time: float = 0.1 ): self.convergence_threshold = convergence_threshold self.stagnation_threshold = stagnation_threshold self.max_evaluation_time = max_evaluation_time # Progress tracking self.iteration_history = [] self.best_fitness_history = [] self.diversity_history = [] self.evaluation_time_history = [] # Convergence state self.converged = False self.stagnation_count = 0 self.best_fitness = float('inf') def update_progress( self, iteration: int, swarm_positions: np.ndarray, fitness_values: List[float], evaluation_time: float ) -> Dict[str, Any]: """ Update optimization progress and assess termination criteria. Returns: Progress update with termination recommendations """ # Update best fitness current_best = min(fitness_values) improvement = self.best_fitness - current_best if improvement > self.convergence_threshold: self.best_fitness = current_best self.stagnation_count = 0 else: self.stagnation_count += 1 # Calculate swarm diversity diversity = self._calculate_swarm_diversity(swarm_positions) # Record history self.iteration_history.append(iteration) self.best_fitness_history.append(current_best) self.diversity_history.append(diversity) self.evaluation_time_history.append(evaluation_time) # Assess convergence convergence_status = self._assess_convergence(diversity, improvement) # Performance assessment performance_status = self._assess_performance(evaluation_time) # Termination recommendation should_terminate, termination_reason = self._should_terminate( convergence_status, performance_status ) return { 'iteration': iteration, 'best_fitness': current_best, 'improvement': improvement, 'diversity': diversity, 'stagnation_count': self.stagnation_count, 'convergence_status': convergence_status, 'performance_status': performance_status, 'should_terminate': should_terminate, 'termination_reason': termination_reason, 'evaluation_time': evaluation_time } def _calculate_swarm_diversity(self, swarm_positions: np.ndarray) -> float: """Calculate swarm diversity metric.""" if len(swarm_positions) < 2: return 0.0 # Calculate pairwise distances distances = [] for i in range(len(swarm_positions)): for j in range(i + 1, len(swarm_positions)): distance = np.linalg.norm(swarm_positions[i] - swarm_positions[j]) distances.append(distance) return np.mean(distances) if distances else 0.0 def _assess_convergence(self, diversity: float, improvement: float) -> str: """Assess convergence status.""" if improvement < self.convergence_threshold and diversity < 0.01: return 'CONVERGED' elif self.stagnation_count >= self.stagnation_threshold: return 'STAGNATED' elif diversity < 0.1: return 'LOW_DIVERSITY' elif improvement > 1.0: return 'IMPROVING' else: return 'SEARCHING' def _assess_performance(self, evaluation_time: float) -> str: """Assess computational performance.""" if evaluation_time > self.max_evaluation_time: return 'SLOW' elif evaluation_time > self.max_evaluation_time * 0.5: return 'MODERATE' else: return 'FAST' def _should_terminate( self, convergence_status: str, performance_status: str ) -> Tuple[bool, str]: """Determine if optimization should terminate early.""" if convergence_status == 'CONVERGED': return True, 'Convergence achieved' if convergence_status == 'STAGNATED': return True, f'Stagnation detected ({self.stagnation_count} iterations)' if performance_status == 'SLOW' and len(self.evaluation_time_history) > 10: avg_time = np.mean(self.evaluation_time_history[-10:]) if avg_time > self.max_evaluation_time * 2: return True, 'Performance degradation detected' return False, 'Continue optimization' def generate_optimization_report(self) -> Dict[str, Any]: """Generate optimization report.""" return { 'optimization_summary': { 'total_iterations': len(self.iteration_history), 'best_fitness_achieved': min(self.best_fitness_history) if self.best_fitness_history else float('inf'), 'final_diversity': self.diversity_history[-1] if self.diversity_history else 0.0, 'convergence_status': 'CONVERGED' if self.converged else 'INCOMPLETE' }, 'performance_metrics': { 'avg_evaluation_time': np.mean(self.evaluation_time_history) if self.evaluation_time_history else 0.0, 'max_evaluation_time': np.max(self.evaluation_time_history) if self.evaluation_time_history else 0.0, 'total_optimization_time': sum(self.evaluation_time_history) }, 'convergence_analysis': { 'fitness_improvement_rate': self._calculate_improvement_rate(), 'diversity_trend': self._calculate_diversity_trend(), 'stagnation_periods': self._identify_stagnation_periods() } } def _calculate_improvement_rate(self) -> float: """Calculate average fitness improvement rate.""" if len(self.best_fitness_history) < 2: return 0.0 improvements = [] for i in range(1, len(self.best_fitness_history)): improvement = self.best_fitness_history[i-1] - self.best_fitness_history[i] improvements.append(max(0, improvement)) return np.mean(improvements) def _calculate_diversity_trend(self) -> str: """Calculate diversity trend over time.""" if len(self.diversity_history) < 10: return 'INSUFFICIENT_DATA' recent_diversity = np.mean(self.diversity_history[-5:]) earlier_diversity = np.mean(self.diversity_history[-10:-5]) if recent_diversity < earlier_diversity * 0.8: return 'DECREASING' elif recent_diversity > earlier_diversity * 1.2: return 'INCREASING' else: return 'STABLE' def _identify_stagnation_periods(self) -> List[Tuple[int, int]]: """Identify periods of stagnation in optimization.""" stagnation_periods = [] current_start = None stagnation_threshold = 5 for i in range(1, len(self.best_fitness_history)): improvement = self.best_fitness_history[i-1] - self.best_fitness_history[i] if improvement < self.convergence_threshold: if current_start is None: current_start = i - 1 else: if current_start is not None and i - current_start >= stagnation_threshold: stagnation_periods.append((current_start, i - 1)) current_start = None # Handle final stagnation period if current_start is not None and len(self.best_fitness_history) - current_start >= stagnation_threshold: stagnation_periods.append((current_start, len(self.best_fitness_history) - 1)) return stagnation_periods

``` This PSO integration workflow provides the foundation for efficient, robust parameter optimization of SMC controllers with advanced monitoring, parallel evaluation, and quality assurance features.