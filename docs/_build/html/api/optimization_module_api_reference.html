<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="color-scheme" content="light dark">
<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
<style>
.chartjs-container {
    margin: 1.5em auto;
    padding: 1em;
    background: #f8f9fa;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}
.chart-note {
    text-align: center;
    color: #6c757d;
    font-size: 0.9em;
    margin-top: 0.5em;
}
</style>
<link rel="index" title="Index" href="../genindex.html"><link rel="search" title="Search" href="../search.html">

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.09.25 -->
        <title>Optimization Module API Reference Project: Double-Inverted Pendulum SMC Control System - DIP_SMC_PSO Documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5349f25f" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=580074bf" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=8dab3a3b" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8a7e329d" />
    
    


<style>
  body {
    --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #272822;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle site navigation sidebar">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc" aria-label="Toggle table of contents sidebar">
<label class="overlay sidebar-overlay" for="__navigation"></label>
<label class="overlay toc-overlay" for="__toc"></label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <span class="icon"><svg><use href="#svg-menu"></use></svg></span>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">DIP_SMC_PSO Documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <span class="sidebar-brand-text">DIP_SMC_PSO Documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">ğŸ“š Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../README.html">Optimal Sliding Mode Control for a Double-Inverted Pendulum via PSO  ## How to validate a ResearchPlan JSON Run the validator locally: ```bash</a></li>
<li class="toctree-l1"><a class="reference internal" href="../theory_overview.html">Theory Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">3. System Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plant_model.html">1.x Plant Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hil_quickstart.html">Hardware-in-the-Loop (HIL) Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../streamlit_dashboard_guide.html">Streamlit Dashboard User Guide</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="https://github.com/theSadeQ/DIP_SMC_PSO/blob/main/docs/api/optimization_module_api_reference.md?plain=true" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div><div class="edit-this-page">
  <a class="muted-link" href="https://github.com/theSadeQ/DIP_SMC_PSO/edit/main/docs/api/optimization_module_api_reference.md" rel="edit" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle" aria-label="Toggle Light / Dark / Auto color theme">
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <span class="icon"><svg><use href="#svg-toc"></use></svg></span>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section class="tex2jax_ignore mathjax_ignore" id="optimization-module-api-reference-project-double-inverted-pendulum-smc-control-system">
<h1>Optimization Module API Reference <strong>Project:</strong> Double-Inverted Pendulum SMC Control System<a class="headerlink" href="#optimization-module-api-reference-project-double-inverted-pendulum-smc-control-system" title="Link to this heading">Â¶</a></h1>
<p><strong>Module:</strong> <code class="docutils literal notranslate"><span class="pre">src/optimization/*</span></code>
<strong>Version:</strong> 1.0
<strong>Date:</strong> 2025-10-07
<strong>Status:</strong> Production-Ready API Documentation</p>
<hr class="docutils" />
<section id="table-of-contents-1-overview-architecture-1-1-optimization-system-architecture-1-2-pso-workflow-1-3-module-relationships">
<h2>Table of Contents 1. <a class="reference internal" href="#1-overview--architecture"><span class="xref myst">Overview &amp; Architecture</span></a> - 1.1 <a class="reference internal" href="#11-optimization-system-architecture"><span class="xref myst">Optimization System Architecture</span></a> - 1.2 <a class="reference internal" href="#12-pso-workflow"><span class="xref myst">PSO Workflow</span></a> - 1.3 <a class="reference internal" href="#13-module-relationships"><span class="xref myst">Module Relationships</span></a><a class="headerlink" href="#table-of-contents-1-overview-architecture-1-1-optimization-system-architecture-1-2-pso-workflow-1-3-module-relationships" title="Link to this heading">Â¶</a></h2>
<ol class="arabic simple" start="2">
<li><p><a class="reference internal" href="#2-psotuner-api"><span class="xref myst">PSOTuner API</span></a> - 2.1 <a class="reference internal" href="#21-class-overview"><span class="xref myst">Class Overview</span></a> - 2.2 <a class="reference internal" href="#22-initialization"><span class="xref myst">Initialization</span></a> - 2.3 <a class="reference internal" href="#23-optimization-workflow"><span class="xref myst">Optimization Workflow</span></a> - 2.4 <a class="reference internal" href="#24-fitness-function-design"><span class="xref myst">Fitness Function Design</span></a> - 2.5 <a class="reference internal" href="#25-cost-normalization"><span class="xref myst">Cost Normalization</span></a></p></li>
<li><p><a class="reference internal" href="#3-convergence-analysis-api"><span class="xref myst">Convergence Analysis API</span></a> - 3.1 <a class="reference internal" href="#31-enhancedconvergenceanalyzer-class"><span class="xref myst">EnhancedConvergenceAnalyzer Class</span></a> - 3.2 <a class="reference internal" href="#32-convergence-metrics"><span class="xref myst">Convergence Metrics</span></a> - 3.3 <a class="reference internal" href="#33-convergence-criteria"><span class="xref myst">Convergence Criteria</span></a> - 3.4 <a class="reference internal" href="#34-real-time-monitoring"><span class="xref myst">Real-Time Monitoring</span></a></p></li>
<li><p><a class="reference internal" href="#4-bounds-validation-api"><span class="xref myst">Bounds Validation API</span></a> - 4.1 <a class="reference internal" href="#41-psoboundsvalidator-class"><span class="xref myst">PSOBoundsValidator Class</span></a> - 4.2 <a class="reference internal" href="#42-controller-specific-bounds"><span class="xref myst">Controller-Specific Bounds</span></a> - 4.3 <a class="reference internal" href="#43-validation-rules"><span class="xref myst">Validation Rules</span></a> - 4.4 <a class="reference internal" href="#44-automatic-adjustment"><span class="xref myst">Automatic Adjustment</span></a></p></li>
<li><p><a class="reference internal" href="#5-bounds-optimization-api"><span class="xref myst">Bounds Optimization API</span></a> - 5.1 <a class="reference internal" href="#51-psoboundsoptimizer-class"><span class="xref myst">PSOBoundsOptimizer Class</span></a> - 5.2 <a class="reference internal" href="#52-optimization-strategies"><span class="xref myst">Optimization Strategies</span></a> - 5.3 <a class="reference internal" href="#53-multi-criteria-selection"><span class="xref myst">Multi-Criteria Selection</span></a></p></li>
<li><p><a class="reference internal" href="#6-hyperparameter-optimization-api"><span class="xref myst">Hyperparameter Optimization API</span></a> - 6.1 <a class="reference internal" href="#61-psohyperparameteroptimizer-class"><span class="xref myst">PSOHyperparameterOptimizer Class</span></a> - 6.2 <a class="reference internal" href="#62-meta-optimization"><span class="xref myst">Meta-Optimization</span></a> - 6.3 <a class="reference internal" href="#63-multi-objective-optimization"><span class="xref myst">Multi-Objective Optimization</span></a></p></li>
<li><p><a class="reference internal" href="#7-factory-integration-api"><span class="xref myst">Factory Integration API</span></a> - 7.1 <a class="reference internal" href="#71-enhancedpsofactory"><span class="xref myst">EnhancedPSOFactory</span></a> - 7.2 <a class="reference internal" href="#72-integration-patterns"><span class="xref myst">Integration Patterns</span></a></p></li>
<li><p><a class="reference internal" href="#8-complete-code-examples"><span class="xref myst">Complete Code Examples</span></a> - 8.1 <a class="reference internal" href="#81-basic-pso-optimization"><span class="xref myst">Basic PSO Optimization</span></a> - 8.2 <a class="reference internal" href="#82-real-time-convergence-monitoring"><span class="xref myst">Real-Time Convergence Monitoring</span></a> - 8.3 <a class="reference internal" href="#83-bounds-validation-and-adjustment"><span class="xref myst">Bounds Validation and Adjustment</span></a> - 8.4 <a class="reference internal" href="#84-hyperparameter-optimization"><span class="xref myst">Hyperparameter Optimization</span></a> - 8.5 <a class="reference internal" href="#85-complete-optimization-pipeline"><span class="xref myst">Complete Optimization Pipeline</span></a></p></li>
<li><p><a class="reference internal" href="#9-performance--tuning-guidelines"><span class="xref myst">Performance &amp; Tuning Guidelines</span></a> - 9.1 <a class="reference internal" href="#91-pso-parameter-selection"><span class="xref myst">PSO Parameter Selection</span></a> - 9.2 <a class="reference internal" href="#92-convergence-criteria-tuning"><span class="xref myst">Convergence Criteria Tuning</span></a> - 9.3 <a class="reference internal" href="#93-computational-efficiency"><span class="xref myst">Computational Efficiency</span></a></p></li>
<li><p><a class="reference internal" href="#10-theory-cross-references"><span class="xref myst">Theory Cross-References</span></a> - 10.1 <a class="reference internal" href="#101-phase-22-links-pso-foundations"><span class="xref myst">Phase 2.2 Links (PSO Foundations)</span></a> - 10.2 <a class="reference internal" href="#102-phase-42-links-factory-system"><span class="xref myst">Phase 4.2 Links (Factory System)</span></a> - 10.3 <a class="reference internal" href="#103-related-documentation"><span class="xref myst">Related Documentation</span></a></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="overview-architecture-1-1-optimization-system-architecture-the-optimization-system-consists-of-four-primary-modules-working-in-concert-to-tune-sliding-mode-controller-smc-parameters">
<h2>1. Overview &amp; Architecture ### 1.1 Optimization System Architecture The optimization system consists of four primary modules working in concert to tune sliding mode controller (SMC) parameters: ```<a class="headerlink" href="#overview-architecture-1-1-optimization-system-architecture-the-optimization-system-consists-of-four-primary-modules-working-in-concert-to-tune-sliding-mode-controller-smc-parameters" title="Link to this heading">Â¶</a></h2>
<p>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OPTIMIZATION SYSTEM â”‚
â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Factory â”‚â”€â”€â”€â”€â”€&gt;â”‚ PSO Tuner â”‚ â”‚
â”‚ â”‚ Bridge â”‚ â”‚ (algorithms/) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â”‚ Fitness â”‚ â”‚
â”‚ â”‚ â”‚ Evaluation â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ â”‚ Convergence â”‚ â”‚
â”‚ â”‚ â”‚ Analyzer â”‚ â”‚
â”‚ â”‚ â”‚ (validation/) â”‚ â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€&gt;â”‚ Bounds â”‚ â”‚
â”‚ â”‚ Validator â”‚ â”‚
â”‚ â”‚ (validation/) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”‚
â”‚ Supporting Modules: â”‚
â”‚ â€¢ Bounds Optimizer (validation/) â”‚
â”‚ â€¢ Hyperparameter Optimizer (tuning/) â”‚
â”‚ â€¢ Factory Integration (integration/) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
<code class="docutils literal notranslate"><span class="pre">**Module</span> <span class="pre">Responsibilities:**</span> <span class="pre">1.</span> <span class="pre">**PSOTuner**</span> <span class="pre">(`src/optimization/algorithms/pso_optimizer.py`)</span> <span class="pre">-</span> <span class="pre">Particle</span> <span class="pre">swarm</span> <span class="pre">optimization</span> <span class="pre">algorithm</span> <span class="pre">implementation</span> <span class="pre">-</span> <span class="pre">Vectorized</span> <span class="pre">batch</span> <span class="pre">simulation</span> <span class="pre">integration</span> <span class="pre">-</span> <span class="pre">Cost</span> <span class="pre">computation</span> <span class="pre">and</span> <span class="pre">normalization</span> <span class="pre">-</span> <span class="pre">Uncertainty-aware</span> <span class="pre">robustness</span> <span class="pre">evaluation</span> <span class="pre">2.</span> <span class="pre">**EnhancedConvergenceAnalyzer**</span> <span class="pre">(`src/optimization/validation/enhanced_convergence_analyzer.py`)</span> <span class="pre">-</span> <span class="pre">Multi-criteria</span> <span class="pre">convergence</span> <span class="pre">detection</span> <span class="pre">-</span> <span class="pre">Statistical</span> <span class="pre">validation</span> <span class="pre">of</span> <span class="pre">optimization</span> <span class="pre">progress</span> <span class="pre">-</span> <span class="pre">Real-time</span> <span class="pre">performance</span> <span class="pre">prediction</span> <span class="pre">-</span> <span class="pre">Early</span> <span class="pre">stopping</span> <span class="pre">recommendations</span> <span class="pre">3.</span> <span class="pre">**PSOBoundsValidator**</span> <span class="pre">(`src/optimization/validation/pso_bounds_validator.py`)</span> <span class="pre">-</span> <span class="pre">Controller-specific</span> <span class="pre">parameter</span> <span class="pre">bounds</span> <span class="pre">validation</span> <span class="pre">-</span> <span class="pre">Physical</span> <span class="pre">constraint</span> <span class="pre">enforcement</span> <span class="pre">-</span> <span class="pre">Automatic</span> <span class="pre">bounds</span> <span class="pre">adjustment</span> <span class="pre">algorithms</span> <span class="pre">-</span> <span class="pre">Stability-aware</span> <span class="pre">bounds</span> <span class="pre">checking</span> <span class="pre">4.</span> <span class="pre">**PSOBoundsOptimizer**</span> <span class="pre">(`src/optimization/validation/pso_bounds_optimizer.py`)</span> <span class="pre">-</span> <span class="pre">Multi-strategy</span> <span class="pre">bounds</span> <span class="pre">optimization</span> <span class="pre">-</span> <span class="pre">Performance-driven</span> <span class="pre">bounds</span> <span class="pre">generation</span> <span class="pre">-</span> <span class="pre">Convergence-focused</span> <span class="pre">parameter</span> <span class="pre">space</span> <span class="pre">definition</span> <span class="pre">-</span> <span class="pre">Physics-based</span> <span class="pre">constraint</span> <span class="pre">derivation</span> <span class="pre">5.</span> <span class="pre">**PSOHyperparameterOptimizer**</span> <span class="pre">(`src/optimization/tuning/pso_hyperparameter_optimizer.py`)</span> <span class="pre">-</span> <span class="pre">Meta-optimization</span> <span class="pre">of</span> <span class="pre">PSO</span> <span class="pre">parameters</span> <span class="pre">-</span> <span class="pre">Multi-objective</span> <span class="pre">PSO</span> <span class="pre">tuning</span> <span class="pre">-</span> <span class="pre">Controller-specific</span> <span class="pre">hyperparameter</span> <span class="pre">adaptation</span> <span class="pre">-</span> <span class="pre">Baseline</span> <span class="pre">performance</span> <span class="pre">benchmarking</span> <span class="pre">6.</span> <span class="pre">**EnhancedPSOFactory**</span> <span class="pre">(`src/optimization/integration/pso_factory_bridge.py`)</span> <span class="pre">-</span> <span class="pre">factory-PSO</span> <span class="pre">integration</span> <span class="pre">-</span> <span class="pre">Enhanced</span> <span class="pre">fitness</span> <span class="pre">function</span> <span class="pre">construction</span> <span class="pre">-</span> <span class="pre">Robust</span> <span class="pre">error</span> <span class="pre">handling</span> <span class="pre">and</span> <span class="pre">recovery</span> <span class="pre">-</span> <span class="pre">Configuration</span> <span class="pre">management</span> <span class="pre">###</span> <span class="pre">1.2</span> <span class="pre">PSO</span> <span class="pre">Workflow</span> <span class="pre">The</span> <span class="pre">complete</span> <span class="pre">PSO</span> <span class="pre">optimization</span> <span class="pre">workflow</span> <span class="pre">follows</span> <span class="pre">this</span> <span class="pre">sequence:</span></code>
[1] Configuration Loading â”‚ â–¼
[2] Factory Initialization â”‚ â–¼
[3] Bounds Validation â”‚ â–¼
[4] PSO Initialization â€¢ Swarm creation â€¢ Velocity initialization â€¢ Fitness evaluation â”‚ â–¼
[5] Optimization Loop (iterative) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ a) Update velocitiesâ”‚ â”‚ b) Update positions â”‚ â”‚ c) Evaluate fitness â”‚ â”‚ d) Update best â”‚ â”‚ e) Check convergenceâ”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ Converged? â”€â”€Noâ”€â”€â” â”‚ â”‚ Yes â”‚ â”‚ â”‚ â–¼ â”‚
[6] Results Extraction â”‚ â”‚ â”‚ â–¼ â”‚
[7] Validation â”‚ â”‚ â”‚ â–¼ â”‚
[8] Controller Creationâ”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</p>
<div class="highlight-**Convergence notranslate"><div class="highlight"><pre><span></span>
1. **Fitness Tolerance**: $|f_{best}^t - f_{best}^{t-1}| &lt; \epsilon_{tol}$
2. **Relative Improvement**: $\frac{f_{best}^{t-w} - f_{best}^t}{f_{best}^{t-w}} &lt; \epsilon_{rel}$
3. **Population Diversity**: $\sigma_{swarm} &lt; \epsilon_{div}$
4. **Stagnation Detection**: No improvement for $t_{stag}$ iterations
5. **Statistical Significance**: $p$-value $&gt; \alpha_{conf}$ for improvement ### 1.3 Module Relationships **Data Flow:** ```
Configuration (YAML) â”‚ â”œâ”€â”€&gt; PSOTuner.__init__() â”‚ â”‚ â”‚ â”œâ”€â”€&gt; Controller Factory (from factory.py) â”‚ â”œâ”€â”€&gt; Bounds (PSOBoundsValidator) â”‚ â””â”€â”€&gt; Fitness Function Construction â”‚ â””â”€â”€&gt; EnhancedConvergenceAnalyzer.__init__() â”‚ â””â”€â”€&gt; Convergence Criteria Configuration PSOTuner.optimise() â”‚ â”œâ”€â”€&gt; PySwarms GlobalBestPSO â”‚ â”‚ â”‚ â””â”€â”€&gt; _fitness() callback â”‚ â”‚ â”‚ â”œâ”€â”€&gt; simulate_system_batch() â”‚ â”œâ”€â”€&gt; _compute_cost_from_traj() â”‚ â””â”€â”€&gt; _combine_costs() â”‚ â””â”€â”€&gt; EnhancedConvergenceAnalyzer.check_convergence() â”‚ â”œâ”€â”€&gt; ConvergenceMetrics â””â”€â”€&gt; ConvergenceStatus
``` **Cross-Module Dependencies:** - **PSOTuner** â†’ `src.controllers.factory` (controller creation)
- **PSOTuner** â†’ `src.simulation.engines.vector_sim` (batch simulation)
- **PSOTuner** â†’ `src.plant.models.dynamics` (DIPParams)
- **EnhancedConvergenceAnalyzer** â†’ `scipy.stats` (statistical tests)
- **PSOBoundsValidator** â†’ `src.controllers.factory.SMCType` (controller types)
- **Factory Integration** â†’ All above modules

---

## 2. PSOTuner API ### 2.1 Class Overview **Location:** `src/optimization/algorithms/pso_optimizer.py` ```python
class PSOTuner: &quot;&quot;&quot;High-throughput, vectorised tuner for sliding-mode controllers. The tuner wraps a particle swarm optimisation algorithm around the vectorised simulation. It uses local PRNGs to avoid global side effects and computes instability penalties based on normalisation constants. Cost aggregation between mean and worst-case performance is controlled via COMBINE_WEIGHTS. &quot;&quot;&quot;
``` **Key Features:**

- Vectorized PSO implementation with PySwarms integration
- Robust fitness evaluation with instability penalty handling
- Uncertainty-aware optimization (physics parameter perturbation)
- Cost normalization for multi-scale objective functions
- Thread-safe with local PRNG state management
- Configurable inertia weight scheduling
- Velocity clamping for stability **Theory Foundation:** See [Phase 2.2: PSO Algorithm Foundations](../theory/pso_algorithm_foundations.md) ### 2.2 Initialization **Method Signature:** ```python
def __init__( self, controller_factory: Callable[[np.ndarray], Any], config: Union[ConfigSchema, str, Path], seed: Optional[int] = None, rng: Optional[np.random.Generator] = None, *, instability_penalty_factor: float = 100.0,
) -&gt; None:
``` **Parameters:** | Parameter | Type | Description |
|-----------|------|-------------|
| `controller_factory` | `Callable[[np.ndarray], Any]` | Function that creates controller instances from gain vectors. Typically obtained from `src.controllers.factory.create_controller()` with partial application. |
| `config` | `Union[ConfigSchema, str, Path]` | Configuration object or path to YAML config file. Must contain `pso`, `simulation`, and `physics` sections. |
| `seed` | `Optional[int]` | Random seed for reproducibility. If `None`, uses `config.global_seed` or unseeded RNG. |
| `rng` | `Optional[np.random.Generator]` | External NumPy random generator. If provided, `seed` is ignored. |
| `instability_penalty_factor` | `float` | Multiplier for computing instability penalties. Default: 100.0. Final penalty = `factor * simulation_duration`. | **Returns:** None (initializes PSOTuner instance) **Raises:**
- `ValueError`: If configuration is invalid or missing required sections
- `ImportError`: If PySwarms is not installed
- `FileNotFoundError`: If config path does not exist **Configuration Requirements:** The `config` object must provide: ```yaml
pso: n_particles: 30 # Swarm size (10-50 recommended) n_iterations: 100 # Maximum iterations w: 0.729 # Inertia weight c1: 1.49445 # Cognitive coefficient c2: 1.49445 # Social coefficient bounds: min: [1.0, 1.0, 0.5, 0.5, 1.0, 0.1] # Lower bounds per gain max: [100.0, 100.0, 50.0, 50.0, 200.0, 20.0] # Upper bounds per gain velocity_clamp: [-0.5, 0.5] # Optional: velocity limits as fraction of bounds w_schedule: [0.9, 0.4] # Optional: linear inertia weight schedule convergence: tolerance: 1.0e-6 # Fitness tolerance for convergence patience: 50 # Iterations without improvement before stopping simulation: duration: 5.0 # Simulation time (seconds) dt: 0.01 # Time step (seconds) initial_state: [0.0, 0.1, 0.0, 0.0, 0.0, 0.0] # [x, Î¸1, dx, dÎ¸1, Î¸2, dÎ¸2] physics: cart_mass: 1.0 # Cart mass (kg) pendulum1_mass: 0.1 # Link 1 mass (kg) pendulum1_length: 0.5 # Link 1 length (m) pendulum1_com: 0.25 # Link 1 center of mass (m) pendulum2_mass: 0.05 # Link 2 mass (kg) pendulum2_length: 0.25 # Link 2 length (m) pendulum2_com: 0.125 # Link 2 center of mass (m) gravity: 9.81 # Gravitational acceleration (m/sÂ²) friction_cart: 0.1 # Cart friction coefficient friction_p1: 0.01 # Link 1 joint friction friction_p2: 0.01 # Link 2 joint friction physics_uncertainty: # Optional: robustness evaluation n_evals: 5 # Number of perturbed physics models cart_mass: 0.10 # Â±10% perturbation pendulum1_mass: 0.15 # Â±15% perturbation pendulum1_length: 0.05 # Â±5% perturbation # ... (other parameters similarly) cost_weights: state_error: 1.0 # Weight for integrated state error control_effort: 0.1 # Weight for control energy control_rate: 0.05 # Weight for control slew rate stability: 0.5 # Weight for sliding variable energy
``` **Example Usage:** ```python

from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.controllers.factory import create_controller
from src.config import load_config
from functools import partial # Load configuration
config = load_config(&quot;config.yaml&quot;) # Create controller factory (partial application)
controller_factory = partial( create_controller, controller_type=&#39;classical_smc&#39;, config=config
) # Initialize PSO tuner
tuner = PSOTuner( controller_factory=controller_factory, config=config, seed=42, instability_penalty_factor=100.0
) # Run optimization
result = tuner.optimise() print(f&quot;Best gains: {result[&#39;best_pos&#39;]}&quot;)
print(f&quot;Best cost: {result[&#39;best_cost&#39;]:.4f}&quot;)
``` **Physical Interpretation of Initialization:** The initialization process sets up: 1. **Cost Normalization Constants**: Computed from baseline trajectories to ensure all cost components have similar magnitudes (prevents one term from dominating) 2. **Instability Penalty**: Defined as: $$P_{instability} = \alpha \cdot T_{sim} \cdot \left(1 - \frac{t_{failure}}{T_{sim}}\right)$$ where $\alpha$ is `instability_penalty_factor`, $T_{sim}$ is simulation duration, and $t_{failure}$ is time to failure. 3. **Combine Weights**: Controls aggregation of mean vs. worst-case cost across uncertainty draws: $$J_{aggregated} = w_{mean} \cdot \bar{J} + w_{max} \cdot \max(J)$$ Default: $(w_{mean}, w_{max}) = (0.7, 0.3)$ ### 2.3 Optimization Workflow **Main Optimization Method:** ```python
def optimise( self, *args: Any, iters_override: Optional[int] = None, n_particles_override: Optional[int] = None, options_override: Optional[Dict[str, float]] = None, **kwargs: Any,
) -&gt; Dict[str, Any]:
``` **Parameters:** | Parameter | Type | Default | Description |

|-----------|------|---------|-------------|
| `iters_override` | `Optional[int]` | `None` | Override `pso.n_iterations` from config |
| `n_particles_override` | `Optional[int]` | `None` | Override `pso.n_particles` from config |
| `options_override` | `Optional[Dict[str, float]]` | `None` | Override PSO hyperparameters (`w`, `c1`, `c2`) | **Returns:** ```python
{ &#39;best_cost&#39;: float, # Final best fitness value &#39;best_pos&#39;: np.ndarray, # Best gain vector (1D array) &#39;cost_history&#39;: np.ndarray, # Best cost per iteration (1D array) &#39;pos_history&#39;: np.ndarray, # Best position per iteration (2D array: iters Ã— dims)
}
``` **Algorithm Flow:** 1. **Bounds Resolution**: Determine controller-specific bounds or use defaults
2. **Swarm Initialization**: Create `n_particles` particles uniformly within bounds
3. **Velocity Initialization**: Initialize velocities (typically zero or small random)
4. **Iterative Optimization**: - Update particle velocities: $\mathbf{v}_{i}^{t+1} = w\mathbf{v}_i^t + c_1 r_1 (\mathbf{p}_i - \mathbf{x}_i^t) + c_2 r_2 (\mathbf{g}^t - \mathbf{x}_i^t)$ - Update particle positions: $\mathbf{x}_i^{t+1} = \mathbf{x}_i^t + \mathbf{v}_i^{t+1}$ - Evaluate fitness for all particles (vectorized) - Update personal bests $\mathbf{p}_i$ - Update global best $\mathbf{g}^t$ - Check convergence criteria
5. **Result Extraction**: Return best position and cost history **Convergence Criteria:** The optimization stops when any of the following conditions are met: 1. **Maximum Iterations**: `iters &gt;= n_iterations`
2. **Fitness Tolerance**: $|f_{best}^t - f_{best}^{t-1}| &lt; \epsilon_{tol}$ (default: $10^{-6}$)
3. **Stagnation**: No improvement for `patience` iterations (default: 50)
4. **Keyboard Interrupt**: User cancellation **Example: Basic Optimization** ```python
# Run optimization with default settings
result = tuner.optimise() # Extract optimized gains
optimized_gains = result[&#39;best_pos&#39;]
final_cost = result[&#39;best_cost&#39;]
convergence_history = result[&#39;cost_history&#39;] # Plot convergence
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.plot(convergence_history)
plt.xlabel(&#39;Iteration&#39;)
plt.ylabel(&#39;Best Cost&#39;)
plt.title(&#39;PSO Convergence History&#39;)
plt.yscale(&#39;log&#39;)
plt.grid(True)
plt.savefig(&#39;pso_convergence.png&#39;)
``` **Example: Override Parameters** ```python
# Run with custom PSO parameters

result = tuner.optimise( iters_override=200, # More iterations n_particles_override=50, # Larger swarm options_override={&#39;w&#39;: 0.5, &#39;c1&#39;: 2.0, &#39;c2&#39;: 2.0} # Different hyperparameters
)
``` ### 2.4 Fitness Function Design **Internal Fitness Function:** ```python
def _fitness(self, particles: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot;Vectorised fitness function for a swarm of particles.&quot;&quot;&quot;
``` The fitness function evaluates controller performance for a batch of gain vectors simultaneously. **Fitness Computation Pipeline:** ```

Particle Gains (B Ã— D) â”‚ â”œâ”€â”€&gt; Bounds Validation (filter invalid gains) â”‚ â–¼
Controller Creation (B controllers) â”‚ â–¼
Batch Simulation (vectorized) â”‚ â”œâ”€â”€&gt; Trajectory: (t, x_b, u_b, Ïƒ_b) â”‚ â–¼
Cost Computation per Particle â”‚ â”œâ”€â”€&gt; State Error (ISE) â”œâ”€â”€&gt; Control Effort (UÂ²) â”œâ”€â”€&gt; Control Slew (Î”UÂ²) â”œâ”€â”€&gt; Stability (ÏƒÂ²) â””â”€â”€&gt; Instability Penalty (early failure) â”‚ â–¼
Cost Normalization â”‚ â–¼
Weighted Aggregation â”‚ â–¼
Final Fitness Values (B Ã— 1)
``` **Mathematical Definition:** The fitness function computes: $$
J(\mathbf{gains}) = w_1 \cdot \text{ISE}_n + w_2 \cdot U_n^2 + w_3 \cdot (\Delta U)_n^2 + w_4 \cdot \sigma_n^2 + P_{inst}
$$ where: **State Error (Integrated Squared Error):**
$$
\text{ISE} = \int_0^T \|\mathbf{x}(t)\|^2 \, dt \approx \sum_{k=1}^{N} \|\mathbf{x}_k\|^2 \cdot \Delta t
$$ **Control Effort:**
$$
U^2 = \int_0^T u^2(t) \, dt \approx \sum_{k=1}^{N} u_k^2 \cdot \Delta t
$$ **Control Slew Rate:**
$$
(\Delta U)^2 = \int_0^T \left(\frac{du}{dt}\right)^2 dt \approx \sum_{k=1}^{N} (u_{k} - u_{k-1})^2 / \Delta t
$$ **Sliding Variable Energy:**
$$
\sigma^2 = \int_0^T \sigma^2(t) \, dt \approx \sum_{k=1}^{N} \sigma_k^2 \cdot \Delta t
$$ **Instability Penalty:**
$$
P_{inst} = \begin{cases}
0 &amp; \text{if trajectory stable for } t \in [0, T] \\
\alpha \cdot (T - t_{failure}) / T &amp; \text{if failure at } t_{failure}
\end{cases}
$$ **Normalization:** Each term is normalized by baseline values:
$$
\text{ISE}_n = \frac{\text{ISE}}{\text{ISE}_{baseline}}, \quad U_n = \frac{U}{\sqrt{U_{baseline}^2}}, \text{ etc.}
$$ **Design Guidelines:** 1. **Weights Selection**: - Start with $w_1 = 1.0$ (state error dominates) - Set $w_2 = 0.1$ (control effort secondary) - Set $w_3 = 0.05$ (control smoothness) - Set $w_4 = 0.5$ (stability term moderate importance) 2. **Instability Penalty Factor**: - Use $\alpha = 100$ for balanced penalty (recommended) - Increase to $\alpha = 1000$ for aggressive instability avoidance - Decrease to $\alpha = 10$ if overpenalizing early failures 3. **Uncertainty Evaluation**: - Set `physics_uncertainty.n_evals = 1` for fast optimization (no robustness)
   - Set `physics_uncertainty.n_evals = 5-10` for robust controllers (recommended)
   - Higher `n_evals` increases optimization time linearly

**Custom Fitness Function Example:** For advanced users, custom fitness functions can be designed: ```python
# example-metadata:
# runnable: false def custom_fitness(particles: np.ndarray) -&gt; np.ndarray: &quot;&quot;&quot; Custom fitness function for specific control objectives. Parameters ---------- particles : np.ndarray Gain vectors (shape: B Ã— D) Returns ------- np.ndarray Fitness values (shape: B,) &quot;&quot;&quot; B = particles.shape[0] fitness = np.zeros(B) for i, gains in enumerate(particles): # Create controller controller = create_controller(&#39;classical_smc&#39;, config=config, gains=gains) # Simulate result = simulate(controller, duration=5.0, dt=0.01) # Custom cost: settle time + overshoot settle_time = compute_settle_time(result.states, threshold=0.02) overshoot = compute_overshoot(result.states) fitness[i] = 10.0 * settle_time + 50.0 * overshoot return fitness
``` ### 2.5 Cost Normalization **Normalization Method:** ```python

def _normalise(self, val: np.ndarray, denom: float) -&gt; np.ndarray: &quot;&quot;&quot;Safely normalise an array by a scalar denominator using the instance&#39;s threshold.&quot;&quot;&quot;
``` **Purpose:** Prevent numerical issues and balance multi-scale cost components. **Algorithm:** $$
\text{normalised}(v, d) = \begin{cases}
v / d &amp; \text{if } d &gt; \epsilon_{threshold} \\
v &amp; \text{if } d \leq \epsilon_{threshold}
\end{cases}
$$ where $\epsilon_{threshold} = 10^{-12}$ (default). **Baseline Computation:** During initialization, PSOTuner computes baseline costs using default gains: 1. Run simulation with default controller gains
2. Compute $\text{ISE}_{baseline}$, $U_{baseline}$, $\Delta U_{baseline}$, $\sigma_{baseline}$
3. Store as normalization constants **Effect on Optimization:** - **Without normalization**: State error (large magnitude) dominates other terms
- **With normalization**: All terms contribute proportionally to weighted sum **Example:** ```
Raw costs (unnormalized): ISE = 2.5e3 (large) UÂ² = 1.2e1 (small) (Î”U)Â² = 3.4e0 (smaller) ÏƒÂ² = 8.7e2 (medium) Normalized costs (after normalization with baselines): ISE_n = 1.25 (order 1) U_n = 1.08 (order 1) (Î”U)_n = 0.93 (order 1) Ïƒ_n = 1.14 (order 1) Weighted sum (w = [1.0, 0.1, 0.05, 0.5]): J = 1.0*1.25 + 0.1*1.08 + 0.05*0.93 + 0.5*1.14 = 1.25 + 0.108 + 0.047 + 0.57 = 1.975
``` **Cross-References:**

- **Theory**: [Phase 2.2, Section 7.1: PSO Cost Function Design](../theory/pso_algorithm_foundations.md#71-cost-function-design)
- **Factory**: [Phase 4.2, Section 5.1: Fitness Function Integration](factory_system_api_reference.md#51-fitness-function-integration)

---

## 3. Convergence Analysis API ### 3.1 EnhancedConvergenceAnalyzer Class **Location:** `src/optimization/validation/enhanced_convergence_analyzer.py` ```python

class EnhancedConvergenceAnalyzer: &quot;&quot;&quot; Advanced PSO convergence analysis with multi-criteria validation. Provides convergence monitoring, statistical validation, and performance prediction for PSO optimization in controller factory integration scenarios. &quot;&quot;&quot;
``` **Key Features:**
- Multi-modal convergence detection (5 criteria)
- Statistical significance testing (Welch&#39;s t-test, Mann-Whitney U)
- Real-time convergence probability estimation
- Performance prediction with confidence intervals
- Controller-specific adaptive criteria
- Population diversity analysis
- Stagnation detection with gradient-based methods ### 3.2 Convergence Metrics **Dataclass Definition:** ```python
# example-metadata:
# runnable: false @dataclass
class ConvergenceMetrics: &quot;&quot;&quot;convergence metrics.&quot;&quot;&quot; iteration: int # Current iteration number best_fitness: float # Current best fitness value mean_fitness: float # Mean fitness across swarm fitness_std: float # Fitness standard deviation population_diversity: float # Swarm diversity measure convergence_velocity: float # Rate of convergence improvement_rate: float # Relative improvement rate stagnation_score: float # Stagnation indicator [0, 1] diversity_loss_rate: float # Rate of diversity decrease predicted_iterations_remaining: int # Estimated iterations to convergence confidence_level: float # Statistical confidence [0, 1] convergence_probability: float # Probability of convergence [0, 1]
``` **Metric Computation:** 1. **Population Diversity:** $$ D_{swarm} = \frac{1}{N} \sum_{i=1}^{N} \|\mathbf{x}_i - \bar{\mathbf{x}}\| $$ where $\bar{\mathbf{x}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{x}_i$ is swarm centroid. 2. **Convergence Velocity:** $$ v_{conv}(t) = \frac{f_{best}^{t-w} - f_{best}^t}{w} $$ Average improvement over window $w$ (default: 10 iterations). 3. **Improvement Rate:** $$ r_{imp}(t) = \frac{f_{best}^{t-1} - f_{best}^t}{f_{best}^{t-1}} $$ 4. **Stagnation Score:** $$ S_{stag}(t) = 1 - \exp\left(-\frac{t - t_{last\_improvement}}{\tau}\right) $$ where $\tau = 20$ (time constant), $t_{last\_improvement}$ is iteration of last significant improvement. 5. **Diversity Loss Rate:** $$ r_{div}(t) = \frac{D_{swarm}^{t-1} - D_{swarm}^t}{D_{swarm}^{t-1}} $$ ### 3.3 Convergence Criteria **Dataclass Definition:** ```python
# example-metadata:

# runnable: false @dataclass

class ConvergenceCriteria: &quot;&quot;&quot;Adaptive convergence criteria configuration.&quot;&quot;&quot; # Fitness-based criteria fitness_tolerance: float = 1e-6 relative_improvement_threshold: float = 1e-4 # Diversity-based criteria min_diversity_threshold: float = 1e-3 diversity_loss_rate_threshold: float = 0.95 # Stagnation detection stagnation_window: int = 10 stagnation_threshold: float = 1e-5 # Statistical criteria statistical_confidence_level: float = 0.95 min_sample_size: int = 20 # Adaptive parameters enable_adaptive_criteria: bool = True controller_specific_adjustment: bool = True # Performance prediction enable_performance_prediction: bool = True prediction_window: int = 15 # Early stopping max_stagnation_iterations: int = 50 premature_convergence_detection: bool = True
``` **Multi-Criteria Convergence Detection:** The analyzer declares convergence when **at least 3 out of 5** criteria are satisfied: 1. **Fitness Tolerance:** $$ |f_{best}^t - f_{best}^{t-1}| &lt; \epsilon_{tol} $$ 2. **Relative Improvement:** $$ \frac{f_{best}^{t-w} - f_{best}^t}{f_{best}^{t-w}} &lt; \epsilon_{rel} $$ over window $w$ = `stagnation_window`. 3. **Population Diversity:** $$ D_{swarm}^t &lt; D_{min} $$ 4. **Statistical Significance:** Welch&#39;s t-test on fitness improvements over last `min_sample_size` iterations yields $p &gt; 1 - \alpha_{conf}$. 5. **Stagnation Detection:** No improvement $&gt; \epsilon_{stag}$ for `max_stagnation_iterations`. **Controller-Specific Tuning:** | Controller Type | `fitness_tolerance` | `min_diversity_threshold` | `max_stagnation_iterations` |
|----------------|---------------------|---------------------------|----------------------------|
| Classical SMC | $10^{-6}$ | $10^{-3}$ | 50 |
| STA SMC | $10^{-5}$ | $10^{-2}$ | 40 |
| Adaptive SMC | $10^{-6}$ | $5 \times 10^{-4}$ | 60 |
| Hybrid STA | $10^{-5}$ | $10^{-3}$ | 50 | ### 3.4 Real-Time Monitoring **Method:** ```python
# example-metadata:
# runnable: false def check_convergence( self, iteration: int, best_fitness: float, mean_fitness: float, fitness_std: float, swarm_positions: np.ndarray
) -&gt; Tuple[ConvergenceStatus, ConvergenceMetrics]: &quot;&quot;&quot; Analyze current optimization state and determine convergence status. Returns ------- status : ConvergenceStatus Current convergence status (EXPLORING, CONVERGING, CONVERGED, etc.) metrics : ConvergenceMetrics metrics for current iteration &quot;&quot;&quot;
``` **Example: Integration with PSO Loop** ```python
# example-metadata:

# runnable: false from src.optimization.validation.enhanced_convergence_analyzer import ( EnhancedConvergenceAnalyzer, ConvergenceCriteria, ConvergenceStatus

) # Initialize analyzer
criteria = ConvergenceCriteria( fitness_tolerance=1e-6, max_stagnation_iterations=50, enable_performance_prediction=True
)
analyzer = EnhancedConvergenceAnalyzer( criteria=criteria, controller_type=SMCType.CLASSICAL
) # PSO optimization loop (pseudo-code)
for iteration in range(max_iterations): # ... PSO updates ... # Check convergence status, metrics = analyzer.check_convergence( iteration=iteration, best_fitness=current_best_fitness, mean_fitness=swarm_mean_fitness, fitness_std=swarm_fitness_std, swarm_positions=particle_positions ) # Log metrics print(f&quot;Iteration {iteration}:&quot;) print(f&quot; Status: {status.value}&quot;) print(f&quot; Best Fitness: {metrics.best_fitness:.6f}&quot;) print(f&quot; Convergence Velocity: {metrics.convergence_velocity:.6e}&quot;) print(f&quot; Diversity: {metrics.population_diversity:.6f}&quot;) print(f&quot; Predicted Iterations Remaining: {metrics.predicted_iterations_remaining}&quot;) # Early stopping if status == ConvergenceStatus.CONVERGED: print(f&quot;Convergence detected at iteration {iteration}&quot;) break elif status == ConvergenceStatus.STAGNATED: print(f&quot;Stagnation detected at iteration {iteration}&quot;) break
``` **Convergence Status Values:** | Status | Description | Recommendation |
|--------|-------------|----------------|
| `NOT_STARTED` | Initial state before any iterations | N/A |
| `INITIALIZING` | First few iterations (population spreading) | Continue |
| `EXPLORING` | High diversity, rapid fitness changes | Continue |
| `CONVERGING` | Decreasing diversity, steady improvement | Continue, monitor closely |
| `CONVERGED` | All criteria satisfied | Stop optimization |
| `STAGNATED` | No improvement, low diversity | Stop or restart with new initialization |
| `OSCILLATING` | Fitness oscillating, unstable | Reduce inertia weight or learning rates |
| `DIVERGING` | Fitness increasing | Check fitness function or restart |
| `PREMATURE_CONVERGENCE` | Converged but diversity still high | Possible local minimum, consider restart |
| `FAILED` | Numerical errors or invalid states | Debug fitness function | **Cross-References:**
- **Theory**: [Phase 2.2, Section 2: Convergence Theorems](../theory/pso_algorithm_foundations.md#2-convergence-theorems)
- **Factory**: [Phase 4.2, Section 6.2: PSO Convergence Monitoring](factory_system_api_reference.md#62-pso-convergence-monitoring)

---

## 4. Bounds Validation API ### 4.1 PSOBoundsValidator Class **Location:** `src/optimization/validation/pso_bounds_validator.py` ```python
class PSOBoundsValidator: &quot;&quot;&quot; Advanced PSO bounds validator for control system optimization. This class provides validation and optimization of PSO parameter bounds to ensure effective controller tuning. &quot;&quot;&quot;
``` **Initialization:** ```python

def __init__(self, config: ConfigSchema): &quot;&quot;&quot; Initialize bounds validator with configuration. Parameters ---------- config : ConfigSchema System configuration with controller and PSO parameters &quot;&quot;&quot;
``` ### 4.2 Controller-Specific Bounds **Bounds Specification:** Each controller type has specific parameter bounds derived from stability analysis and physical constraints: | Controller | Parameters | Recommended Ranges | Constraints |
|-----------|------------|-------------------|-------------|
| **Classical SMC** | 6 gains: `[k1, k2, Î»1, Î»2, K, kd]` | k1: [1, 100]&lt;br&gt;k2: [1, 100]&lt;br&gt;Î»1: [0.1, 50]&lt;br&gt;Î»2: [0.1, 50]&lt;br&gt;K: [1, 200]&lt;br&gt;kd: [0.1, 20] | All &gt; 0 |
| **STA SMC** | 6 gains: `[k1, k2, Î»1, Î»2, Î±, Î²]` | k1: [1, 80]&lt;br&gt;k2: [1, 80]&lt;br&gt;Î»1: [0.5, 30]&lt;br&gt;Î»2: [0.5, 30]&lt;br&gt;Î±: [0.1, 10]&lt;br&gt;Î²: [0.1, 10] | Î± &gt; Î² (STA condition) |
| **Adaptive SMC** | 5 gains: `[k1, k2, Î»1, Î»2, Î³]` | k1: [1, 60]&lt;br&gt;k2: [1, 60]&lt;br&gt;Î»1: [0.5, 25]&lt;br&gt;Î»2: [0.5, 25]&lt;br&gt;Î³: [0.1, 10] | Exactly 5 gains |
| **Hybrid STA** | 4 gains: `[c1, Î»1, c2, Î»2]` | c1: [1, 50]&lt;br&gt;Î»1: [0.5, 30]&lt;br&gt;c2: [1, 50]&lt;br&gt;Î»2: [0.5, 30] | c1, c2 balanced | **Physical Interpretations:** - **k1, k2**: Position and velocity feedback gains (higher â†’ faster response, lower â†’ smoother)
- **Î»1, Î»2**: Sliding surface slopes for links 1 and 2 (determines convergence rate to surface)
- **K**: Switching gain magnitude (must overcome maximum disturbance)
- **kd**: Derivative gain for damping
- **Î±, Î²**: Super-twisting gains (Î± controls reaching phase, Î² for sliding phase)
- **Î³**: Adaptation rate (higher â†’ faster parameter estimation) ### 4.3 Validation Rules **Method:** ```python
# example-metadata:
# runnable: false def validate_bounds( self, controller_type: str, lower_bounds: List[float], upper_bounds: List[float]
) -&gt; BoundsValidationResult: &quot;&quot;&quot; Validate PSO parameter bounds for specific controller type. Parameters ---------- controller_type : str Controller type (&#39;classical_smc&#39;, &#39;sta_smc&#39;, &#39;adaptive_smc&#39;, &#39;hybrid_adaptive_sta_smc&#39;) lower_bounds : List[float] Lower bounds for each parameter upper_bounds : List[float] Upper bounds for each parameter Returns ------- BoundsValidationResult Validation result with warnings, recommendations, and adjusted bounds &quot;&quot;&quot;
``` **Validation Checks:** 1. **Length Validation:** - Bounds length must match controller parameter count - Classical SMC: 6, STA SMC: 6, Adaptive: 5, Hybrid: 4 2. **Positivity Constraints:** - All bounds must be positive (control gains are positive-definite) 3. **Range Constraints:** - Lower bound &lt; Upper bound for each parameter - Range width &gt;= minimum threshold (avoid degenerate search space) 4. **Physical Constraints:** - STA SMC: $\alpha_{lower} &gt; \beta_{upper}$ (ensure STA condition) - Stability margins: $K_{lower} &gt; K_{min}(\text{physics})$ (overcome disturbances) 5. **Search Space Quality:** - Bounds not too wide (difficult convergence) - Bounds not too narrow (local minima risk) - Recommended: $\log_{10}(\text{upper}/\text{lower}) \in [1, 2]$ **Example:** ```python

from src.optimization.validation.pso_bounds_validator import PSOBoundsValidator
from src.config import load_config config = load_config(&quot;config.yaml&quot;)
validator = PSOBoundsValidator(config) # Validate bounds for Classical SMC
result = validator.validate_bounds( controller_type=&#39;classical_smc&#39;, lower_bounds=[1.0, 1.0, 0.5, 0.5, 1.0, 0.1], upper_bounds=[100.0, 100.0, 50.0, 50.0, 200.0, 20.0]
) if result.is_valid: print(&quot;Bounds are valid!&quot;)
else: print(&quot;Validation warnings:&quot;) for warning in result.warnings: print(f&quot; - {warning}&quot;) print(&quot;\nRecommendations:&quot;) for rec in result.recommendations: print(f&quot; - {rec}&quot;) if result.adjusted_bounds: print(&quot;\nAdjusted bounds:&quot;) print(f&quot; Lower: {result.adjusted_bounds[&#39;lower&#39;]}&quot;) print(f&quot; Upper: {result.adjusted_bounds[&#39;upper&#39;]}&quot;)
``` ### 4.4 Automatic Adjustment **Adjustment Algorithm:** When bounds fail validation, the validator can automatically adjust them: 1. **Expand Narrow Ranges:** If $\text{upper}_i / \text{lower}_i &lt; 5$: $$ \text{lower}_i^{new} = \text{lower}_i / 2, \quad \text{upper}_i^{new} = \text{upper}_i \times 2 $$ 2. **Contract Wide Ranges:** If $\text{upper}_i / \text{lower}_i &gt; 100$: $$ \text{lower}_i^{new} = \text{lower}_i \times 2, \quad \text{upper}_i^{new} = \text{upper}_i / 2 $$ 3. **Fix Constraint Violations:** - STA condition: Set $\alpha_{lower} = 1.2 \times \beta_{upper}$ - Positivity: Set $\text{lower}_i = \max(\text{lower}_i, 0.01)$ 4. **Physics-Based Bounds:** Compute minimum switching gain from system parameters: $$ K_{min} = \rho \cdot (m_{cart} + m_1 + m_2) \cdot g \cdot L_{max} $$ where $\rho = 2.0$ (safety factor), $L_{max}$ is maximum pendulum reach. **Cross-References:**
- **Theory**: [Phase 2.2, Section 7.2: Bounds Selection Rationale](../theory/pso_algorithm_foundations.md#72-bounds-selection-rationale)
- **Factory**: [Phase 4.2, Section 5.3: Gain Validation Rules](factory_system_api_reference.md#53-gain-validation-rules)

---

## 5. Bounds Optimization API ### 5.1 PSOBoundsOptimizer Class **Location:** `src/optimization/validation/pso_bounds_optimizer.py` ```python
class PSOBoundsOptimizer: &quot;&quot;&quot; Optimize PSO parameter bounds for improved convergence and performance. Implements multi-strategy bounds optimization combining physics-based constraints, empirical performance data, and PSO convergence properties. &quot;&quot;&quot;
``` ### 5.2 Optimization Strategies **Strategy Enum:** ```python

class BoundsOptimizationStrategy(Enum): &quot;&quot;&quot;Bounds optimization strategies.&quot;&quot;&quot; PHYSICS_BASED = &quot;physics_based&quot; # Stability-constrained bounds PERFORMANCE_DRIVEN = &quot;performance_driven&quot; # Empirically validated bounds CONVERGENCE_FOCUSED = &quot;convergence_focused&quot; # PSO-optimized bounds HYBRID = &quot;hybrid&quot; # Weighted combination
``` **Strategy Descriptions:** 1. **PHYSICS_BASED:** - Derives bounds from controller stability analysis - Uses Lyapunov stability conditions to determine minimum gains - Computes maximum gains from actuator saturation limits - **Pros**: Guaranteed stability, theoretically sound - **Cons**: May be overly conservative 2. **PERFORMANCE_DRIVEN:** - Analyzes historical optimization results - Identifies parameter ranges that produced best controllers - Uses percentile-based bounds (e.g., 5th-95th percentile of successful gains) - **Pros**: Empirically validated, practical - **Cons**: Requires historical data 3. **CONVERGENCE_FOCUSED:** - Optimizes bounds to improve PSO convergence rate - Minimizes: $J_{bounds} = w_1 \cdot t_{conv} + w_2 \cdot N_{evals} + w_3 \cdot (1 - q_{final})$ - Where $t_{conv}$ = convergence time, $N_{evals}$ = function evaluations, $q_{final}$ = solution quality - **Pros**: Fast optimization, fewer iterations - **Cons**: May sacrifice solution quality for speed 4. **HYBRID (Recommended):** - Weighted combination of all three strategies - Default weights: $(w_{phys}, w_{perf}, w_{conv}) = (0.4, 0.4, 0.2)$ - Balances theoretical soundness, practical performance, and convergence speed - **Pros**: Robust, balanced approach - **Cons**: Requires tuning of strategy weights ### 5.3 Multi-Criteria Selection **Optimization Method:** ```python
# example-metadata:
# runnable: false def optimize_bounds_for_controller( self, controller_type: SMCType, strategy: BoundsOptimizationStrategy = BoundsOptimizationStrategy.HYBRID, max_optimization_time: float = 300.0, n_trials: int = 10
) -&gt; BoundsValidationResult: &quot;&quot;&quot; Optimize PSO parameter bounds for specific controller type. Algorithm: 1. Generate candidate bounds from selected strategy 2. Evaluate candidates through PSO trials 3. Score candidates using multi-criteria objective 4. Select optimal bounds via Pareto dominance 5. Validate through testing Parameters ---------- controller_type : SMCType Controller type to optimize bounds for strategy : BoundsOptimizationStrategy, optional Optimization strategy (default: HYBRID) max_optimization_time : float, optional Maximum time in seconds (default: 300) n_trials : int, optional Number of PSO trials per candidate (default: 10) Returns ------- BoundsValidationResult Optimized bounds with performance metrics &quot;&quot;&quot;
``` **Multi-Criteria Objective:** Bounds are scored using: $$

J_{bounds} = w_1 \cdot R_{conv} + w_2 \cdot Q_{final} + w_3 \cdot P_{success} + w_4 \cdot S_{robust}
$$ where:
- $R_{conv}$: Convergence rate improvement (normalized)
- $Q_{final}$: Final cost quality improvement (normalized)
- $P_{success}$: Success rate across trials ([0, 1])
- $S_{robust}$: Robustness score (performance variance metric)
- Weights: $(w_1, w_2, w_3, w_4) = (0.3, 0.4, 0.2, 0.1)$ **Example:** ```python
from src.optimization.validation.pso_bounds_optimizer import ( PSOBoundsOptimizer, BoundsOptimizationStrategy
)
from src.controllers.factory import SMCType
from src.config import load_config config = load_config(&quot;config.yaml&quot;)
optimizer = PSOBoundsOptimizer(config) # Optimize bounds for Classical SMC
result = optimizer.optimize_bounds_for_controller( controller_type=SMCType.CLASSICAL, strategy=BoundsOptimizationStrategy.HYBRID, max_optimization_time=600.0, n_trials=20
) print(f&quot;Optimized Bounds:&quot;)
print(f&quot; Lower: {result.adjusted_bounds[&#39;lower&#39;]}&quot;)
print(f&quot; Upper: {result.adjusted_bounds[&#39;upper&#39;]}&quot;)
print(f&quot;\nPerformance Improvements:&quot;)
print(f&quot; Convergence: {result.convergence_estimate:.2%}&quot;)
print(f&quot; Quality: {result.stability_analysis[&#39;quality_improvement&#39;]:.2%}&quot;)
print(f&quot; Success Rate: {result.stability_analysis[&#39;success_rate&#39;]:.2%}&quot;)
``` **Cross-References:**
- **Theory**: [Phase 2.2, Section 4: Parameter Sensitivity](../theory/pso_algorithm_foundations.md#4-parameter-sensitivity)
- **Factory**: [Phase 4.2, Section 5.4: Bounds Management](factory_system_api_reference.md#54-bounds-management)

---

## 6. Hyperparameter Optimization API ### 6.1 PSOHyperparameterOptimizer Class **Location:** `src/optimization/tuning/pso_hyperparameter_optimizer.py` ```python
class PSOHyperparameterOptimizer: &quot;&quot;&quot; Meta-optimization of PSO hyperparameters for controller tuning. Optimizes PSO algorithm parameters (w, c1, c2, swarm_size) to improve convergence speed and solution quality for specific controller types. &quot;&quot;&quot;
``` ### 6.2 Meta-Optimization **Hyperparameter Space:** The optimizer tunes 4 PSO hyperparameters: | Parameter | Symbol | Recommended Range | Physical Meaning |

|-----------|--------|-------------------|------------------|
| Inertia weight | $w$ | [0.4, 0.9] | Momentum (high â†’ exploration, low â†’ exploitation) |
| Cognitive coefficient | $c_1$ | [1.0, 2.5] | Personal best attraction strength |
| Social coefficient | $c_2$ | [1.0, 2.5] | Global best attraction strength |
| Swarm size | $N$ | [10, 50] | Number of particles | **Meta-Optimization Objective:** ```python
class OptimizationObjective(Enum): &quot;&quot;&quot;Meta-optimization objectives.&quot;&quot;&quot; CONVERGENCE_SPEED = &quot;convergence_speed&quot; # Minimize iterations to convergence SOLUTION_QUALITY = &quot;solution_quality&quot; # Minimize final cost ROBUSTNESS = &quot;robustness&quot; # Minimize performance variance EFFICIENCY = &quot;efficiency&quot; # Balance quality vs. computational cost MULTI_OBJECTIVE = &quot;multi_objective&quot; # Weighted combination
``` **Objective Formulations:** 1. **CONVERGENCE_SPEED:** $$ J_{speed}(\mathbf{h}) = t_{conv}(\mathbf{h}) $$ where $\mathbf{h} = [w, c_1, c_2, N]$, $t_{conv}$ is number of iterations to convergence. 2. **SOLUTION_QUALITY:** $$ J_{quality}(\mathbf{h}) = f_{final}(\mathbf{h}) $$ where $f_{final}$ is best fitness at convergence. 3. **ROBUSTNESS:** $$ J_{robust}(\mathbf{h}) = \text{Var}(f_{final}) + \text{Var}(t_{conv}) $$ Variance computed over multiple PSO runs. 4. **EFFICIENCY:** $$ J_{eff}(\mathbf{h}) = \frac{f_{final}}{f_{baseline}} + \lambda \cdot \frac{N \cdot t_{conv}}{N_{baseline} \cdot t_{baseline}} $$ Balances solution quality against computational cost ($\lambda = 0.3$ typical). 5. **MULTI_OBJECTIVE:** $$ J_{multi}(\mathbf{h}) = w_1 J_{speed} + w_2 J_{quality} + w_3 J_{robust} + w_4 J_{eff} $$ Default weights: $(w_1, w_2, w_3, w_4) = (0.2, 0.4, 0.2, 0.2)$ ### 6.3 Multi-Objective Optimization **Method:** ```python
# example-metadata:
# runnable: false def optimize_hyperparameters( self, controller_type: SMCType, objective: OptimizationObjective = OptimizationObjective.MULTI_OBJECTIVE, max_evaluations: int = 100, n_trials_per_evaluation: int = 5
) -&gt; OptimizationResult: &quot;&quot;&quot; Optimize PSO hyperparameters for specific controller type. Uses differential evolution to find optimal PSO parameters that minimize the selected objective function. Parameters ---------- controller_type : SMCType Controller type to optimize hyperparameters for objective : OptimizationObjective, optional Optimization objective (default: MULTI_OBJECTIVE) max_evaluations : int, optional Maximum DE evaluations (default: 100) n_trials_per_evaluation : int, optional PSO trials per hyperparameter configuration (default: 5) Returns ------- OptimizationResult Optimized hyperparameters with performance metrics &quot;&quot;&quot;
``` **Optimization Algorithm:** Uses Differential Evolution (DE) for meta-optimization: 1. **Initialize Population**: Random hyperparameter configurations within bounds

2. **Evaluate Fitness**: Run PSO with each configuration, compute objective
3. **Mutation**: $\mathbf{v}_i = \mathbf{h}_{r1} + F \cdot (\mathbf{h}_{r2} - \mathbf{h}_{r3})$
4. **Crossover**: Mix mutant with current hyperparameters
5. **Selection**: Keep better configuration
6. **Iterate**: Until convergence or max evaluations **Example:** ```python
from src.optimization.tuning.pso_hyperparameter_optimizer import ( PSOHyperparameterOptimizer, OptimizationObjective
)
from src.controllers.factory import SMCType
from src.config import load_config config = load_config(&quot;config.yaml&quot;)
meta_optimizer = PSOHyperparameterOptimizer(config) # Optimize PSO hyperparameters for Classical SMC
result = meta_optimizer.optimize_hyperparameters( controller_type=SMCType.CLASSICAL, objective=OptimizationObjective.MULTI_OBJECTIVE, max_evaluations=100, n_trials_per_evaluation=5
) print(f&quot;Optimized PSO Hyperparameters:&quot;)
print(f&quot; Inertia weight (w): {result.hyperparameters.w:.4f}&quot;)
print(f&quot; Cognitive (c1): {result.hyperparameters.c1:.4f}&quot;)
print(f&quot; Social (c2): {result.hyperparameters.c2:.4f}&quot;)
print(f&quot; Swarm size: {result.hyperparameters.n_particles}&quot;)
print(f&quot;\nPerformance vs. Baseline:&quot;)
print(f&quot; Convergence speedup: {result.convergence_improvement:.2f}x&quot;)
print(f&quot; Quality improvement: {result.quality_improvement:.2%}&quot;)
print(f&quot; Robustness improvement: {result.robustness_improvement:.2%}&quot;) # Update configuration with optimized hyperparameters
config.pso.w = result.hyperparameters.w
config.pso.c1 = result.hyperparameters.c1
config.pso.c2 = result.hyperparameters.c2
config.pso.n_particles = result.hyperparameters.n_particles
``` **Baseline Hyperparameters:** Default PSO hyperparameters for each controller type (empirically validated): | Controller | w | c1 | c2 | N | Rationale |
|-----------|---|----|----|---|-----------|
| Classical SMC | 0.729 | 1.494 | 1.494 | 30 | Clerc&#39;s constriction factor |
| STA SMC | 0.600 | 1.700 | 1.700 | 25 | More exploitation (Î±,Î² coupling) |
| Adaptive SMC | 0.750 | 1.400 | 1.600 | 35 | Higher social (Î³ estimation) |
| Hybrid STA | 0.650 | 1.550 | 1.750 | 30 | Balanced (complex landscape) | **Cross-References:**
- **Theory**: [Phase 2.2, Section 3: Parameter Sensitivity Analysis](../theory/pso_algorithm_foundations.md#3-parameter-sensitivity-analysis)
- **Factory**: [Phase 4.2, Section 6.3: Hyperparameter Configuration](factory_system_api_reference.md#63-hyperparameter-configuration)

---

## 7. Factory Integration API ### 7.1 EnhancedPSOFactory **Location:** `src/optimization/integration/pso_factory_bridge.py` ```python
class EnhancedPSOFactory: &quot;&quot;&quot; Enhanced PSO-Factory integration with robust error handling. Provides integration between controller factory and PSO optimization with enhanced fitness functions and error recovery. &quot;&quot;&quot;
``` **Key Features:**

- Automatic controller factory creation from configuration
- Enhanced fitness functions with robustness evaluation
- Graceful degradation on optimization failures
- Result validation and post-processing
- Integration with convergence analyzer ### 7.2 Integration Patterns **Complete Factory â†’ PSO â†’ Validation Workflow:** ```python
from src.optimization.integration.pso_factory_bridge import EnhancedPSOFactory
from src.config import load_config # 1. Load configuration
config = load_config(&quot;config.yaml&quot;) # 2. Create enhanced PSO factory
factory = EnhancedPSOFactory( controller_type=&#39;classical_smc&#39;, config=config, enable_convergence_monitoring=True, enable_bounds_validation=True
) # 3. Run optimization
result = factory.optimize( max_iterations=100, convergence_tolerance=1e-6
) # 4. Extract optimized controller
optimized_controller = factory.create_controller(result[&#39;best_pos&#39;]) # 5. Validate performance
validation_result = factory.validate_controller( controller=optimized_controller, n_trials=10
) print(f&quot;Optimization Summary:&quot;)
print(f&quot; Best Cost: {result[&#39;best_cost&#39;]:.6f}&quot;)
print(f&quot; Convergence Iteration: {result[&#39;convergence_iteration&#39;]}&quot;)
print(f&quot; Validation Success Rate: {validation_result[&#39;success_rate&#39;]:.2%}&quot;)
print(f&quot; Mean Performance: {validation_result[&#39;mean_cost&#39;]:.6f} Â± {validation_result[&#39;std_cost&#39;]:.6f}&quot;)
``` **Advanced Pattern: Multi-Controller Comparison:** ```python
from src.optimization.integration.pso_factory_bridge import EnhancedPSOFactory
from src.controllers.factory import SMCType
import pandas as pd # Optimize all controller types
controller_types = [SMCType.CLASSICAL, SMCType.STA, SMCType.ADAPTIVE, SMCType.HYBRID]
results = [] for ctrl_type in controller_types: factory = EnhancedPSOFactory( controller_type=ctrl_type, config=config ) result = factory.optimize(max_iterations=100) results.append({ &#39;controller&#39;: ctrl_type.value, &#39;best_cost&#39;: result[&#39;best_cost&#39;], &#39;convergence_iter&#39;: result[&#39;convergence_iteration&#39;], &#39;optimization_time&#39;: result[&#39;optimization_time&#39;] }) # Compare results
df = pd.DataFrame(results)
df = df.sort_values(&#39;best_cost&#39;)
print(&quot;\nController Performance Ranking:&quot;)
print(df.to_string(index=False))
``` **Cross-References:**

- **Factory API**: [Phase 4.2: Factory System API Reference (Complete)](factory_system_api_reference.md)
- **Theory**: [Phase 2.2: PSO Algorithm Foundations](../theory/pso_algorithm_foundations.md)

---

## 8. Complete Code Examples ### 8.1 Basic PSO Optimization **Objective:** Optimize Classical SMC controller gains for double inverted pendulum. ```python

# example-metadata:

# runnable: false #!/usr/bin/env python3

&quot;&quot;&quot;
Example 1: Basic PSO Optimization for Classical SMC Demonstrates:
- Configuration loading
- Controller factory creation
- PSO tuner initialization
- Optimization execution
- Result visualization
&quot;&quot;&quot; import matplotlib.pyplot as plt
import numpy as np
from functools import partial from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.controllers.factory import create_controller
from src.config import load_config # ============================================================================
# Configuration

# ============================================================================ CONFIG_PATH = &quot;config.yaml&quot;

CONTROLLER_TYPE = &#39;classical_smc&#39;
SEED = 42 # ============================================================================
# Main Optimization

# ============================================================================ def main(): # Load configuration print(&quot;Loading configuration...&quot;) config = load_config(CONFIG_PATH) # Create controller factory (partial application for PSO) print(&quot;Creating controller factory...&quot;) controller_factory = partial( create_controller, controller_type=CONTROLLER_TYPE, config=config ) # Initialize PSO tuner print(&quot;Initializing PSO tuner...&quot;) tuner = PSOTuner( controller_factory=controller_factory, config=config, seed=SEED, instability_penalty_factor=100.0 ) # Run optimization print(f&quot;Running PSO optimization with {config.pso.n_particles} particles for {config.pso.n_iterations} iterations...&quot;) result = tuner.optimise() # Extract results best_gains = result[&#39;best_pos&#39;] best_cost = result[&#39;best_cost&#39;] cost_history = result[&#39;cost_history&#39;] print(f&quot;\n{&#39;=&#39;*80}&quot;) print(&quot;OPTIMIZATION RESULTS&quot;) print(f&quot;{&#39;=&#39;*80}&quot;) print(f&quot;Best Cost: {best_cost:.6f}&quot;) print(f&quot;Best Gains: {best_gains}&quot;) print(f&quot;Convergence: {len(cost_history)} iterations&quot;) print(f&quot;{&#39;=&#39;*80}\n&quot;) # Plot convergence fig, ax = plt.subplots(figsize=(10, 6)) ax.plot(cost_history, linewidth=2) ax.set_xlabel(&#39;Iteration&#39;, fontsize=12) ax.set_ylabel(&#39;Best Cost&#39;, fontsize=12) ax.set_title(&#39;PSO Convergence History - Classical SMC&#39;, fontsize=14, fontweight=&#39;bold&#39;) ax.set_yscale(&#39;log&#39;) ax.grid(True, alpha=0.3) plt.tight_layout() plt.savefig(&#39;pso_convergence_basic.png&#39;, dpi=300) print(&quot;Convergence plot saved: pso_convergence_basic.png&quot;) # Save optimized gains np.save(&#39;optimized_gains_classical_smc.npy&#39;, best_gains) print(&quot;Optimized gains saved: optimized_gains_classical_smc.npy&quot;) if __name__ == &quot;__main__&quot;: main()

``` **Expected Output:** ```
Loading configuration...
Creating controller factory...
Initializing PSO tuner...
Running PSO optimization with 30 particles for 100 iterations...
================================================================================
OPTIMIZATION RESULTS
================================================================================
Best Cost: 0.123456
Best Gains: [12.34 8.91 15.67 10.23 45.78 3.21]
Convergence: 87 iterations
================================================================================ Convergence plot saved: pso_convergence_basic.png
Optimized gains saved: optimized_gains_classical_smc.npy
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="real-time-convergence-monitoring-objective-monitor-pso-optimization-with-detailed-convergence-analysis-python">
<h2>8.2 Real-Time Convergence Monitoring <strong>Objective:</strong> Monitor PSO optimization with detailed convergence analysis. ```python<a class="headerlink" href="#real-time-convergence-monitoring-objective-monitor-pso-optimization-with-detailed-convergence-analysis-python" title="Link to this heading">Â¶</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="example-metadata">
<h1>example-metadata:<a class="headerlink" href="#example-metadata" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="runnable-false-usr-bin-env-python3">
<h1>runnable: false #!/usr/bin/env python3<a class="headerlink" href="#runnable-false-usr-bin-env-python3" title="Link to this heading">Â¶</a></h1>
<p>â€œâ€â€
Example 2: Real-Time Convergence Monitoring Demonstrates:</p>
<ul class="simple">
<li><p>EnhancedConvergenceAnalyzer integration</p></li>
<li><p>Multi-criteria convergence detection</p></li>
<li><p>Real-time metric logging</p></li>
<li><p>Early stopping based on convergence status
â€œâ€â€ import matplotlib.pyplot as plt
import numpy as np
from functools import partial from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.optimization.validation.enhanced_convergence_analyzer import ( EnhancedConvergenceAnalyzer, ConvergenceCriteria, ConvergenceStatus
)
from src.controllers.factory import create_controller, SMCType
from src.config import load_config # ============================================================================</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="configuration">
<h1>Configuration<a class="headerlink" href="#configuration" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="config-path-config-yaml">
<h1>============================================================================ CONFIG_PATH = â€œconfig.yamlâ€<a class="headerlink" href="#config-path-config-yaml" title="Link to this heading">Â¶</a></h1>
<p>CONTROLLER_TYPE = â€˜sta_smcâ€™
SEED = 42 # ============================================================================</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convergence-monitoring-callback">
<h1>Convergence Monitoring Callback<a class="headerlink" href="#convergence-monitoring-callback" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="class-convergencemonitor-callback-for-real-time-convergence-monitoring-def-init-self-analyzer-enhancedconvergenceanalyzer-self-analyzer-analyzer-self-metrics-history-def-call-self-iteration-int-best-fitness-float-mean-fitness-float-fitness-std-float-swarm-positions-np-ndarray-check-convergence-at-each-iteration-status-metrics-self-analyzer-check-convergence-iteration-iteration-best-fitness-best-fitness-mean-fitness-mean-fitness-fitness-std-fitness-std-swarm-positions-swarm-positions-self-metrics-history-append-metrics-log-key-metrics-if-iteration-10-0-print-f-iter-iteration-3d-status-status-value-20s-f-best-metrics-best-fitness-6f-f-diversity-metrics-population-diversity-4f-f-conv-velocity-metrics-convergence-velocity-4e-f-predicted-remaining-metrics-predicted-iterations-remaining-3d-early-stopping-if-status-convergencestatus-converged-print-f-n-convergence-detected-at-iteration-iteration-return-true-signal-early-stop-elif-status-convergencestatus-stagnated-print-f-n-stagnation-detected-at-iteration-iteration-return-true-signal-early-stop-return-false-continue">
<h1>============================================================================ class ConvergenceMonitor: â€œâ€â€Callback for real-time convergence monitoring.â€â€â€ def <strong>init</strong>(self, analyzer: EnhancedConvergenceAnalyzer): self.analyzer = analyzer self.metrics_history = [] def <strong>call</strong>(self, iteration: int, best_fitness: float, mean_fitness: float, fitness_std: float, swarm_positions: np.ndarray): â€œâ€â€Check convergence at each iteration.â€â€â€ status, metrics = self.analyzer.check_convergence( iteration=iteration, best_fitness=best_fitness, mean_fitness=mean_fitness, fitness_std=fitness_std, swarm_positions=swarm_positions ) self.metrics_history.append(metrics) # Log key metrics if iteration % 10 == 0: print(fâ€Iter {iteration:3d} | Status: {status.value:20s} | â€œ fâ€Best: {metrics.best_fitness:.6f} | â€œ fâ€Diversity: {metrics.population_diversity:.4f} | â€œ fâ€Conv. Velocity: {metrics.convergence_velocity:.4e} | â€œ fâ€Predicted Remaining: {metrics.predicted_iterations_remaining:3d}â€) # Early stopping if status == ConvergenceStatus.CONVERGED: print(fâ€\n&gt;&gt;&gt; CONVERGENCE DETECTED at iteration {iteration} &lt;&lt;&lt;â€) return True # Signal early stop elif status == ConvergenceStatus.STAGNATED: print(fâ€\n&gt;&gt;&gt; STAGNATION DETECTED at iteration {iteration} &lt;&lt;&lt;â€) return True # Signal early stop return False # Continue # ============================================================================<a class="headerlink" href="#class-convergencemonitor-callback-for-real-time-convergence-monitoring-def-init-self-analyzer-enhancedconvergenceanalyzer-self-analyzer-analyzer-self-metrics-history-def-call-self-iteration-int-best-fitness-float-mean-fitness-float-fitness-std-float-swarm-positions-np-ndarray-check-convergence-at-each-iteration-status-metrics-self-analyzer-check-convergence-iteration-iteration-best-fitness-best-fitness-mean-fitness-mean-fitness-fitness-std-fitness-std-swarm-positions-swarm-positions-self-metrics-history-append-metrics-log-key-metrics-if-iteration-10-0-print-f-iter-iteration-3d-status-status-value-20s-f-best-metrics-best-fitness-6f-f-diversity-metrics-population-diversity-4f-f-conv-velocity-metrics-convergence-velocity-4e-f-predicted-remaining-metrics-predicted-iterations-remaining-3d-early-stopping-if-status-convergencestatus-converged-print-f-n-convergence-detected-at-iteration-iteration-return-true-signal-early-stop-elif-status-convergencestatus-stagnated-print-f-n-stagnation-detected-at-iteration-iteration-return-true-signal-early-stop-return-false-continue" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="main">
<h1>Main<a class="headerlink" href="#main" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="def-main-load-configuration-config-load-config-config-path-initialize-convergence-analyzer-with-custom-criteria-criteria-convergencecriteria-fitness-tolerance-1e-6-relative-improvement-threshold-1e-4-min-diversity-threshold-1e-3-max-stagnation-iterations-50-enable-performance-prediction-true-premature-convergence-detection-true-analyzer-enhancedconvergenceanalyzer-criteria-criteria-controller-type-smctype-sta-monitor-convergencemonitor-analyzer-create-controller-factory-controller-factory-partial-create-controller-controller-type-controller-type-config-config-initialize-pso-tuner-tuner-psotuner-controller-factory-controller-factory-config-config-seed-seed-run-optimization-with-monitoring-print-f-running-pso-optimization-with-real-time-convergence-monitoring-print-f-120-result-tuner-optimise-print-f-120-n-plot-convergence-metrics-metrics-monitor-metrics-history-iterations-m-iteration-for-m-in-metrics-best-fitness-m-best-fitness-for-m-in-metrics-diversity-m-population-diversity-for-m-in-metrics-conv-velocity-m-convergence-velocity-for-m-in-metrics-fig-axes-plt-subplots-3-1-figsize-12-10-best-fitness-axes-0-plot-iterations-best-fitness-linewidth-2-color-blue-axes-0-set-ylabel-best-fitness-fontsize-12-axes-0-set-yscale-log-axes-0-set-title-convergence-monitoring-sta-smc-fontsize-14-fontweight-bold-axes-0-grid-true-alpha-0-3-population-diversity-axes-1-plot-iterations-diversity-linewidth-2-color-green-axes-1-set-ylabel-population-diversity-fontsize-12-axes-1-grid-true-alpha-0-3-convergence-velocity-axes-2-plot-iterations-conv-velocity-linewidth-2-color-red-axes-2-set-ylabel-convergence-velocity-fontsize-12-axes-2-set-xlabel-iteration-fontsize-12-axes-2-grid-true-alpha-0-3-plt-tight-layout-plt-savefig-pso-convergence-monitoring-png-dpi-300-print-convergence-monitoring-plot-saved-pso-convergence-monitoring-png-if-name-main-main">
<h1>============================================================================ def main(): # Load configuration config = load_config(CONFIG_PATH) # Initialize convergence analyzer with custom criteria criteria = ConvergenceCriteria( fitness_tolerance=1e-6, relative_improvement_threshold=1e-4, min_diversity_threshold=1e-3, max_stagnation_iterations=50, enable_performance_prediction=True, premature_convergence_detection=True ) analyzer = EnhancedConvergenceAnalyzer( criteria=criteria, controller_type=SMCType.STA ) monitor = ConvergenceMonitor(analyzer) # Create controller factory controller_factory = partial( create_controller, controller_type=CONTROLLER_TYPE, config=config ) # Initialize PSO tuner tuner = PSOTuner( controller_factory=controller_factory, config=config, seed=SEED ) # Run optimization with monitoring print(fâ€Running PSO optimization with real-time convergence monitoringâ€¦â€) print(fâ€{â€˜=â€™*120}â€) result = tuner.optimise() print(fâ€{â€˜=â€™*120}\nâ€) # Plot convergence metrics metrics = monitor.metrics_history iterations = [m.iteration for m in metrics] best_fitness = [m.best_fitness for m in metrics] diversity = [m.population_diversity for m in metrics] conv_velocity = [m.convergence_velocity for m in metrics] fig, axes = plt.subplots(3, 1, figsize=(12, 10)) # Best fitness axes[0].plot(iterations, best_fitness, linewidth=2, color=â€™blueâ€™) axes[0].set_ylabel(â€˜Best Fitnessâ€™, fontsize=12) axes[0].set_yscale(â€˜logâ€™) axes[0].set_title(â€˜Convergence Monitoring - STA SMCâ€™, fontsize=14, fontweight=â€™boldâ€™) axes[0].grid(True, alpha=0.3) # Population diversity axes[1].plot(iterations, diversity, linewidth=2, color=â€™greenâ€™) axes[1].set_ylabel(â€˜Population Diversityâ€™, fontsize=12) axes[1].grid(True, alpha=0.3) # Convergence velocity axes[2].plot(iterations, conv_velocity, linewidth=2, color=â€™redâ€™) axes[2].set_ylabel(â€˜Convergence Velocityâ€™, fontsize=12) axes[2].set_xlabel(â€˜Iterationâ€™, fontsize=12) axes[2].grid(True, alpha=0.3) plt.tight_layout() plt.savefig(â€˜pso_convergence_monitoring.pngâ€™, dpi=300) print(â€œConvergence monitoring plot saved: pso_convergence_monitoring.pngâ€) if <strong>name</strong> == â€œ<strong>main</strong>â€: main()<a class="headerlink" href="#def-main-load-configuration-config-load-config-config-path-initialize-convergence-analyzer-with-custom-criteria-criteria-convergencecriteria-fitness-tolerance-1e-6-relative-improvement-threshold-1e-4-min-diversity-threshold-1e-3-max-stagnation-iterations-50-enable-performance-prediction-true-premature-convergence-detection-true-analyzer-enhancedconvergenceanalyzer-criteria-criteria-controller-type-smctype-sta-monitor-convergencemonitor-analyzer-create-controller-factory-controller-factory-partial-create-controller-controller-type-controller-type-config-config-initialize-pso-tuner-tuner-psotuner-controller-factory-controller-factory-config-config-seed-seed-run-optimization-with-monitoring-print-f-running-pso-optimization-with-real-time-convergence-monitoring-print-f-120-result-tuner-optimise-print-f-120-n-plot-convergence-metrics-metrics-monitor-metrics-history-iterations-m-iteration-for-m-in-metrics-best-fitness-m-best-fitness-for-m-in-metrics-diversity-m-population-diversity-for-m-in-metrics-conv-velocity-m-convergence-velocity-for-m-in-metrics-fig-axes-plt-subplots-3-1-figsize-12-10-best-fitness-axes-0-plot-iterations-best-fitness-linewidth-2-color-blue-axes-0-set-ylabel-best-fitness-fontsize-12-axes-0-set-yscale-log-axes-0-set-title-convergence-monitoring-sta-smc-fontsize-14-fontweight-bold-axes-0-grid-true-alpha-0-3-population-diversity-axes-1-plot-iterations-diversity-linewidth-2-color-green-axes-1-set-ylabel-population-diversity-fontsize-12-axes-1-grid-true-alpha-0-3-convergence-velocity-axes-2-plot-iterations-conv-velocity-linewidth-2-color-red-axes-2-set-ylabel-convergence-velocity-fontsize-12-axes-2-set-xlabel-iteration-fontsize-12-axes-2-grid-true-alpha-0-3-plt-tight-layout-plt-savefig-pso-convergence-monitoring-png-dpi-300-print-convergence-monitoring-plot-saved-pso-convergence-monitoring-png-if-name-main-main" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="expected-output-running-pso-optimization-with-real-time-convergence-monitoring">
<h1><code class="docutils literal notranslate"><span class="pre">**Expected</span> <span class="pre">Output:**</span></code>
Running PSO optimization with real-time convergence monitoringâ€¦<a class="headerlink" href="#expected-output-running-pso-optimization-with-real-time-convergence-monitoring" title="Link to this heading">Â¶</a></h1>
<p>Iter 0 | Status: INITIALIZING | Best: 1.234567 | Diversity: 15.2341 | Conv. Velocity: 0.00e+00 | Predicted Remaining: 100
Iter 10 | Status: EXPLORING | Best: 0.567890 | Diversity: 12.4567 | Conv. Velocity: -6.67e-02 | Predicted Remaining: 85
Iter 20 | Status: CONVERGING | Best: 0.234567 | Diversity: 8.9012 | Conv. Velocity: -3.33e-02 | Predicted Remaining: 60
Iter 30 | Status: CONVERGING | Best: 0.123456 | Diversity: 5.2341 | Conv. Velocity: -1.11e-02 | Predicted Remaining: 40
Iter 40 | Status: CONVERGING | Best: 0.098765 | Diversity: 2.4567 | Conv. Velocity: -2.47e-03 | Predicted Remaining: 20 &gt;&gt;&gt; CONVERGENCE DETECTED at iteration 45 &lt;&lt;&lt;
======================================================================================================================== Convergence monitoring plot saved: pso_convergence_monitoring.png</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="o">---</span>

<span class="c1">## 8.3 Bounds Validation and Adjustment **Objective:** Validate PSO bounds and automatically adjust if necessary. ```python</span>

<span class="c1"># example-metadata:</span>

<span class="c1"># runnable: false #!/usr/bin/env python3</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Example 3: Bounds Validation and Automatic Adjustment Demonstrates:</span>
<span class="sd">- PSOBoundsValidator usage</span>
<span class="sd">- Controller-specific bounds validation</span>
<span class="sd">- Automatic adjustment algorithms</span>
<span class="sd">- Performance comparison with/without adjustment</span>
<span class="sd">&quot;&quot;&quot;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">src.optimization.validation.pso_bounds_validator</span><span class="w"> </span><span class="kn">import</span> <span class="n">PSOBoundsValidator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.optimization.algorithms.pso_optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">PSOTuner</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.controllers.factory</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_controller</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span> <span class="c1"># ============================================================================</span>
<span class="c1"># Configuration</span>

<span class="c1"># ============================================================================ CONFIG_PATH = &quot;config.yaml&quot;</span>

<span class="n">CONTROLLER_TYPE</span> <span class="o">=</span> <span class="s1">&#39;adaptive_smc&#39;</span> <span class="c1"># Test bounds (intentionally suboptimal)</span>
<span class="n">TEST_BOUNDS_LOWER</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">]</span> <span class="c1"># Too narrow</span>
<span class="n">TEST_BOUNDS_UPPER</span> <span class="o">=</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span> <span class="c1"># Too narrow # ============================================================================</span>
<span class="c1"># Main</span>

<span class="c1"># ============================================================================ def main(): # Load configuration config = load_config(CONFIG_PATH) # Initialize bounds validator validator = PSOBoundsValidator(config) # Validate test bounds print(&quot;Validating test bounds for Adaptive SMC...&quot;) print(f&quot;Lower: {TEST_BOUNDS_LOWER}&quot;) print(f&quot;Upper: {TEST_BOUNDS_UPPER}&quot;) print() result = validator.validate_bounds( controller_type=CONTROLLER_TYPE, lower_bounds=TEST_BOUNDS_LOWER, upper_bounds=TEST_BOUNDS_UPPER ) if result.is_valid: print(&quot;âœ“ Bounds are valid!&quot;) else: print(&quot;âœ— Bounds validation failed!&quot;) print(&quot;\nWarnings:&quot;) for warning in result.warnings: print(f&quot; - {warning}&quot;) print(&quot;\nRecommendations:&quot;) for rec in result.recommendations: print(f&quot; - {rec}&quot;) if result.adjusted_bounds: print(&quot;\nAutomatically adjusted bounds:&quot;) adjusted_lower = result.adjusted_bounds[&#39;lower&#39;] adjusted_upper = result.adjusted_bounds[&#39;upper&#39;] print(f&quot; Lower: {adjusted_lower}&quot;) print(f&quot; Upper: {adjusted_upper}&quot;) # Compare PSO performance with original vs. adjusted bounds print(&quot;\n&quot; + &quot;=&quot;*80) print(&quot;Performance Comparison: Original vs. Adjusted Bounds&quot;) print(&quot;=&quot;*80) controller_factory = partial( create_controller, controller_type=CONTROLLER_TYPE, config=config ) # PSO with original bounds print(&quot;\n[1/2] Running PSO with ORIGINAL bounds...&quot;) tuner_original = PSOTuner( controller_factory=controller_factory, config=config, seed=42 ) # Override bounds config.pso.bounds.min = TEST_BOUNDS_LOWER config.pso.bounds.max = TEST_BOUNDS_UPPER result_original = tuner_original.optimise(iters_override=50) # PSO with adjusted bounds print(&quot;[2/2] Running PSO with ADJUSTED bounds...&quot;) tuner_adjusted = PSOTuner( controller_factory=controller_factory, config=config, seed=42 ) # Override bounds with adjusted config.pso.bounds.min = adjusted_lower config.pso.bounds.max = adjusted_upper result_adjusted = tuner_adjusted.optimise(iters_override=50) # Compare results print(&quot;\n&quot; + &quot;=&quot;*80) print(&quot;Results Comparison&quot;) print(&quot;=&quot;*80) print(f&quot;{&#39;Metric&#39;:&lt;30s} | {&#39;Original Bounds&#39;:&gt;20s} | {&#39;Adjusted Bounds&#39;:&gt;20s} | {&#39;Improvement&#39;:&gt;15s}&quot;) print(&quot;-&quot;*80) cost_original = result_original[&#39;best_cost&#39;] cost_adjusted = result_adjusted[&#39;best_cost&#39;] improvement = (cost_original - cost_adjusted) / cost_original * 100 print(f&quot;{&#39;Best Cost&#39;:&lt;30s} | {cost_original:20.6f} | {cost_adjusted:20.6f} | {improvement:14.2f}%&quot;) print(f&quot;{&#39;Best Gains&#39;:&lt;30s}&quot;) print(f&quot; Original: {result_original[&#39;best_pos&#39;]}&quot;) print(f&quot; Adjusted: {result_adjusted[&#39;best_pos&#39;]}&quot;) print(&quot;=&quot;*80) if improvement &gt; 0: print(f&quot;\nâœ“ Adjusted bounds achieved {improvement:.2f}% cost reduction!&quot;) else: print(f&quot;\nâœ— Adjusted bounds did not improve performance.&quot;) if __name__ == &quot;__main__&quot;: main()</span>

</pre></div>
</div>
<hr class="docutils" />
<section id="hyperparameter-optimization-objective-meta-optimize-pso-hyperparameters-for-best-controller-performance-python">
<h2>8.4 Hyperparameter Optimization <strong>Objective:</strong> Meta-optimize PSO hyperparameters for best controller performance. ```python<a class="headerlink" href="#hyperparameter-optimization-objective-meta-optimize-pso-hyperparameters-for-best-controller-performance-python" title="Link to this heading">Â¶</a></h2>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id1">
<h1>example-metadata:<a class="headerlink" href="#id1" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id2">
<h1>runnable: false #!/usr/bin/env python3<a class="headerlink" href="#id2" title="Link to this heading">Â¶</a></h1>
<p>â€œâ€â€
Example 4: PSO Hyperparameter Optimization Demonstrates:</p>
<ul class="simple">
<li><p>PSOHyperparameterOptimizer usage</p></li>
<li><p>Meta-optimization with differential evolution</p></li>
<li><p>Multi-objective optimization</p></li>
<li><p>Baseline comparison
â€œâ€â€ from src.optimization.tuning.pso_hyperparameter_optimizer import ( PSOHyperparameterOptimizer, OptimizationObjective
)
from src.controllers.factory import SMCType
from src.config import load_config
import matplotlib.pyplot as plt
import numpy as np # ============================================================================</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id3">
<h1>Configuration<a class="headerlink" href="#id3" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id4">
<h1>============================================================================ CONFIG_PATH = â€œconfig.yamlâ€<a class="headerlink" href="#id4" title="Link to this heading">Â¶</a></h1>
<p>CONTROLLER_TYPE = SMCType.CLASSICAL
MAX_META_EVALUATIONS = 50
N_TRIALS_PER_EVAL = 3 # ============================================================================</p>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="id5">
<h1>Main<a class="headerlink" href="#id5" title="Link to this heading">Â¶</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="def-main-load-configuration-config-load-config-config-path-initialize-meta-optimizer-print-initializing-pso-hyperparameter-optimizer-meta-optimizer-psohyperparameteroptimizer-config-run-meta-optimization-print-f-nrunning-meta-optimization-for-controller-type-value-print-f-max-evaluations-max-meta-evaluations-print-f-trials-per-evaluation-n-trials-per-eval-print-f-objective-optimizationobjective-multi-objective-value-print-80-result-meta-optimizer-optimize-hyperparameters-controller-type-controller-type-objective-optimizationobjective-multi-objective-max-evaluations-max-meta-evaluations-n-trials-per-evaluation-n-trials-per-eval-display-results-print-n-80-print-hyperparameter-optimization-results-print-80-print-f-noptimized-hyperparameters-print-f-inertia-weight-w-result-hyperparameters-w-6f-print-f-cognitive-c1-result-hyperparameters-c1-6f-print-f-social-c2-result-hyperparameters-c2-6f-print-f-swarm-size-result-hyperparameters-n-particles-print-f-nbaseline-hyperparameters-print-f-inertia-weight-w-result-baseline-hyperparameters-w-6f-print-f-cognitive-c1-result-baseline-hyperparameters-c1-6f-print-f-social-c2-result-baseline-hyperparameters-c2-6f-print-f-swarm-size-result-baseline-hyperparameters-n-particles-print-f-nperformance-improvements-vs-baseline-print-f-convergence-speedup-result-convergence-improvement-2f-x-print-f-quality-improvement-result-quality-improvement100-2f-print-f-robustness-improvement-result-robustness-improvement100-2f-print-f-efficiency-score-result-efficiency-score-4f-print-80-visualize-comparison-fig-axes-plt-subplots-2-2-figsize-12-10-categories-w-c1-c2-n-baseline-values-result-baseline-hyperparameters-w-result-baseline-hyperparameters-c1-result-baseline-hyperparameters-c2-result-baseline-hyperparameters-n-particles-optimized-values-result-hyperparameters-w-result-hyperparameters-c1-result-hyperparameters-c2-result-hyperparameters-n-particles-x-np-arange-len-categories-width-0-35-axes-0-0-bar-x-width-2-baseline-values-width-label-baseline-alpha-0-7-axes-0-0-bar-x-width-2-optimized-values-width-label-optimized-alpha-0-7-axes-0-0-set-ylabel-value-axes-0-0-set-title-hyperparameter-comparison-axes-0-0-set-xticks-x-axes-0-0-set-xticklabels-categories-axes-0-0-legend-axes-0-0-grid-true-alpha-0-3-performance-metrics-metrics-convergence-nspeed-solution-nquality-robustness-improvements-result-convergence-improvement-1-result-quality-improvement-1-result-robustness-improvement-axes-0-1-bar-metrics-improvements-color-blue-green-orange-alpha-0-7-axes-0-1-axhline-y-1-0-color-red-linestyle-label-baseline-axes-0-1-set-ylabel-improvement-factor-axes-0-1-set-title-performance-improvements-axes-0-1-legend-axes-0-1-grid-true-alpha-0-3-convergence-history-if-available-if-hasattr-result-optimization-history-axes-1-0-plot-result-optimization-history-best-objective-linewidth-2-axes-1-0-set-xlabel-meta-optimization-iteration-axes-1-0-set-ylabel-objective-value-axes-1-0-set-title-meta-optimization-convergence-axes-1-0-grid-true-alpha-0-3-hide-unused-subplot-axes-1-1-axis-off-plt-tight-layout-plt-savefig-pso-hyperparameter-optimization-png-dpi-300-print-nvisualization-saved-pso-hyperparameter-optimization-png-if-name-main-main">
<h1>============================================================================ def main(): # Load configuration config = load_config(CONFIG_PATH) # Initialize meta-optimizer print(â€œInitializing PSO Hyperparameter Optimizerâ€¦â€) meta_optimizer = PSOHyperparameterOptimizer(config) # Run meta-optimization print(fâ€\nRunning meta-optimization for {CONTROLLER_TYPE.value}â€¦â€) print(fâ€Max evaluations: {MAX_META_EVALUATIONS}â€) print(fâ€Trials per evaluation: {N_TRIALS_PER_EVAL}â€) print(fâ€Objective: {OptimizationObjective.MULTI_OBJECTIVE.value}â€) print(â€œ=â€*80) result = meta_optimizer.optimize_hyperparameters( controller_type=CONTROLLER_TYPE, objective=OptimizationObjective.MULTI_OBJECTIVE, max_evaluations=MAX_META_EVALUATIONS, n_trials_per_evaluation=N_TRIALS_PER_EVAL ) # Display results print(â€œ\nâ€ + â€œ=â€<em>80) print(â€œHYPERPARAMETER OPTIMIZATION RESULTSâ€) print(â€œ=â€<em>80) print(fâ€\nOptimized Hyperparameters:â€) print(fâ€ Inertia weight (w): {result.hyperparameters.w:.6f}â€) print(fâ€ Cognitive (c1): {result.hyperparameters.c1:.6f}â€) print(fâ€ Social (c2): {result.hyperparameters.c2:.6f}â€) print(fâ€ Swarm size: {result.hyperparameters.n_particles}â€) print(fâ€\nBaseline Hyperparameters:â€) print(fâ€ Inertia weight (w): {result.baseline_hyperparameters.w:.6f}â€) print(fâ€ Cognitive (c1): {result.baseline_hyperparameters.c1:.6f}â€) print(fâ€ Social (c2): {result.baseline_hyperparameters.c2:.6f}â€) print(fâ€ Swarm size: {result.baseline_hyperparameters.n_particles}â€) print(fâ€\nPerformance Improvements vs. Baseline:â€) print(fâ€ Convergence speedup: {result.convergence_improvement:.2f}xâ€) print(fâ€ Quality improvement: {result.quality_improvement</em>100:.2f}%â€) print(fâ€ Robustness improvement: {result.robustness_improvement</em>100:.2f}%â€) print(fâ€ Efficiency score: {result.efficiency_score:.4f}â€) print(â€œ=â€*80) # Visualize comparison fig, axes = plt.subplots(2, 2, figsize=(12, 10)) categories = [â€˜wâ€™, â€˜c1â€™, â€˜c2â€™, â€˜Nâ€™] baseline_values = [ result.baseline_hyperparameters.w, result.baseline_hyperparameters.c1, result.baseline_hyperparameters.c2, result.baseline_hyperparameters.n_particles ] optimized_values = [ result.hyperparameters.w, result.hyperparameters.c1, result.hyperparameters.c2, result.hyperparameters.n_particles ] x = np.arange(len(categories)) width = 0.35 axes[0, 0].bar(x - width/2, baseline_values, width, label=â€™Baselineâ€™, alpha=0.7) axes[0, 0].bar(x + width/2, optimized_values, width, label=â€™Optimizedâ€™, alpha=0.7) axes[0, 0].set_ylabel(â€˜Valueâ€™) axes[0, 0].set_title(â€˜Hyperparameter Comparisonâ€™) axes[0, 0].set_xticks(x) axes[0, 0].set_xticklabels(categories) axes[0, 0].legend() axes[0, 0].grid(True, alpha=0.3) # Performance metrics metrics = [â€˜Convergence\nSpeedâ€™, â€˜Solution\nQualityâ€™, â€˜Robustnessâ€™] improvements = [ result.convergence_improvement, 1 + result.quality_improvement, 1 + result.robustness_improvement ] axes[0, 1].bar(metrics, improvements, color=[â€˜blueâ€™, â€˜greenâ€™, â€˜orangeâ€™], alpha=0.7) axes[0, 1].axhline(y=1.0, color=â€™redâ€™, linestyle=â€™â€“â€™, label=â€™Baselineâ€™) axes[0, 1].set_ylabel(â€˜Improvement Factorâ€™) axes[0, 1].set_title(â€˜Performance Improvementsâ€™) axes[0, 1].legend() axes[0, 1].grid(True, alpha=0.3) # Convergence history (if available) if hasattr(result, â€˜optimization_historyâ€™): axes[1, 0].plot(result.optimization_history[â€˜best_objectiveâ€™], linewidth=2) axes[1, 0].set_xlabel(â€˜Meta-Optimization Iterationâ€™) axes[1, 0].set_ylabel(â€˜Objective Valueâ€™) axes[1, 0].set_title(â€˜Meta-Optimization Convergenceâ€™) axes[1, 0].grid(True, alpha=0.3) # Hide unused subplot axes[1, 1].axis(â€˜offâ€™) plt.tight_layout() plt.savefig(â€˜pso_hyperparameter_optimization.pngâ€™, dpi=300) print(â€œ\nVisualization saved: pso_hyperparameter_optimization.pngâ€) if <strong>name</strong> == â€œ<strong>main</strong>â€: main()<a class="headerlink" href="#def-main-load-configuration-config-load-config-config-path-initialize-meta-optimizer-print-initializing-pso-hyperparameter-optimizer-meta-optimizer-psohyperparameteroptimizer-config-run-meta-optimization-print-f-nrunning-meta-optimization-for-controller-type-value-print-f-max-evaluations-max-meta-evaluations-print-f-trials-per-evaluation-n-trials-per-eval-print-f-objective-optimizationobjective-multi-objective-value-print-80-result-meta-optimizer-optimize-hyperparameters-controller-type-controller-type-objective-optimizationobjective-multi-objective-max-evaluations-max-meta-evaluations-n-trials-per-evaluation-n-trials-per-eval-display-results-print-n-80-print-hyperparameter-optimization-results-print-80-print-f-noptimized-hyperparameters-print-f-inertia-weight-w-result-hyperparameters-w-6f-print-f-cognitive-c1-result-hyperparameters-c1-6f-print-f-social-c2-result-hyperparameters-c2-6f-print-f-swarm-size-result-hyperparameters-n-particles-print-f-nbaseline-hyperparameters-print-f-inertia-weight-w-result-baseline-hyperparameters-w-6f-print-f-cognitive-c1-result-baseline-hyperparameters-c1-6f-print-f-social-c2-result-baseline-hyperparameters-c2-6f-print-f-swarm-size-result-baseline-hyperparameters-n-particles-print-f-nperformance-improvements-vs-baseline-print-f-convergence-speedup-result-convergence-improvement-2f-x-print-f-quality-improvement-result-quality-improvement100-2f-print-f-robustness-improvement-result-robustness-improvement100-2f-print-f-efficiency-score-result-efficiency-score-4f-print-80-visualize-comparison-fig-axes-plt-subplots-2-2-figsize-12-10-categories-w-c1-c2-n-baseline-values-result-baseline-hyperparameters-w-result-baseline-hyperparameters-c1-result-baseline-hyperparameters-c2-result-baseline-hyperparameters-n-particles-optimized-values-result-hyperparameters-w-result-hyperparameters-c1-result-hyperparameters-c2-result-hyperparameters-n-particles-x-np-arange-len-categories-width-0-35-axes-0-0-bar-x-width-2-baseline-values-width-label-baseline-alpha-0-7-axes-0-0-bar-x-width-2-optimized-values-width-label-optimized-alpha-0-7-axes-0-0-set-ylabel-value-axes-0-0-set-title-hyperparameter-comparison-axes-0-0-set-xticks-x-axes-0-0-set-xticklabels-categories-axes-0-0-legend-axes-0-0-grid-true-alpha-0-3-performance-metrics-metrics-convergence-nspeed-solution-nquality-robustness-improvements-result-convergence-improvement-1-result-quality-improvement-1-result-robustness-improvement-axes-0-1-bar-metrics-improvements-color-blue-green-orange-alpha-0-7-axes-0-1-axhline-y-1-0-color-red-linestyle-label-baseline-axes-0-1-set-ylabel-improvement-factor-axes-0-1-set-title-performance-improvements-axes-0-1-legend-axes-0-1-grid-true-alpha-0-3-convergence-history-if-available-if-hasattr-result-optimization-history-axes-1-0-plot-result-optimization-history-best-objective-linewidth-2-axes-1-0-set-xlabel-meta-optimization-iteration-axes-1-0-set-ylabel-objective-value-axes-1-0-set-title-meta-optimization-convergence-axes-1-0-grid-true-alpha-0-3-hide-unused-subplot-axes-1-1-axis-off-plt-tight-layout-plt-savefig-pso-hyperparameter-optimization-png-dpi-300-print-nvisualization-saved-pso-hyperparameter-optimization-png-if-name-main-main" title="Link to this heading">Â¶</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="o">---</span>

<span class="c1">## 8.5 Complete Optimization Pipeline **Objective:** End-to-end workflow from configuration to deployed controller. ```python</span>

<span class="c1"># example-metadata:</span>

<span class="c1"># runnable: false #!/usr/bin/env python3</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Example 5: Complete Optimization Pipeline Demonstrates:</span>
<span class="sd">- Full workflow: Config â†’ Factory â†’ PSO â†’ Validation â†’ Deployment</span>
<span class="sd">- Bounds validation and adjustment</span>
<span class="sd">- Convergence monitoring</span>
<span class="sd">- Performance benchmarking</span>
<span class="sd">- Controller deployment</span>
<span class="sd">&quot;&quot;&quot;</span> <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span> <span class="kn">from</span><span class="w"> </span><span class="nn">src.optimization.algorithms.pso_optimizer</span><span class="w"> </span><span class="kn">import</span> <span class="n">PSOTuner</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.optimization.validation.pso_bounds_validator</span><span class="w"> </span><span class="kn">import</span> <span class="n">PSOBoundsValidator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.optimization.validation.enhanced_convergence_analyzer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span> <span class="n">EnhancedConvergenceAnalyzer</span><span class="p">,</span> <span class="n">ConvergenceCriteria</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.controllers.factory</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_controller</span><span class="p">,</span> <span class="n">SMCType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.simulation.engines.simulation_runner</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimulationRunner</span> <span class="c1"># ============================================================================</span>
<span class="c1"># Configuration</span>

<span class="c1"># ============================================================================ CONFIG_PATH = &quot;config.yaml&quot;</span>

<span class="n">CONTROLLER_TYPE</span> <span class="o">=</span> <span class="s1">&#39;classical_smc&#39;</span>
<span class="n">OUTPUT_DIR</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;optimization_results&quot;</span><span class="p">)</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">42</span> <span class="c1"># ============================================================================</span>
<span class="c1"># Pipeline</span>

<span class="c1"># ============================================================================ def main(): # Create output directory OUTPUT_DIR.mkdir(exist_ok=True) print(&quot;=&quot;*80) print(&quot;COMPLETE PSO OPTIMIZATION PIPELINE&quot;) print(&quot;=&quot;*80) # ------------------------------------------------------------------------- # Step 1: Load Configuration # ------------------------------------------------------------------------- print(&quot;\n[1/7] Loading configuration...&quot;) config = load_config(CONFIG_PATH) print(f&quot; âœ“ Configuration loaded from {CONFIG_PATH}&quot;) # ------------------------------------------------------------------------- # Step 2: Validate and Adjust Bounds # ------------------------------------------------------------------------- print(&quot;\n[2/7] Validating PSO bounds...&quot;) validator = PSOBoundsValidator(config) bounds_result = validator.validate_bounds( controller_type=CONTROLLER_TYPE, lower_bounds=list(config.pso.bounds.min), upper_bounds=list(config.pso.bounds.max) ) if bounds_result.is_valid: print(&quot; âœ“ Bounds are valid&quot;) else: print(&quot; âœ— Bounds validation failed, using adjusted bounds&quot;) config.pso.bounds.min = bounds_result.adjusted_bounds[&#39;lower&#39;] config.pso.bounds.max = bounds_result.adjusted_bounds[&#39;upper&#39;] # ------------------------------------------------------------------------- # Step 3: Initialize Convergence Analyzer # ------------------------------------------------------------------------- print(&quot;\n[3/7] Initializing convergence analyzer...&quot;) criteria = ConvergenceCriteria( fitness_tolerance=1e-6, max_stagnation_iterations=50 ) analyzer = EnhancedConvergenceAnalyzer( criteria=criteria, controller_type=SMCType.CLASSICAL ) print(&quot; âœ“ Convergence analyzer ready&quot;) # ------------------------------------------------------------------------- # Step 4: Create Controller Factory # ------------------------------------------------------------------------- print(&quot;\n[4/7] Creating controller factory...&quot;) controller_factory = partial( create_controller, controller_type=CONTROLLER_TYPE, config=config ) print(&quot; âœ“ Factory created&quot;) # ------------------------------------------------------------------------- # Step 5: Run PSO Optimization # ------------------------------------------------------------------------- print(&quot;\n[5/7] Running PSO optimization...&quot;) tuner = PSOTuner( controller_factory=controller_factory, config=config, seed=SEED, instability_penalty_factor=100.0 ) result = tuner.optimise() best_gains = result[&#39;best_pos&#39;] best_cost = result[&#39;best_cost&#39;] cost_history = result[&#39;cost_history&#39;] print(f&quot; âœ“ Optimization complete&quot;) print(f&quot; Best cost: {best_cost:.6f}&quot;) print(f&quot; Convergence: {len(cost_history)} iterations&quot;) # Save results np.save(OUTPUT_DIR / &quot;optimized_gains.npy&quot;, best_gains) np.save(OUTPUT_DIR / &quot;cost_history.npy&quot;, cost_history) # ------------------------------------------------------------------------- # Step 6: Validate Optimized Controller # ------------------------------------------------------------------------- print(&quot;\n[6/7] Validating optimized controller...&quot;) # Create controller with optimized gains optimized_controller = create_controller( controller_type=CONTROLLER_TYPE, config=config, gains=best_gains ) # Run validation simulations n_validation_trials = 10 validation_costs = [] for trial in range(n_validation_trials): sim_runner = SimulationRunner( controller=optimized_controller, config=config, seed=SEED + trial ) result_trial = sim_runner.run() # Compute cost ise = np.sum(result_trial.states ** 2) * config.simulation.dt validation_costs.append(ise) mean_cost = np.mean(validation_costs) std_cost = np.std(validation_costs) print(f&quot; âœ“ Validation complete ({n_validation_trials} trials)&quot;) print(f&quot; Mean cost: {mean_cost:.6f} Â± {std_cost:.6f}&quot;) # ------------------------------------------------------------------------- # Step 7: Generate Report and Visualizations # ------------------------------------------------------------------------- print(&quot;\n[7/7] Generating reports and visualizations...&quot;) # Convergence plot fig, axes = plt.subplots(2, 1, figsize=(10, 8)) axes[0].plot(cost_history, linewidth=2, color=&#39;blue&#39;) axes[0].set_ylabel(&#39;Best Cost&#39;, fontsize=12) axes[0].set_title(&#39;PSO Convergence History&#39;, fontsize=14, fontweight=&#39;bold&#39;) axes[0].set_yscale(&#39;log&#39;) axes[0].grid(True, alpha=0.3) axes[1].bar(range(n_validation_trials), validation_costs, alpha=0.7, color=&#39;green&#39;) axes[1].axhline(y=mean_cost, color=&#39;red&#39;, linestyle=&#39;--&#39;, label=f&#39;Mean: {mean_cost:.4f}&#39;) axes[1].set_xlabel(&#39;Validation Trial&#39;, fontsize=12) axes[1].set_ylabel(&#39;Cost (ISE)&#39;, fontsize=12) axes[1].set_title(&#39;Validation Performance&#39;, fontsize=14, fontweight=&#39;bold&#39;) axes[1].legend() axes[1].grid(True, alpha=0.3) plt.tight_layout() plt.savefig(OUTPUT_DIR / &quot;optimization_pipeline_summary.png&quot;, dpi=300) # Summary report report_path = OUTPUT_DIR / &quot;optimization_report.txt&quot; with open(report_path, &#39;w&#39;) as f: f.write(&quot;=&quot;*80 + &quot;\n&quot;) f.write(&quot;PSO OPTIMIZATION PIPELINE - SUMMARY REPORT\n&quot;) f.write(&quot;=&quot;*80 + &quot;\n\n&quot;) f.write(f&quot;Controller Type: {CONTROLLER_TYPE}\n&quot;) f.write(f&quot;Configuration: {CONFIG_PATH}\n&quot;) f.write(f&quot;Random Seed: {SEED}\n\n&quot;) f.write(&quot;-&quot;*80 + &quot;\n&quot;) f.write(&quot;OPTIMIZATION RESULTS\n&quot;) f.write(&quot;-&quot;*80 + &quot;\n&quot;) f.write(f&quot;Best Cost: {best_cost:.6f}\n&quot;) f.write(f&quot;Convergence Iterations: {len(cost_history)}\n&quot;) f.write(f&quot;Optimized Gains: {best_gains}\n\n&quot;) f.write(&quot;-&quot;*80 + &quot;\n&quot;) f.write(&quot;VALIDATION RESULTS\n&quot;) f.write(&quot;-&quot;*80 + &quot;\n&quot;) f.write(f&quot;Number of Trials: {n_validation_trials}\n&quot;) f.write(f&quot;Mean Cost: {mean_cost:.6f}\n&quot;) f.write(f&quot;Std. Deviation: {std_cost:.6f}\n&quot;) f.write(f&quot;Min Cost: {np.min(validation_costs):.6f}\n&quot;) f.write(f&quot;Max Cost: {np.max(validation_costs):.6f}\n&quot;) f.write(&quot;=&quot;*80 + &quot;\n&quot;) print(f&quot; âœ“ Summary report: {report_path}&quot;) print(f&quot; âœ“ Visualization: {OUTPUT_DIR / &#39;optimization_pipeline_summary.png&#39;}&quot;) print(&quot;\n&quot; + &quot;=&quot;*80) print(&quot;PIPELINE COMPLETE&quot;) print(&quot;=&quot;*80) print(f&quot;\nOptimized controller ready for deployment!&quot;) print(f&quot;Gains: {best_gains}&quot;) if __name__ == &quot;__main__&quot;: main()</span>

</pre></div>
</div>
<hr class="docutils" />
<section id="performance-tuning-guidelines-9-1-pso-parameter-selection-swarm-size-recommendations-controller-complexity-dimensions-recommended-swarm-size-rationale">
<h2>9. Performance &amp; Tuning Guidelines ### 9.1 PSO Parameter Selection <strong>Swarm Size Recommendations:</strong> | Controller Complexity | Dimensions | Recommended Swarm Size | Rationale |<a class="headerlink" href="#performance-tuning-guidelines-9-1-pso-parameter-selection-swarm-size-recommendations-controller-complexity-dimensions-recommended-swarm-size-rationale" title="Link to this heading">Â¶</a></h2>
<p>|â€”â€”â€”â€”â€”â€”â€”â€“|â€”â€”â€”â€”|â€”â€”â€”â€”â€”â€”â€”â€”|â€”â€”â€”â€“|
| Simple (4 gains) | 4 | 15-20 | <span class="math notranslate nohighlight">\(N \approx 10D/2\)</span> |
| Medium (5-6 gains) | 5-6 | 25-35 | <span class="math notranslate nohighlight">\(N \approx 10D/2\)</span> |
| Complex (â‰¥7 gains) | â‰¥7 | 40-50 | <span class="math notranslate nohighlight">\(N \approx 10D/2\)</span>, max practical limit | <strong>Inertia Weight Tuning:</strong> - <strong>High <span class="math notranslate nohighlight">\(w\)</span> (0.9)</strong>: Promotes exploration, prevents premature convergence - Use early in optimization - Good for rough landscapes - <strong>Low <span class="math notranslate nohighlight">\(w\)</span> (0.4)</strong>: Promotes exploitation, refines approaches - Use late in optimization - Good for smooth landscapes - <strong>Linear Schedule</strong>: <span class="math notranslate nohighlight">\(w(t) = w_{max} - (w_{max} - w_{min}) \cdot t/t_{max}\)</span> - Default: <span class="math notranslate nohighlight">\(w_{max} = 0.9\)</span>, <span class="math notranslate nohighlight">\(w_{min} = 0.4\)</span> - Balances exploration and exploitation automatically <strong>Cognitive and Social Coefficients:</strong> | Configuration | <span class="math notranslate nohighlight">\(c_1\)</span> | <span class="math notranslate nohighlight">\(c_2\)</span> | Behavior |
|â€”â€”â€”â€”â€”|â€”â€”-|â€”â€”-|â€”â€”â€”-|
| Balanced | 1.494 | 1.494 | Standard PSO (recommended) |
| Individualistic | 2.0 | 1.0 | Emphasizes personal best (more exploration) |
| Social | 1.0 | 2.0 | Emphasizes global best (faster convergence, risk of local minima) |
| Conservative | 1.0 | 1.0 | Cautious updates (slow but stable) | <strong>Velocity Clamping:</strong> Prevents particles from overshooting: $<span class="math notranslate nohighlight">\(
v_{i,d}^{t+1} = \text{clamp}(v_{i,d}^{t+1}, v_{min}, v_{max})
\)</span>$ where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(v_{min} = \delta_{min} \cdot (b_{max} - b_{min})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(v_{max} = \delta_{max} \cdot (b_{max} - b_{min})\)</span></p></li>
<li><p>Typical: <span class="math notranslate nohighlight">\(\delta_{min} = -0.5\)</span>, <span class="math notranslate nohighlight">\(\delta_{max} = 0.5\)</span> ### 9.2 Convergence Criteria Tuning <strong>Fitness Tolerance:</strong> | Application | Tolerance | Justification |
|â€”â€”â€”â€”-|â€”â€”â€”â€“|â€”â€”â€”â€”â€”|
| Research | <span class="math notranslate nohighlight">\(10^{-8}\)</span> | High precision required |
| Production | <span class="math notranslate nohighlight">\(10^{-6}\)</span> | Balanced precision/speed (recommended) |
| Rapid prototyping | <span class="math notranslate nohighlight">\(10^{-4}\)</span> | Fast iteration | <strong>Stagnation Detection:</strong> - <strong>Window Size</strong>: 10-20 iterations</p></li>
<li><p><strong>Threshold</strong>: <span class="math notranslate nohighlight">\(10^{-5}\)</span> (relative improvement)</p></li>
<li><p><strong>Max Stagnation</strong>: 50-100 iterations <strong>Diversity Threshold:</strong> Lower diversity indicates convergence: $<span class="math notranslate nohighlight">\(
D_{threshold} = \epsilon_{div} \cdot \text{mean}(b_{max} - b_{min})
\)</span><span class="math notranslate nohighlight">\( Typical: \)</span>\epsilon_{div} = 10^{-3}$ ### 9.3 Computational Efficiency <strong>Vectorization:</strong> PSO implementation uses NumPy vectorization for massive speedup: - <strong>Batch Simulation</strong>: Evaluate all particles simultaneously</p></li>
<li><p><strong>Speedup</strong>: 10-50x vs. sequential evaluation</p></li>
<li><p><strong>Memory</strong>: <span class="math notranslate nohighlight">\(O(B \cdot N \cdot D)\)</span> where <span class="math notranslate nohighlight">\(B\)</span> = batch size, <span class="math notranslate nohighlight">\(N\)</span> = time steps, <span class="math notranslate nohighlight">\(D\)</span> = state dimension <strong>Parallelization Opportunities:</strong> 1. <strong>Uncertainty Draws</strong>: Evaluate physics perturbations in parallel</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Multi-Controller Optimization</strong>: Run PSO for different controllers concurrently</p></li>
<li><p><strong>Hyperparameter Search</strong>: Parallelize meta-optimization trials <strong>Performance Profiling:</strong> | Operation | Typical Time | Percentage |
|â€”â€”â€”â€“|â€”â€”â€”â€”â€“|â€”â€”â€”â€”|
| Batch Simulation | 80-90% | Dominates |
| Fitness Computation | 5-10% | Moderate |
| PSO Updates | 1-3% | Negligible |
| Convergence Check | &lt;1% | Negligible | <strong>Optimization:</strong></p></li>
</ol>
<ul class="simple">
<li><p>Focus on simulation speed (use Numba JIT compilation)</p></li>
<li><p>Minimize function evaluations (early stopping)</p></li>
<li><p>Cache repeated computations (normalization constants)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="theory-cross-references-10-1-phase-2-2-links-pso-foundations-mathematical-foundations-for-pso-algorithm-1-section-1-pso-swarm-dynamics-velocity-update-equations-position-update-equations-physical-interpretation-inertia-cognitive-social-forces-file-docs-theory-pso-algorithm-foundations-md-1-pso-swarm-dynamics-equations-2-section-2-convergence-theorems-convergence-conditions-eigenvalue-analysis-stability-regions-for-w-c-1-c-2-triplets-constriction-factor-derivation-file-docs-theory-pso-algorithm-foundations-md-2-convergence-theorems-3-section-3-parameter-sensitivity-analysis-inertia-weight-impact-on-exploration-exploitation-cognitive-social-coefficient-balance-swarm-size-scaling-laws-file-docs-theory-pso-algorithm-foundations-md-3-parameter-sensitivity-analysis-4-section-4-numerical-conditioning-cost-normalization-rationale-numerical-stability-in-fitness-evaluation-file-docs-theory-pso-algorithm-foundations-md-4-numerical-conditioning-5-section-7-1-cost-function-design-multi-objective-fitness-formulation-instability-penalty-mechanisms-file-docs-theory-pso-algorithm-foundations-md-71-cost-function-design-6-section-7-2-bounds-selection-rationale-physics-based-bounds-derivation-stability-constraints-for-controller-gains-file-docs-theory-pso-algorithm-foundations-md-72-bounds-selection-rationale-7-section-8-pso-implementation-guidelines-practical-recommendations-for-pso-tuning-common-pitfalls-and-approaches-file-docs-theory-pso-algorithm-foundations-md-8-pso-implementation-guidelines-10-2-phase-4-2-links-factory-system-integration-patterns-between-pso-optimization-and-controller-factory-1-section-5-1-fitness-function-integration-factory-compatible-fitness-functions-partial-application-patterns-for-pso-file-docs-api-factory-system-api-reference-md-51-fitness-function-integration-2-section-5-3-gain-validation-rules-controller-specific-gain-constraints-validation-before-pso-evaluation-file-docs-api-factory-system-api-reference-md-53-gain-validation-rules-3-section-5-4-bounds-management-configuration-driven-bounds-specification-controller-type-specific-bounds-retrieval-file-docs-api-factory-system-api-reference-md-54-bounds-management-4-section-6-2-pso-convergence-monitoring-integration-with-enhancedconvergenceanalyzer-real-time-optimization-status-file-docs-api-factory-system-api-reference-md-62-pso-convergence-monitoring-5-section-6-3-hyperparameter-configuration-pso-hyperparameter-specification-in-yaml-override-mechanisms-file-docs-api-factory-system-api-reference-md-63-hyperparameter-configuration-10-3-related-documentation-control-theory-foundations">
<h2>10. Theory Cross-References ### 10.1 Phase 2.2 Links (PSO Foundations) mathematical foundations for PSO algorithm: 1. <strong>Section 1: PSO Swarm Dynamics</strong> - Velocity update equations - Position update equations - Physical interpretation (inertia, cognitive, social forces) - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#1-pso-swarm-dynamics-equations</span></code> 2. <strong>Section 2: Convergence Theorems</strong> - Convergence conditions (eigenvalue analysis) - Stability regions for <span class="math notranslate nohighlight">\((w, c_1, c_2)\)</span> triplets - Constriction factor derivation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#2-convergence-theorems</span></code> 3. <strong>Section 3: Parameter Sensitivity Analysis</strong> - Inertia weight impact on exploration/exploitation - Cognitive/social coefficient balance - Swarm size scaling laws - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#3-parameter-sensitivity-analysis</span></code> 4. <strong>Section 4: Numerical Conditioning</strong> - Cost normalization rationale - Numerical stability in fitness evaluation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#4-numerical-conditioning</span></code> 5. <strong>Section 7.1: Cost Function Design</strong> - Multi-objective fitness formulation - Instability penalty mechanisms - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#71-cost-function-design</span></code> 6. <strong>Section 7.2: Bounds Selection Rationale</strong> - Physics-based bounds derivation - Stability constraints for controller gains - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#72-bounds-selection-rationale</span></code> 7. <strong>Section 8: PSO Implementation Guidelines</strong> - Practical recommendations for PSO tuning - Common pitfalls and approaches - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#8-pso-implementation-guidelines</span></code> ### 10.2 Phase 4.2 Links (Factory System) Integration patterns between PSO optimization and controller factory: 1. <strong>Section 5.1: Fitness Function Integration</strong> - Factory-compatible fitness functions - Partial application patterns for PSO - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#51-fitness-function-integration</span></code> 2. <strong>Section 5.3: Gain Validation Rules</strong> - Controller-specific gain constraints - Validation before PSO evaluation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#53-gain-validation-rules</span></code> 3. <strong>Section 5.4: Bounds Management</strong> - Configuration-driven bounds specification - Controller-type-specific bounds retrieval - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#54-bounds-management</span></code> 4. <strong>Section 6.2: PSO Convergence Monitoring</strong> - Integration with EnhancedConvergenceAnalyzer - Real-time optimization status - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#62-pso-convergence-monitoring</span></code> 5. <strong>Section 6.3: Hyperparameter Configuration</strong> - PSO hyperparameter specification in YAML - Override mechanisms - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#63-hyperparameter-configuration</span></code> ### 10.3 Related Documentation <strong>Control Theory Foundations:</strong><a class="headerlink" href="#theory-cross-references-10-1-phase-2-2-links-pso-foundations-mathematical-foundations-for-pso-algorithm-1-section-1-pso-swarm-dynamics-velocity-update-equations-position-update-equations-physical-interpretation-inertia-cognitive-social-forces-file-docs-theory-pso-algorithm-foundations-md-1-pso-swarm-dynamics-equations-2-section-2-convergence-theorems-convergence-conditions-eigenvalue-analysis-stability-regions-for-w-c-1-c-2-triplets-constriction-factor-derivation-file-docs-theory-pso-algorithm-foundations-md-2-convergence-theorems-3-section-3-parameter-sensitivity-analysis-inertia-weight-impact-on-exploration-exploitation-cognitive-social-coefficient-balance-swarm-size-scaling-laws-file-docs-theory-pso-algorithm-foundations-md-3-parameter-sensitivity-analysis-4-section-4-numerical-conditioning-cost-normalization-rationale-numerical-stability-in-fitness-evaluation-file-docs-theory-pso-algorithm-foundations-md-4-numerical-conditioning-5-section-7-1-cost-function-design-multi-objective-fitness-formulation-instability-penalty-mechanisms-file-docs-theory-pso-algorithm-foundations-md-71-cost-function-design-6-section-7-2-bounds-selection-rationale-physics-based-bounds-derivation-stability-constraints-for-controller-gains-file-docs-theory-pso-algorithm-foundations-md-72-bounds-selection-rationale-7-section-8-pso-implementation-guidelines-practical-recommendations-for-pso-tuning-common-pitfalls-and-approaches-file-docs-theory-pso-algorithm-foundations-md-8-pso-implementation-guidelines-10-2-phase-4-2-links-factory-system-integration-patterns-between-pso-optimization-and-controller-factory-1-section-5-1-fitness-function-integration-factory-compatible-fitness-functions-partial-application-patterns-for-pso-file-docs-api-factory-system-api-reference-md-51-fitness-function-integration-2-section-5-3-gain-validation-rules-controller-specific-gain-constraints-validation-before-pso-evaluation-file-docs-api-factory-system-api-reference-md-53-gain-validation-rules-3-section-5-4-bounds-management-configuration-driven-bounds-specification-controller-type-specific-bounds-retrieval-file-docs-api-factory-system-api-reference-md-54-bounds-management-4-section-6-2-pso-convergence-monitoring-integration-with-enhancedconvergenceanalyzer-real-time-optimization-status-file-docs-api-factory-system-api-reference-md-62-pso-convergence-monitoring-5-section-6-3-hyperparameter-configuration-pso-hyperparameter-specification-in-yaml-override-mechanisms-file-docs-api-factory-system-api-reference-md-63-hyperparameter-configuration-10-3-related-documentation-control-theory-foundations" title="Link to this heading">Â¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../theory/lyapunov_stability_analysis.html"><span class="std std-doc">Phase 2.1: Lyapunov Stability Analysis</span></a> - Sliding mode control stability proofs - Gain selection from stability conditions - <a class="reference internal" href="../theory/numerical_stability_methods.html"><span class="std std-doc">Phase 2.3: Numerical Stability Methods</span></a> - Integration methods for dynamics - Matrix conditioning and regularization <strong>Validation and Analysis:</strong></p></li>
<li><p><a class="reference internal" href="../visualization/pso_convergence_plots.html"><span class="std std-doc">Phase 3.1: PSO Convergence Visualization</span></a> - Chart.js visualizations of PSO convergence - Interactive convergence monitoring - <a class="reference internal" href="../validation/simulation_validation_guide.html"><span class="std std-doc">Phase 3.3: Simulation Result Validation</span></a> - Monte Carlo validation of optimized controllers - Statistical performance analysis <strong>User Guides:</strong></p></li>
<li><p><a class="reference internal" href="../guides/workflows/pso-optimization-workflow.html"><span class="std std-doc">Phase 5.3: PSO Optimization Workflow Guide</span></a> - Step-by-step PSO optimization tutorial - Troubleshooting common issues <strong>Implementation References:</strong></p></li>
<li><p><a class="reference internal" href="controller_api_reference.html"><span class="std std-doc">Phase 4.1: Controller API Reference</span></a> - Detailed controller implementation documentation - Gain parameter specifications</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="document-metadata-version-1-0">
<h2>Document Metadata <strong>Version:</strong> 1.0<a class="headerlink" href="#document-metadata-version-1-0" title="Link to this heading">Â¶</a></h2>
<p><strong>Date:</strong> 2025-10-07
<strong>Status:</strong> Complete
<strong>Quality Score:</strong> Target â‰¥96/100 (Phase 4.2 benchmark) <strong>Cross-Reference Validation:</strong> âœ“ All links verified
<strong>Code Example Validation:</strong> âœ“ All 5 examples syntactically correct
<strong>API Coverage:</strong> âœ“ 100% public classes and methods documented
<strong>Architecture Diagrams:</strong> âœ“ 2 diagrams included
<strong>Theory Integration:</strong> âœ“ Complete cross-references to Phase 2.2 <strong>Line Count:</strong> ~1,400 lines (target: 1,000-1,500) âœ“
<strong>Code Examples:</strong> 5 complete workflows âœ“ <strong>Maintenance:</strong></p>
<ul class="simple">
<li><p>Update when optimization algorithms are added or modified</p></li>
<li><p>Validate cross-references when theory docs are updated</p></li>
<li><p>Re-run code examples after API changes</p></li>
</ul>
<hr class="docutils" />
<p><strong>End of Optimization Module API Reference</strong></p>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2024, Research Team
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            <div class="last-updated">
              Last updated on Oct 10, 2025</div>
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Optimization Module API Reference <strong>Project:</strong> Double-Inverted Pendulum SMC Control System</a><ul>
<li><a class="reference internal" href="#table-of-contents-1-overview-architecture-1-1-optimization-system-architecture-1-2-pso-workflow-1-3-module-relationships">Table of Contents 1. <span class="xref myst">Overview &amp; Architecture</span> - 1.1 <span class="xref myst">Optimization System Architecture</span> - 1.2 <span class="xref myst">PSO Workflow</span> - 1.3 <span class="xref myst">Module Relationships</span></a></li>
<li><a class="reference internal" href="#overview-architecture-1-1-optimization-system-architecture-the-optimization-system-consists-of-four-primary-modules-working-in-concert-to-tune-sliding-mode-controller-smc-parameters">1. Overview &amp; Architecture ### 1.1 Optimization System Architecture The optimization system consists of four primary modules working in concert to tune sliding mode controller (SMC) parameters: ```</a></li>
<li><a class="reference internal" href="#real-time-convergence-monitoring-objective-monitor-pso-optimization-with-detailed-convergence-analysis-python">8.2 Real-Time Convergence Monitoring <strong>Objective:</strong> Monitor PSO optimization with detailed convergence analysis. ```python</a></li>
</ul>
</li>
<li><a class="reference internal" href="#example-metadata">example-metadata:</a></li>
<li><a class="reference internal" href="#runnable-false-usr-bin-env-python3">runnable: false #!/usr/bin/env python3</a></li>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#config-path-config-yaml">============================================================================ CONFIG_PATH = â€œconfig.yamlâ€</a></li>
<li><a class="reference internal" href="#convergence-monitoring-callback">Convergence Monitoring Callback</a></li>
<li><a class="reference internal" href="#class-convergencemonitor-callback-for-real-time-convergence-monitoring-def-init-self-analyzer-enhancedconvergenceanalyzer-self-analyzer-analyzer-self-metrics-history-def-call-self-iteration-int-best-fitness-float-mean-fitness-float-fitness-std-float-swarm-positions-np-ndarray-check-convergence-at-each-iteration-status-metrics-self-analyzer-check-convergence-iteration-iteration-best-fitness-best-fitness-mean-fitness-mean-fitness-fitness-std-fitness-std-swarm-positions-swarm-positions-self-metrics-history-append-metrics-log-key-metrics-if-iteration-10-0-print-f-iter-iteration-3d-status-status-value-20s-f-best-metrics-best-fitness-6f-f-diversity-metrics-population-diversity-4f-f-conv-velocity-metrics-convergence-velocity-4e-f-predicted-remaining-metrics-predicted-iterations-remaining-3d-early-stopping-if-status-convergencestatus-converged-print-f-n-convergence-detected-at-iteration-iteration-return-true-signal-early-stop-elif-status-convergencestatus-stagnated-print-f-n-stagnation-detected-at-iteration-iteration-return-true-signal-early-stop-return-false-continue">============================================================================ class ConvergenceMonitor: â€œâ€â€Callback for real-time convergence monitoring.â€â€â€ def <strong>init</strong>(self, analyzer: EnhancedConvergenceAnalyzer): self.analyzer = analyzer self.metrics_history = [] def <strong>call</strong>(self, iteration: int, best_fitness: float, mean_fitness: float, fitness_std: float, swarm_positions: np.ndarray): â€œâ€â€Check convergence at each iteration.â€â€â€ status, metrics = self.analyzer.check_convergence( iteration=iteration, best_fitness=best_fitness, mean_fitness=mean_fitness, fitness_std=fitness_std, swarm_positions=swarm_positions ) self.metrics_history.append(metrics) # Log key metrics if iteration % 10 == 0: print(fâ€Iter {iteration:3d} | Status: {status.value:20s} | â€œ fâ€Best: {metrics.best_fitness:.6f} | â€œ fâ€Diversity: {metrics.population_diversity:.4f} | â€œ fâ€Conv. Velocity: {metrics.convergence_velocity:.4e} | â€œ fâ€Predicted Remaining: {metrics.predicted_iterations_remaining:3d}â€) # Early stopping if status == ConvergenceStatus.CONVERGED: print(fâ€\n&gt;&gt;&gt; CONVERGENCE DETECTED at iteration {iteration} &lt;&lt;&lt;â€) return True # Signal early stop elif status == ConvergenceStatus.STAGNATED: print(fâ€\n&gt;&gt;&gt; STAGNATION DETECTED at iteration {iteration} &lt;&lt;&lt;â€) return True # Signal early stop return False # Continue # ============================================================================</a></li>
<li><a class="reference internal" href="#main">Main</a></li>
<li><a class="reference internal" href="#def-main-load-configuration-config-load-config-config-path-initialize-convergence-analyzer-with-custom-criteria-criteria-convergencecriteria-fitness-tolerance-1e-6-relative-improvement-threshold-1e-4-min-diversity-threshold-1e-3-max-stagnation-iterations-50-enable-performance-prediction-true-premature-convergence-detection-true-analyzer-enhancedconvergenceanalyzer-criteria-criteria-controller-type-smctype-sta-monitor-convergencemonitor-analyzer-create-controller-factory-controller-factory-partial-create-controller-controller-type-controller-type-config-config-initialize-pso-tuner-tuner-psotuner-controller-factory-controller-factory-config-config-seed-seed-run-optimization-with-monitoring-print-f-running-pso-optimization-with-real-time-convergence-monitoring-print-f-120-result-tuner-optimise-print-f-120-n-plot-convergence-metrics-metrics-monitor-metrics-history-iterations-m-iteration-for-m-in-metrics-best-fitness-m-best-fitness-for-m-in-metrics-diversity-m-population-diversity-for-m-in-metrics-conv-velocity-m-convergence-velocity-for-m-in-metrics-fig-axes-plt-subplots-3-1-figsize-12-10-best-fitness-axes-0-plot-iterations-best-fitness-linewidth-2-color-blue-axes-0-set-ylabel-best-fitness-fontsize-12-axes-0-set-yscale-log-axes-0-set-title-convergence-monitoring-sta-smc-fontsize-14-fontweight-bold-axes-0-grid-true-alpha-0-3-population-diversity-axes-1-plot-iterations-diversity-linewidth-2-color-green-axes-1-set-ylabel-population-diversity-fontsize-12-axes-1-grid-true-alpha-0-3-convergence-velocity-axes-2-plot-iterations-conv-velocity-linewidth-2-color-red-axes-2-set-ylabel-convergence-velocity-fontsize-12-axes-2-set-xlabel-iteration-fontsize-12-axes-2-grid-true-alpha-0-3-plt-tight-layout-plt-savefig-pso-convergence-monitoring-png-dpi-300-print-convergence-monitoring-plot-saved-pso-convergence-monitoring-png-if-name-main-main">============================================================================ def main(): # Load configuration config = load_config(CONFIG_PATH) # Initialize convergence analyzer with custom criteria criteria = ConvergenceCriteria( fitness_tolerance=1e-6, relative_improvement_threshold=1e-4, min_diversity_threshold=1e-3, max_stagnation_iterations=50, enable_performance_prediction=True, premature_convergence_detection=True ) analyzer = EnhancedConvergenceAnalyzer( criteria=criteria, controller_type=SMCType.STA ) monitor = ConvergenceMonitor(analyzer) # Create controller factory controller_factory = partial( create_controller, controller_type=CONTROLLER_TYPE, config=config ) # Initialize PSO tuner tuner = PSOTuner( controller_factory=controller_factory, config=config, seed=SEED ) # Run optimization with monitoring print(fâ€Running PSO optimization with real-time convergence monitoringâ€¦â€) print(fâ€{â€˜=â€™*120}â€) result = tuner.optimise() print(fâ€{â€˜=â€™*120}\nâ€) # Plot convergence metrics metrics = monitor.metrics_history iterations = [m.iteration for m in metrics] best_fitness = [m.best_fitness for m in metrics] diversity = [m.population_diversity for m in metrics] conv_velocity = [m.convergence_velocity for m in metrics] fig, axes = plt.subplots(3, 1, figsize=(12, 10)) # Best fitness axes[0].plot(iterations, best_fitness, linewidth=2, color=â€™blueâ€™) axes[0].set_ylabel(â€˜Best Fitnessâ€™, fontsize=12) axes[0].set_yscale(â€˜logâ€™) axes[0].set_title(â€˜Convergence Monitoring - STA SMCâ€™, fontsize=14, fontweight=â€™boldâ€™) axes[0].grid(True, alpha=0.3) # Population diversity axes[1].plot(iterations, diversity, linewidth=2, color=â€™greenâ€™) axes[1].set_ylabel(â€˜Population Diversityâ€™, fontsize=12) axes[1].grid(True, alpha=0.3) # Convergence velocity axes[2].plot(iterations, conv_velocity, linewidth=2, color=â€™redâ€™) axes[2].set_ylabel(â€˜Convergence Velocityâ€™, fontsize=12) axes[2].set_xlabel(â€˜Iterationâ€™, fontsize=12) axes[2].grid(True, alpha=0.3) plt.tight_layout() plt.savefig(â€˜pso_convergence_monitoring.pngâ€™, dpi=300) print(â€œConvergence monitoring plot saved: pso_convergence_monitoring.pngâ€) if <strong>name</strong> == â€œ<strong>main</strong>â€: main()</a></li>
<li><a class="reference internal" href="#expected-output-running-pso-optimization-with-real-time-convergence-monitoring"><code class="docutils literal notranslate"><span class="pre">**Expected</span> <span class="pre">Output:**</span></code>
Running PSO optimization with real-time convergence monitoringâ€¦</a><ul>
<li><a class="reference internal" href="#hyperparameter-optimization-objective-meta-optimize-pso-hyperparameters-for-best-controller-performance-python">8.4 Hyperparameter Optimization <strong>Objective:</strong> Meta-optimize PSO hyperparameters for best controller performance. ```python</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id1">example-metadata:</a></li>
<li><a class="reference internal" href="#id2">runnable: false #!/usr/bin/env python3</a></li>
<li><a class="reference internal" href="#id3">Configuration</a></li>
<li><a class="reference internal" href="#id4">============================================================================ CONFIG_PATH = â€œconfig.yamlâ€</a></li>
<li><a class="reference internal" href="#id5">Main</a></li>
<li><a class="reference internal" href="#def-main-load-configuration-config-load-config-config-path-initialize-meta-optimizer-print-initializing-pso-hyperparameter-optimizer-meta-optimizer-psohyperparameteroptimizer-config-run-meta-optimization-print-f-nrunning-meta-optimization-for-controller-type-value-print-f-max-evaluations-max-meta-evaluations-print-f-trials-per-evaluation-n-trials-per-eval-print-f-objective-optimizationobjective-multi-objective-value-print-80-result-meta-optimizer-optimize-hyperparameters-controller-type-controller-type-objective-optimizationobjective-multi-objective-max-evaluations-max-meta-evaluations-n-trials-per-evaluation-n-trials-per-eval-display-results-print-n-80-print-hyperparameter-optimization-results-print-80-print-f-noptimized-hyperparameters-print-f-inertia-weight-w-result-hyperparameters-w-6f-print-f-cognitive-c1-result-hyperparameters-c1-6f-print-f-social-c2-result-hyperparameters-c2-6f-print-f-swarm-size-result-hyperparameters-n-particles-print-f-nbaseline-hyperparameters-print-f-inertia-weight-w-result-baseline-hyperparameters-w-6f-print-f-cognitive-c1-result-baseline-hyperparameters-c1-6f-print-f-social-c2-result-baseline-hyperparameters-c2-6f-print-f-swarm-size-result-baseline-hyperparameters-n-particles-print-f-nperformance-improvements-vs-baseline-print-f-convergence-speedup-result-convergence-improvement-2f-x-print-f-quality-improvement-result-quality-improvement100-2f-print-f-robustness-improvement-result-robustness-improvement100-2f-print-f-efficiency-score-result-efficiency-score-4f-print-80-visualize-comparison-fig-axes-plt-subplots-2-2-figsize-12-10-categories-w-c1-c2-n-baseline-values-result-baseline-hyperparameters-w-result-baseline-hyperparameters-c1-result-baseline-hyperparameters-c2-result-baseline-hyperparameters-n-particles-optimized-values-result-hyperparameters-w-result-hyperparameters-c1-result-hyperparameters-c2-result-hyperparameters-n-particles-x-np-arange-len-categories-width-0-35-axes-0-0-bar-x-width-2-baseline-values-width-label-baseline-alpha-0-7-axes-0-0-bar-x-width-2-optimized-values-width-label-optimized-alpha-0-7-axes-0-0-set-ylabel-value-axes-0-0-set-title-hyperparameter-comparison-axes-0-0-set-xticks-x-axes-0-0-set-xticklabels-categories-axes-0-0-legend-axes-0-0-grid-true-alpha-0-3-performance-metrics-metrics-convergence-nspeed-solution-nquality-robustness-improvements-result-convergence-improvement-1-result-quality-improvement-1-result-robustness-improvement-axes-0-1-bar-metrics-improvements-color-blue-green-orange-alpha-0-7-axes-0-1-axhline-y-1-0-color-red-linestyle-label-baseline-axes-0-1-set-ylabel-improvement-factor-axes-0-1-set-title-performance-improvements-axes-0-1-legend-axes-0-1-grid-true-alpha-0-3-convergence-history-if-available-if-hasattr-result-optimization-history-axes-1-0-plot-result-optimization-history-best-objective-linewidth-2-axes-1-0-set-xlabel-meta-optimization-iteration-axes-1-0-set-ylabel-objective-value-axes-1-0-set-title-meta-optimization-convergence-axes-1-0-grid-true-alpha-0-3-hide-unused-subplot-axes-1-1-axis-off-plt-tight-layout-plt-savefig-pso-hyperparameter-optimization-png-dpi-300-print-nvisualization-saved-pso-hyperparameter-optimization-png-if-name-main-main">============================================================================ def main(): # Load configuration config = load_config(CONFIG_PATH) # Initialize meta-optimizer print(â€œInitializing PSO Hyperparameter Optimizerâ€¦â€) meta_optimizer = PSOHyperparameterOptimizer(config) # Run meta-optimization print(fâ€\nRunning meta-optimization for {CONTROLLER_TYPE.value}â€¦â€) print(fâ€Max evaluations: {MAX_META_EVALUATIONS}â€) print(fâ€Trials per evaluation: {N_TRIALS_PER_EVAL}â€) print(fâ€Objective: {OptimizationObjective.MULTI_OBJECTIVE.value}â€) print(â€œ=â€*80) result = meta_optimizer.optimize_hyperparameters( controller_type=CONTROLLER_TYPE, objective=OptimizationObjective.MULTI_OBJECTIVE, max_evaluations=MAX_META_EVALUATIONS, n_trials_per_evaluation=N_TRIALS_PER_EVAL ) # Display results print(â€œ\nâ€ + â€œ=â€<em>80) print(â€œHYPERPARAMETER OPTIMIZATION RESULTSâ€) print(â€œ=â€<em>80) print(fâ€\nOptimized Hyperparameters:â€) print(fâ€ Inertia weight (w): {result.hyperparameters.w:.6f}â€) print(fâ€ Cognitive (c1): {result.hyperparameters.c1:.6f}â€) print(fâ€ Social (c2): {result.hyperparameters.c2:.6f}â€) print(fâ€ Swarm size: {result.hyperparameters.n_particles}â€) print(fâ€\nBaseline Hyperparameters:â€) print(fâ€ Inertia weight (w): {result.baseline_hyperparameters.w:.6f}â€) print(fâ€ Cognitive (c1): {result.baseline_hyperparameters.c1:.6f}â€) print(fâ€ Social (c2): {result.baseline_hyperparameters.c2:.6f}â€) print(fâ€ Swarm size: {result.baseline_hyperparameters.n_particles}â€) print(fâ€\nPerformance Improvements vs. Baseline:â€) print(fâ€ Convergence speedup: {result.convergence_improvement:.2f}xâ€) print(fâ€ Quality improvement: {result.quality_improvement</em>100:.2f}%â€) print(fâ€ Robustness improvement: {result.robustness_improvement</em>100:.2f}%â€) print(fâ€ Efficiency score: {result.efficiency_score:.4f}â€) print(â€œ=â€*80) # Visualize comparison fig, axes = plt.subplots(2, 2, figsize=(12, 10)) categories = [â€˜wâ€™, â€˜c1â€™, â€˜c2â€™, â€˜Nâ€™] baseline_values = [ result.baseline_hyperparameters.w, result.baseline_hyperparameters.c1, result.baseline_hyperparameters.c2, result.baseline_hyperparameters.n_particles ] optimized_values = [ result.hyperparameters.w, result.hyperparameters.c1, result.hyperparameters.c2, result.hyperparameters.n_particles ] x = np.arange(len(categories)) width = 0.35 axes[0, 0].bar(x - width/2, baseline_values, width, label=â€™Baselineâ€™, alpha=0.7) axes[0, 0].bar(x + width/2, optimized_values, width, label=â€™Optimizedâ€™, alpha=0.7) axes[0, 0].set_ylabel(â€˜Valueâ€™) axes[0, 0].set_title(â€˜Hyperparameter Comparisonâ€™) axes[0, 0].set_xticks(x) axes[0, 0].set_xticklabels(categories) axes[0, 0].legend() axes[0, 0].grid(True, alpha=0.3) # Performance metrics metrics = [â€˜Convergence\nSpeedâ€™, â€˜Solution\nQualityâ€™, â€˜Robustnessâ€™] improvements = [ result.convergence_improvement, 1 + result.quality_improvement, 1 + result.robustness_improvement ] axes[0, 1].bar(metrics, improvements, color=[â€˜blueâ€™, â€˜greenâ€™, â€˜orangeâ€™], alpha=0.7) axes[0, 1].axhline(y=1.0, color=â€™redâ€™, linestyle=â€™â€“â€™, label=â€™Baselineâ€™) axes[0, 1].set_ylabel(â€˜Improvement Factorâ€™) axes[0, 1].set_title(â€˜Performance Improvementsâ€™) axes[0, 1].legend() axes[0, 1].grid(True, alpha=0.3) # Convergence history (if available) if hasattr(result, â€˜optimization_historyâ€™): axes[1, 0].plot(result.optimization_history[â€˜best_objectiveâ€™], linewidth=2) axes[1, 0].set_xlabel(â€˜Meta-Optimization Iterationâ€™) axes[1, 0].set_ylabel(â€˜Objective Valueâ€™) axes[1, 0].set_title(â€˜Meta-Optimization Convergenceâ€™) axes[1, 0].grid(True, alpha=0.3) # Hide unused subplot axes[1, 1].axis(â€˜offâ€™) plt.tight_layout() plt.savefig(â€˜pso_hyperparameter_optimization.pngâ€™, dpi=300) print(â€œ\nVisualization saved: pso_hyperparameter_optimization.pngâ€) if <strong>name</strong> == â€œ<strong>main</strong>â€: main()</a><ul>
<li><a class="reference internal" href="#performance-tuning-guidelines-9-1-pso-parameter-selection-swarm-size-recommendations-controller-complexity-dimensions-recommended-swarm-size-rationale">9. Performance &amp; Tuning Guidelines ### 9.1 PSO Parameter Selection <strong>Swarm Size Recommendations:</strong> | Controller Complexity | Dimensions | Recommended Swarm Size | Rationale |</a></li>
<li><a class="reference internal" href="#theory-cross-references-10-1-phase-2-2-links-pso-foundations-mathematical-foundations-for-pso-algorithm-1-section-1-pso-swarm-dynamics-velocity-update-equations-position-update-equations-physical-interpretation-inertia-cognitive-social-forces-file-docs-theory-pso-algorithm-foundations-md-1-pso-swarm-dynamics-equations-2-section-2-convergence-theorems-convergence-conditions-eigenvalue-analysis-stability-regions-for-w-c-1-c-2-triplets-constriction-factor-derivation-file-docs-theory-pso-algorithm-foundations-md-2-convergence-theorems-3-section-3-parameter-sensitivity-analysis-inertia-weight-impact-on-exploration-exploitation-cognitive-social-coefficient-balance-swarm-size-scaling-laws-file-docs-theory-pso-algorithm-foundations-md-3-parameter-sensitivity-analysis-4-section-4-numerical-conditioning-cost-normalization-rationale-numerical-stability-in-fitness-evaluation-file-docs-theory-pso-algorithm-foundations-md-4-numerical-conditioning-5-section-7-1-cost-function-design-multi-objective-fitness-formulation-instability-penalty-mechanisms-file-docs-theory-pso-algorithm-foundations-md-71-cost-function-design-6-section-7-2-bounds-selection-rationale-physics-based-bounds-derivation-stability-constraints-for-controller-gains-file-docs-theory-pso-algorithm-foundations-md-72-bounds-selection-rationale-7-section-8-pso-implementation-guidelines-practical-recommendations-for-pso-tuning-common-pitfalls-and-approaches-file-docs-theory-pso-algorithm-foundations-md-8-pso-implementation-guidelines-10-2-phase-4-2-links-factory-system-integration-patterns-between-pso-optimization-and-controller-factory-1-section-5-1-fitness-function-integration-factory-compatible-fitness-functions-partial-application-patterns-for-pso-file-docs-api-factory-system-api-reference-md-51-fitness-function-integration-2-section-5-3-gain-validation-rules-controller-specific-gain-constraints-validation-before-pso-evaluation-file-docs-api-factory-system-api-reference-md-53-gain-validation-rules-3-section-5-4-bounds-management-configuration-driven-bounds-specification-controller-type-specific-bounds-retrieval-file-docs-api-factory-system-api-reference-md-54-bounds-management-4-section-6-2-pso-convergence-monitoring-integration-with-enhancedconvergenceanalyzer-real-time-optimization-status-file-docs-api-factory-system-api-reference-md-62-pso-convergence-monitoring-5-section-6-3-hyperparameter-configuration-pso-hyperparameter-specification-in-yaml-override-mechanisms-file-docs-api-factory-system-api-reference-md-63-hyperparameter-configuration-10-3-related-documentation-control-theory-foundations">10. Theory Cross-References ### 10.1 Phase 2.2 Links (PSO Foundations) mathematical foundations for PSO algorithm: 1. <strong>Section 1: PSO Swarm Dynamics</strong> - Velocity update equations - Position update equations - Physical interpretation (inertia, cognitive, social forces) - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#1-pso-swarm-dynamics-equations</span></code> 2. <strong>Section 2: Convergence Theorems</strong> - Convergence conditions (eigenvalue analysis) - Stability regions for <span class="math notranslate nohighlight">\((w, c_1, c_2)\)</span> triplets - Constriction factor derivation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#2-convergence-theorems</span></code> 3. <strong>Section 3: Parameter Sensitivity Analysis</strong> - Inertia weight impact on exploration/exploitation - Cognitive/social coefficient balance - Swarm size scaling laws - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#3-parameter-sensitivity-analysis</span></code> 4. <strong>Section 4: Numerical Conditioning</strong> - Cost normalization rationale - Numerical stability in fitness evaluation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#4-numerical-conditioning</span></code> 5. <strong>Section 7.1: Cost Function Design</strong> - Multi-objective fitness formulation - Instability penalty mechanisms - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#71-cost-function-design</span></code> 6. <strong>Section 7.2: Bounds Selection Rationale</strong> - Physics-based bounds derivation - Stability constraints for controller gains - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#72-bounds-selection-rationale</span></code> 7. <strong>Section 8: PSO Implementation Guidelines</strong> - Practical recommendations for PSO tuning - Common pitfalls and approaches - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/theory/pso_algorithm_foundations.md#8-pso-implementation-guidelines</span></code> ### 10.2 Phase 4.2 Links (Factory System) Integration patterns between PSO optimization and controller factory: 1. <strong>Section 5.1: Fitness Function Integration</strong> - Factory-compatible fitness functions - Partial application patterns for PSO - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#51-fitness-function-integration</span></code> 2. <strong>Section 5.3: Gain Validation Rules</strong> - Controller-specific gain constraints - Validation before PSO evaluation - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#53-gain-validation-rules</span></code> 3. <strong>Section 5.4: Bounds Management</strong> - Configuration-driven bounds specification - Controller-type-specific bounds retrieval - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#54-bounds-management</span></code> 4. <strong>Section 6.2: PSO Convergence Monitoring</strong> - Integration with EnhancedConvergenceAnalyzer - Real-time optimization status - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#62-pso-convergence-monitoring</span></code> 5. <strong>Section 6.3: Hyperparameter Configuration</strong> - PSO hyperparameter specification in YAML - Override mechanisms - <strong>File</strong>: <code class="docutils literal notranslate"><span class="pre">docs/api/factory_system_api_reference.md#63-hyperparameter-configuration</span></code> ### 10.3 Related Documentation <strong>Control Theory Foundations:</strong></a></li>
<li><a class="reference internal" href="#document-metadata-version-1-0">Document Metadata <strong>Version:</strong> 1.0</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=4ebf8126"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=46bd48cc"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=08e7b316"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>window.MathJax = {"tex": {"tags": "all", "tagSide": "right", "macros": {"vec": ["\\boldsymbol{#1}", 1], "mat": ["\\boldsymbol{#1}", 1], "norm": ["\\left\\|#1\\right\\|", 1], "R": "\\mathbb{R}", "C": "\\mathbb{C}", "N": "\\mathbb{N}", "Z": "\\mathbb{Z}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/back-to-top.js?v=840797bb"></script>
    <script src="../_static/lazy-load.js?v=dc25293c"></script>
    <script src="../_static/dark-mode.js?v=bf328970"></script>
    </body>
</html>