# Benchmarking Framework Technical Guide **DIP-SMC-PSO Project**
**Last Updated**: 2025-10-04
**Status**: Production-Ready Modular Benchmarking Infrastructure --- ## Table of Contents 1. [Overview](#1-overview)
2. [Statistical Benchmarking Framework](#2-statistical-benchmarking-framework)
3. [Integration Benchmarking Framework](#3-integration-benchmarking-framework)
4. [Performance Benchmarking](#4-performance-benchmarking)
5. [Benchmark Configuration](#5-benchmark-configuration)
6. [Benchmark Execution](#6-benchmark-execution) --- ## 1. Overview ### 1.1 Benchmarking Architecture The DIP-SMC-PSO project employs a **dual benchmarking framework** for performance validation: 1. **Statistical Benchmarking** (`src/benchmarks/`) - Controller performance evaluation with statistical rigor
2. **Integration Benchmarking** (`benchmarks/`) - Numerical integration method comparison ```
Benchmarking Infrastructure
├── src/benchmarks/ # Statistical benchmarking framework (modular)
│ ├── metrics/ # Performance metric computation
│ │ ├── control_metrics.py # ISE, ITAE, RMS control effort
│ │ ├── stability_metrics.py # Overshoot, damping, oscillation
│ │ ├── constraint_metrics.py # Saturation violations
│ │ └── __init__.py # Unified metrics interface
│ ├── core/ # Trial execution engine
│ │ ├── trial_runner.py # Multi-trial orchestration
│ │ └── __init__.py
│ ├── statistics/ # Statistical analysis
│ │ ├── confidence_intervals.py # CI computation, hypothesis testing
│ │ └── __init__.py
│ ├── config/ # Benchmark configuration
│ └── statistical_benchmarks_v2.py # Main interface (backward compatible)
│
├── benchmarks/ # Integration benchmarking framework
│ ├── integration/ # Numerical integration methods
│ │ ├── numerical_methods.py # Euler, RK4, RK45 implementations
│ │ └── __init__.py
│ ├── analysis/ # Accuracy & performance analysis
│ │ ├── accuracy_metrics.py # Energy conservation, convergence
│ │ └── __init__.py
│ ├── comparison/ # Method comparison framework
│ │ ├── method_comparison.py # Systematic comparison engine
│ │ └── __init__.py
│ ├── benchmark/ # Benchmark orchestration
│ │ ├── integration_benchmark.py # Main benchmark class
│ │ └── __init__.py
│ └── examples/ # Usage demonstrations
│
└── tests/test_benchmarks/ # Benchmark framework tests (18 files) ├── core/ # Core functionality tests ├── statistics/ # Statistical analysis tests ├── integration/ # Integration method tests └── performance/ # Performance regression tests
``` ### 1.2 Design Principles #### 1.2.1 Modular Architecture
- **Single Responsibility**: Each module handles one specific domain
- **Dependency Injection**: Configurable components for flexibility
- **Backward Compatibility**: Legacy interfaces preserved during refactoring #### 1.2.2 Scientific Rigor
- **Statistical Validity**: CLT-compliant sample sizes (n≥30)
- **Reproducibility**: Deterministic seeding for all random processes
- **Confidence Intervals**: 95% CI reported for all metrics
- **Hypothesis Testing**: Welch's t-test for controller comparisons #### 1.2.3 Performance Focus
- **Numba Acceleration**: JIT compilation for numerical computations
- **Vectorization**: Batch processing for parallel efficiency
- **Minimal Overhead**: <5% benchmarking overhead on measurements ### 1.3 Key Metrics #### Performance Metrics
| Metric | Formula | Units | Purpose |
|--------|---------|-------|---------|
| **ISE** | ∫₀ᵀ ‖x(t)‖² dt | [state²·time] | Cumulative tracking error |
| **ITAE** | ∫₀ᵀ t·‖x(t)‖ dt | [state·time²] | Time-weighted tracking error |
| **RMS Control** | √(1/T ∫₀ᵀ u(t)² dt) | [Force] | Average control energy |
| **Max Overshoot** | max_{t∈[0,T]} ‖θᵢ(t)‖ | [radians] | Peak angular deviation |
| **Violations** | &#124;{t : &#124;u(t)&#124; > u_max}&#124; | [count] | Constraint violation count |
| **Sliding Energy** | ∫₀ᵀ σ(t)² dt | [dimensionless] | Sliding surface adherence | #### Integration Metrics
| Metric | Formula | Purpose |
|--------|---------|---------|
| **Energy Drift** | &#124;E(T) - E(0)&#124; / E(0) | Conservation accuracy |
| **Convergence Order** | log(e_h/e_{h/2}) / log(2) | Integration accuracy |
| **Computation Time** | Wall-clock execution time | Efficiency measurement | --- ## 2. Statistical Benchmarking Framework ### 2.1 Modular Architecture The statistical benchmarking system uses a modular architecture introduced in **September 2024** for enhanced maintainability: ```python
# High-level workflow
from src.benchmarks import run_trials, compute_all_metrics
from src.benchmarks.statistics import compute_t_confidence_intervals # Execute trials
metrics_list, ci_results = run_trials( controller_factory=create_controller, cfg=config, n_trials=30, seed=42
) # Confidence intervals automatically computed
for metric, (mean, ci_width) in ci_results.items(): print(f"{metric}: {mean:.4f} ± {ci_width:.4f}")
``` ### 2.2 Metrics Module (`src/benchmarks/metrics/`) #### 2.2.1 Control Metrics ```python
# example-metadata:
# runnable: false # src/benchmarks/metrics/control_metrics.py def compute_ise(t: np.ndarray, x: np.ndarray) -> float: """Compute Integral of Squared Error (ISE). ISE = ∫₀ᵀ ||x(t)||² dt Physical Interpretation: Measures cumulative deviation from desired trajectory. Lower values indicate better tracking performance. Control Engineering Context: - Quadratic cost function component in optimal control - Related to H₂ norm of closed-loop system - Emphasizes large deviations more than small ones Parameters ---------- t : np.ndarray Time vector, shape (N+1,) x : np.ndarray State trajectories, shape (B, N+1, S) for B batches, S states Returns ------- float ISE value averaged across batch dimension Examples -------- >>> t = np.linspace(0, 5, 501) >>> x = np.random.randn(10, 501, 6) # 10 trials, 6 states >>> ise = compute_ise(t, x) >>> print(f"ISE: {ise:.4f}") """ dt = t[1] - t[0] # Handle batched or single trajectory if x.ndim == 3: # Batched: (B, N, S) squared_errors = np.sum(x**2, axis=2) # Sum over states ise_per_batch = np.sum(squared_errors, axis=1) * dt return np.mean(ise_per_batch) else: # Single trajectory: (N, S) squared_errors = np.sum(x**2, axis=1) return np.sum(squared_errors) * dt def compute_itae(t: np.ndarray, x: np.ndarray) -> float: """Compute Integral of Time-weighted Absolute Error (ITAE). ITAE = ∫₀ᵀ t·||x(t)|| dt Emphasizes errors occurring later in the trajectory. Penalizes sustained deviations more heavily than transient errors. Parameters ---------- t : np.ndarray Time vector x : np.ndarray State trajectories Returns ------- float ITAE value Notes ----- ITAE is preferred for evaluating settling characteristics because: - Late-stage errors receive higher penalty - Encourages faster settling to equilibrium - Better reflects control quality in stabilization tasks """ dt = t[1] - t[0] if x.ndim == 3: abs_errors = np.linalg.norm(x, axis=2) # (B, N) time_weighted = abs_errors * t[np.newaxis, :] itae_per_batch = np.sum(time_weighted, axis=1) * dt return np.mean(itae_per_batch) else: abs_errors = np.linalg.norm(x, axis=1) time_weighted = abs_errors * t return np.sum(time_weighted) * dt def compute_rms_control(t: np.ndarray, u: np.ndarray) -> float: """Compute RMS (Root Mean Square) control effort. RMS_u = √(1/T ∫₀ᵀ u(t)² dt) Measures average control energy consumption. Important for actuator sizing and power requirements. Parameters ---------- t : np.ndarray Time vector u : np.ndarray Control history Returns ------- float RMS control effort [Force] """ dt = t[1] - t[0] T = t[-1] - t[0] if u.ndim == 2: # Batched squared_control = u**2 integral = np.sum(squared_control, axis=1) * dt rms_per_batch = np.sqrt(integral / T) return np.mean(rms_per_batch) else: squared_control = u**2 integral = np.sum(squared_control) * dt return np.sqrt(integral / T) def compute_control_rate(t: np.ndarray, u: np.ndarray) -> float: """Compute RMS control rate (slew rate). du_RMS = √(1/T ∫₀ᵀ (du/dt)² dt) Measures control signal smoothness. High values indicate chattering or aggressive switching. Parameters ---------- t : np.ndarray Time vector u : np.ndarray Control history Returns ------- float RMS control rate [Force/time] Notes ----- Critical for: - Actuator wear and lifetime estimation - Implementation feasibility (discrete-time constraints) - Chattering quantification in sliding mode control """ dt = t[1] - t[0] if u.ndim == 2: du_dt = np.diff(u, axis=1) / dt return compute_rms_control(t[:-1], du_dt) else: du_dt = np.diff(u) / dt return compute_rms_control(t[:-1], du_dt)
``` #### 2.2.2 Stability Metrics ```python
# src/benchmarks/metrics/stability_metrics.py def compute_overshoot(x: np.ndarray, state_indices: List[int] = [1, 2]) -> float: """Compute maximum overshoot for angular states. Overshoot = max_{t∈[0,T]} |θᵢ(t)| for i ∈ {1,2} Safety-critical for physical systems. Parameters ---------- x : np.ndarray State trajectories, shape (B, N, S) or (N, S) state_indices : list of int Indices of angular states (default: [1, 2] for θ1, θ2) Returns ------- float Maximum overshoot [radians] """ if x.ndim == 3: angular_states = x[:, :, state_indices] # (B, N, 2) max_per_state = np.max(np.abs(angular_states), axis=1) # (B, 2) max_overshoot = np.max(max_per_state, axis=1) # (B,) return np.mean(max_overshoot) else: angular_states = x[:, state_indices] return np.max(np.abs(angular_states)) def compute_settling_time(t: np.ndarray, x: np.ndarray, tolerance: float = 0.02) -> float: """Compute settling time to within tolerance of equilibrium. Settling time: minimum t where ||x(τ)|| < tolerance ∀ τ > t Parameters ---------- t : np.ndarray Time vector x : np.ndarray State trajectories tolerance : float Settling tolerance (default: 0.02 = 2% of initial error) Returns ------- float Settling time [seconds], or np.inf if never settles """ if x.ndim == 3: errors = np.linalg.norm(x, axis=2) # (B, N) settling_times = [] for batch_errors in errors: settled_mask = batch_errors < tolerance if np.any(settled_mask): # Find first index where it stays below tolerance for i in range(len(settled_mask)): if np.all(settled_mask[i:]): settling_times.append(t[i]) break else: settling_times.append(np.inf) else: settling_times.append(np.inf) return np.mean(settling_times) else: errors = np.linalg.norm(x, axis=1) settled_mask = errors < tolerance if np.any(settled_mask): for i in range(len(settled_mask)): if np.all(settled_mask[i:]): return t[i] return np.inf def compute_damping_ratio(x: np.ndarray, t: np.ndarray) -> float: """Estimate damping ratio from oscillatory response. Uses logarithmic decrement method: ζ = ln(x_peak1 / x_peak2) / √(4π² + ln²(x_peak1 / x_peak2)) Parameters ---------- x : np.ndarray State response (typically angular states) t : np.ndarray Time vector Returns ------- float Estimated damping ratio (0 = undamped, 1 = critically damped) """ from scipy.signal import find_peaks # Find peaks in response peaks, _ = find_peaks(np.abs(x)) if len(peaks) < 2: return 1.0 # No oscillation = critically damped or overdamped # Use first two peaks for logarithmic decrement x1 = np.abs(x[peaks[0]]) x2 = np.abs(x[peaks[1]]) if x2 == 0: return 1.0 delta = np.log(x1 / x2) # Logarithmic decrement zeta = delta / np.sqrt(4 * np.pi**2 + delta**2) return min(zeta, 1.0) # Cap at 1.0
``` #### 2.2.3 Constraint Metrics ```python
# example-metadata:
# runnable: false # src/benchmarks/metrics/constraint_metrics.py def count_control_violations(u: np.ndarray, max_force: float) -> int: """Count control saturation violations. Violations = |{t : |u(t)| > u_max}| Zero violations required for safe operation. Parameters ---------- u : np.ndarray Control history max_force : float Maximum allowable control force Returns ------- int Number of timesteps exceeding limit """ if u.ndim == 2: violations_per_batch = np.sum(np.abs(u) > max_force, axis=1) return int(np.mean(violations_per_batch)) else: return int(np.sum(np.abs(u) > max_force)) def compute_violation_severity(u: np.ndarray, max_force: float) -> float: """Compute severity of constraint violations. Severity = (1/N) Σ max(0, |u(t)| - u_max) Quantifies how far violations exceed limits. Parameters ---------- u : np.ndarray Control history max_force : float Maximum allowable control force Returns ------- float Average violation severity [Force] """ if u.ndim == 2: excess = np.maximum(0, np.abs(u) - max_force) return np.mean(np.mean(excess, axis=1)) else: excess = np.maximum(0, np.abs(u) - max_force) return np.mean(excess)
``` #### 2.2.4 Unified Metrics Interface ```python
# src/benchmarks/metrics/__init__.py from .control_metrics import compute_ise, compute_itae, compute_rms_control, compute_control_rate
from .stability_metrics import compute_overshoot, compute_settling_time, compute_damping_ratio
from .constraint_metrics import count_control_violations, compute_violation_severity __all__ = [ 'compute_all_metrics', 'compute_basic_metrics', 'compute_ise', 'compute_itae', 'compute_rms_control', 'compute_overshoot', 'count_control_violations'
] def compute_all_metrics(t: np.ndarray, x: np.ndarray, u: np.ndarray, max_force: float, include_advanced: bool = False) -> dict: """Compute all performance metrics for a simulation result. Parameters ---------- t : np.ndarray Time vector x : np.ndarray State trajectories u : np.ndarray Control history max_force : float Maximum control force limit include_advanced : bool Include advanced metrics (settling time, damping ratio) Returns ------- dict Dictionary of all computed metrics Examples -------- >>> metrics = compute_all_metrics(t, x, u, max_force=100.0) >>> print(f"ISE: {metrics['ise']:.4f}") >>> print(f"Violations: {metrics['violations']}") """ metrics = { 'ise': compute_ise(t, x), 'itae': compute_itae(t, x), 'rms_control': compute_rms_control(t, u), 'control_rate': compute_control_rate(t, u), 'overshoot': compute_overshoot(x), 'violations': count_control_violations(u, max_force), 'violation_severity': compute_violation_severity(u, max_force) } if include_advanced: metrics['settling_time'] = compute_settling_time(t, x) metrics['damping_ratio'] = compute_damping_ratio(x[:, 1], t) # θ1 response return metrics def compute_basic_metrics(t: np.ndarray, x: np.ndarray, u: np.ndarray) -> dict: """Compute basic performance metrics (no max_force required). Minimal metric set for quick analysis. """ return { 'ise': compute_ise(t, x), 'rms_control': compute_rms_control(t, u), 'overshoot': compute_overshoot(x) }
``` ### 2.3 Core Trial Execution (`src/benchmarks/core/`) ```python
# example-metadata:
# runnable: false # src/benchmarks/core/trial_runner.py def run_multiple_trials(controller_factory: Callable, cfg: Any, n_trials: int = 30, seed: Optional[int] = None, progress_callback: Optional[Callable] = None) -> List[dict]: """Execute multiple simulation trials with different random seeds. Implements Central Limit Theorem requirements (n ≥ 30) for statistical validity. Parameters ---------- controller_factory : callable Function that returns a fresh controller instance cfg : object Configuration object with simulation parameters n_trials : int Number of trials (default: 30 for CLT compliance) seed : int, optional Base random seed for reproducibility progress_callback : callable, optional Callback function(current, total) for progress tracking Returns ------- list of dict List of metric dictionaries, one per trial Examples -------- >>> def create_controller(): ... return ClassicalSMC(gains=[10, 8, 15, 12, 50, 5], max_force=100) ... >>> metrics_list = run_multiple_trials( ... create_controller, config, n_trials=30, seed=42, ... progress_callback=lambda i, n: print(f"Trial {i}/{n}") ... ) >>> ise_values = [m['ise'] for m in metrics_list] >>> mean_ise = np.mean(ise_values) """ if seed is not None: rng = np.random.default_rng(seed) else: rng = np.random.default_rng() # Generate independent seeds for each trial trial_seeds = rng.integers(0, 2**31-1, size=n_trials) metrics_list = [] for i, trial_seed in enumerate(trial_seeds): # Set seed for this trial np.random.seed(trial_seed) # Create fresh controller controller = controller_factory() # Run simulation from src.core.simulation_runner import run_simulation result = run_simulation( controller=controller, duration=cfg.simulation.duration, dt=cfg.simulation.dt, initial_state=cfg.simulation.initial_state ) # Compute metrics from src.benchmarks.metrics import compute_all_metrics metrics = compute_all_metrics( result['time'], np.array(result['states']), np.array(result['controls']), max_force=cfg.controllers.max_force ) metrics_list.append(metrics) # Progress callback if progress_callback: progress_callback(i + 1, n_trials) return metrics_list
``` ### 2.4 Statistical Analysis (`src/benchmarks/statistics/`) ```python
# example-metadata:
# runnable: false # src/benchmarks/statistics/confidence_intervals.py def compute_t_confidence_intervals(metrics_list: List[dict], confidence_level: float = 0.95) -> dict: """Compute t-distribution confidence intervals for all metrics. Uses Student's t-distribution for small sample sizes (n < 100). Parameters ---------- metrics_list : list of dict Metrics from multiple trials confidence_level : float Confidence level (default: 0.95 for 95% CI) Returns ------- dict For each metric: {'mean': float, 'ci_width': float, 'ci_lower': float, 'ci_upper': float} Examples -------- >>> ci_results = compute_t_confidence_intervals(metrics_list) >>> ise_mean = ci_results['ise']['mean'] >>> ise_ci = ci_results['ise']['ci_width'] >>> print(f"ISE: {ise_mean:.4f} ± {ise_ci:.4f}") """ from scipy import stats # Extract metric names metric_names = list(metrics_list[0].keys()) ci_results = {} for metric in metric_names: values = np.array([m[metric] for m in metrics_list]) n = len(values) mean = np.mean(values) std = np.std(values, ddof=1) # Bessel's correction sem = std / np.sqrt(n) # Standard error of mean # t-critical value alpha = 1 - confidence_level t_crit = stats.t.ppf(1 - alpha/2, df=n-1) ci_width = t_crit * sem ci_lower = mean - ci_width ci_upper = mean + ci_width ci_results[metric] = { 'mean': float(mean), 'std': float(std), 'ci_width': float(ci_width), 'ci_lower': float(ci_lower), 'ci_upper': float(ci_upper), 'n': n } return ci_results def compute_bootstrap_confidence_intervals(metrics_list: List[dict], confidence_level: float = 0.95, n_bootstrap: int = 10000) -> dict: """Compute bootstrap confidence intervals (non-parametric). Useful when metric distributions are non-normal. Parameters ---------- metrics_list : list of dict Metrics from multiple trials confidence_level : float Confidence level n_bootstrap : int Number of bootstrap samples Returns ------- dict Bootstrap CI results for each metric """ metric_names = list(metrics_list[0].keys()) ci_results = {} for metric in metric_names: values = np.array([m[metric] for m in metrics_list]) # Bootstrap resampling bootstrap_means = [] for _ in range(n_bootstrap): resample = np.random.choice(values, size=len(values), replace=True) bootstrap_means.append(np.mean(resample)) bootstrap_means = np.array(bootstrap_means) # Percentile method alpha = 1 - confidence_level ci_lower = np.percentile(bootstrap_means, 100 * alpha/2) ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha/2)) mean = np.mean(values) ci_width = (ci_upper - ci_lower) / 2 ci_results[metric] = { 'mean': float(mean), 'ci_width': float(ci_width), 'ci_lower': float(ci_lower), 'ci_upper': float(ci_upper), 'method': 'bootstrap' } return ci_results def compare_controllers(metrics_a: List[dict], metrics_b: List[dict], metric_name: str = 'ise', alpha: float = 0.05) -> dict: """Compare two controllers using Welch's t-test. Welch's t-test handles unequal variances between groups. Parameters ---------- metrics_a : list of dict Metrics from controller A metrics_b : list of dict Metrics from controller B metric_name : str Metric to compare (e.g., 'ise') alpha : float Significance level (default: 0.05) Returns ------- dict Test results: { 't_statistic': float, 'p_value': float, 'significant': bool, 'mean_a': float, 'mean_b': float, 'better_controller': str } Examples -------- >>> comparison = compare_controllers(classical_metrics, adaptive_metrics, 'ise') >>> if comparison['significant']: ... print(f"{comparison['better_controller']} is significantly better (p={comparison['p_value']:.4f})") """ from scipy import stats values_a = np.array([m[metric_name] for m in metrics_a]) values_b = np.array([m[metric_name] for m in metrics_b]) # Welch's t-test (unequal variances) t_stat, p_value = stats.ttest_ind(values_a, values_b, equal_var=False) mean_a = np.mean(values_a) mean_b = np.mean(values_b) significant = p_value < alpha better_controller = 'A' if mean_a < mean_b else 'B' return { 't_statistic': float(t_stat), 'p_value': float(p_value), 'significant': bool(significant), 'mean_a': float(mean_a), 'mean_b': float(mean_b), 'better_controller': better_controller, 'effect_size': float(abs(mean_a - mean_b) / np.sqrt((np.var(values_a) + np.var(values_b)) / 2)) }
``` ### 2.5 Main Benchmarking Interface ```python
# example-metadata:
# runnable: false # src/benchmarks/statistical_benchmarks_v2.py def run_trials(controller_factory: Callable, cfg: Any, n_trials: int = 30, seed: Optional[int] = None, randomise_physics: bool = False, noise_std: float = 0.0) -> Tuple[List[dict], dict]: """Run statistical benchmark trials with confidence interval computation. Backward-compatible interface with enhanced capabilities. Parameters ---------- controller_factory : callable Function returning fresh controller instance cfg : object Configuration object n_trials : int Number of trials (≥30 for CLT) seed : int, optional Random seed for reproducibility randomise_physics : bool Add physics parameter uncertainty noise_std : float Sensor noise standard deviation Returns ------- metrics_list : list of dict Raw metrics from all trials ci_results : dict Confidence interval results for each metric Examples -------- >>> metrics_list, ci_results = run_trials( ... controller_factory=lambda: ClassicalSMC(gains=[10,8,15,12,50,5], max_force=100), ... cfg=config, ... n_trials=30, ... seed=42 ... ) >>> for metric, stats in ci_results.items(): ... print(f"{metric}: {stats['mean']:.4f} ± {stats['ci_width']:.4f}") """ from .core import run_multiple_trials from .statistics import compute_t_confidence_intervals # Execute trials metrics_list = run_multiple_trials( controller_factory, cfg, n_trials=n_trials, seed=seed ) # Compute confidence intervals ci_results = compute_t_confidence_intervals(metrics_list) return metrics_list, ci_results def run_trials_with_advanced_statistics(controller_factory: Callable, cfg: Any, n_trials: int = 30, confidence_level: float = 0.95, use_bootstrap: bool = False, **kwargs) -> Tuple[List[dict], dict]: """Run trials with advanced statistical analysis. Enhanced version with non-parametric options. Parameters ---------- use_bootstrap : bool Use bootstrap CI instead of t-distribution Returns ------- metrics_list : list of dict analysis : dict Extended analysis including distribution tests """ from .core import run_multiple_trials from .statistics import ( compute_t_confidence_intervals, compute_bootstrap_confidence_intervals ) metrics_list = run_multiple_trials(controller_factory, cfg, n_trials, **kwargs) if use_bootstrap: ci_results = compute_bootstrap_confidence_intervals(metrics_list, confidence_level) else: ci_results = compute_t_confidence_intervals(metrics_list, confidence_level) return metrics_list, ci_results def compare_controllers(controller_a_factory: Callable, controller_b_factory: Callable, cfg: Any, n_trials: int = 30, metric: str = 'ise') -> dict: """Compare two controllers statistically. Parameters ---------- controller_a_factory : callable controller_b_factory : callable cfg : object n_trials : int metric : str Metric for comparison Returns ------- dict Comparison results with statistical significance """ from .core import run_multiple_trials from .statistics import compare_controllers as compare_fn metrics_a = run_multiple_trials(controller_a_factory, cfg, n_trials) metrics_b = run_multiple_trials(controller_b_factory, cfg, n_trials) return compare_fn(metrics_a, metrics_b, metric)
``` --- ## 3. Integration Benchmarking Framework ### 3.1 Numerical Integration Methods ```python
# benchmarks/integration/numerical_methods.py class EulerIntegrator: """Forward Euler integration method. First-order accurate: O(h) """ def __init__(self, dynamics): self.dynamics = dynamics def integrate(self, x0: np.ndarray, t_span: tuple, dt: float, controller: Optional[Any] = None) -> dict: """Integrate dynamics using Forward Euler. Parameters ---------- x0 : np.ndarray Initial state t_span : tuple (t_start, t_end) dt : float Time step controller : object, optional Controller for closed-loop simulation Returns ------- dict { 't': time vector, 'x': state history, 'u': control history (if controller provided) } """ t_start, t_end = t_span t = np.arange(t_start, t_end + dt, dt) n_steps = len(t) x = np.zeros((n_steps, len(x0))) x[0] = x0 if controller: u = np.zeros(n_steps - 1) for i in range(n_steps - 1): if controller: result = controller.compute_control(x[i], {}, {}) u[i] = result.get('control_output', result.get('control', 0.0)) x_dot = self.dynamics.dynamics(x[i], u[i]) else: x_dot = self.dynamics.dynamics(x[i], 0.0) x[i+1] = x[i] + dt * x_dot result = {'t': t, 'x': x} if controller: result['u'] = u return result class RK4Integrator: """Fourth-order Runge-Kutta integration. Fourth-order accurate: O(h⁴) """ def __init__(self, dynamics): self.dynamics = dynamics def integrate(self, x0: np.ndarray, t_span: tuple, dt: float, controller: Optional[Any] = None) -> dict: """Integrate using RK4 method.""" t_start, t_end = t_span t = np.arange(t_start, t_end + dt, dt) n_steps = len(t) x = np.zeros((n_steps, len(x0))) x[0] = x0 if controller: u = np.zeros(n_steps - 1) for i in range(n_steps - 1): if controller: result = controller.compute_control(x[i], {}, {}) u[i] = result.get('control_output', result.get('control', 0.0)) u_current = u[i] else: u_current = 0.0 # RK4 stages k1 = self.dynamics.dynamics(x[i], u_current) k2 = self.dynamics.dynamics(x[i] + 0.5*dt*k1, u_current) k3 = self.dynamics.dynamics(x[i] + 0.5*dt*k2, u_current) k4 = self.dynamics.dynamics(x[i] + dt*k3, u_current) x[i+1] = x[i] + (dt/6) * (k1 + 2*k2 + 2*k3 + k4) result = {'t': t, 'x': x} if controller: result['u'] = u return result class AdaptiveRK45Integrator: """Adaptive Runge-Kutta 4-5 method (Dormand-Prince). Variable step size for error control. """ def __init__(self, dynamics): self.dynamics = dynamics def integrate(self, x0: np.ndarray, t_span: tuple, rtol: float = 1e-6, atol: float = 1e-9, controller: Optional[Any] = None) -> dict: """Integrate using adaptive RK45.""" from scipy.integrate import solve_ivp if controller: def dynamics_func(t, x): result = controller.compute_control(x, {}, {}) u = result.get('control_output', result.get('control', 0.0)) return self.dynamics.dynamics(x, u) else: def dynamics_func(t, x): return self.dynamics.dynamics(x, 0.0) sol = solve_ivp( dynamics_func, t_span, x0, method='RK45', rtol=rtol, atol=atol ) return { 't': sol.t, 'x': sol.y.T }
``` ### 3.2 Accuracy Analysis ```python
# example-metadata:
# runnable: false # benchmarks/analysis/accuracy_metrics.py def compute_energy_conservation(t: np.ndarray, x: np.ndarray, physics_params: dict) -> dict: """Analyze energy conservation for Hamiltonian systems. Parameters ---------- t : np.ndarray Time vector x : np.ndarray State trajectories physics_params : dict Physics parameters Returns ------- dict { 'initial_energy': float, 'final_energy': float, 'max_drift': float, 'relative_drift': float } """ def compute_energy(state): # Kinetic energy x_dot, theta1_dot, theta2_dot = state[3], state[4], state[5] KE = 0.5 * physics_params['M'] * x_dot**2 # Cart # ... (pendulum kinetic energy) # Potential energy theta1, theta2 = state[1], state[2] PE = physics_params['m1'] * physics_params['g'] * physics_params['L1'] * (1 - np.cos(theta1)) # ... (second pendulum PE) return KE + PE energies = np.array([compute_energy(state) for state in x]) initial_energy = energies[0] final_energy = energies[-1] max_drift = np.max(np.abs(energies - initial_energy)) relative_drift = max_drift / initial_energy if initial_energy != 0 else np.inf return { 'initial_energy': float(initial_energy), 'final_energy': float(final_energy), 'max_drift': float(max_drift), 'relative_drift': float(relative_drift), 'energies': energies } def estimate_convergence_order(integrator, x0: np.ndarray, t_span: tuple, dt_values: List[float]) -> dict: """Estimate numerical convergence order. Uses Richardson extrapolation to estimate p in: e_h = C·h^p Parameters ---------- integrator : object Integration method instance x0 : np.ndarray Initial state t_span : tuple Time span dt_values : list of float Decreasing time steps for convergence analysis Returns ------- dict { 'convergence_order': float, 'errors': list of float, 'dt_values': list of float } """ # Get reference solution (finest dt) ref_dt = min(dt_values) / 4 ref_result = integrator.integrate(x0, t_span, ref_dt) ref_x_final = ref_result['x'][-1] errors = [] for dt in dt_values: result = integrator.integrate(x0, t_span, dt) x_final = result['x'][-1] error = np.linalg.norm(x_final - ref_x_final) errors.append(error) # Estimate convergence order: p = log(e_h1/e_h2) / log(h1/h2) orders = [] for i in range(len(errors) - 1): if errors[i+1] > 0: order = np.log(errors[i] / errors[i+1]) / np.log(dt_values[i] / dt_values[i+1]) orders.append(order) avg_order = np.mean(orders) if orders else np.nan return { 'convergence_order': float(avg_order), 'errors': [float(e) for e in errors], 'dt_values': dt_values }
``` ### 3.3 Method Comparison Framework ```python
# benchmarks/comparison/method_comparison.py class IntegrationMethodComparator: """Systematic comparison of integration methods.""" def __init__(self, dynamics, physics_params: dict): self.dynamics = dynamics self.physics_params = physics_params def compare_methods(self, methods: List[str], x0: np.ndarray, t_span: tuple, dt_values: List[float]) -> dict: """Compare multiple integration methods. Parameters ---------- methods : list of str Method names: ['Euler', 'RK4', 'RK45'] x0 : np.ndarray Initial state t_span : tuple Time span dt_values : list of float Time steps to test Returns ------- dict Comparison results for all methods """ from benchmarks.integration import EulerIntegrator, RK4Integrator, AdaptiveRK45Integrator from benchmarks.analysis import compute_energy_conservation, estimate_convergence_order integrators = { 'Euler': EulerIntegrator(self.dynamics), 'RK4': RK4Integrator(self.dynamics), 'RK45': AdaptiveRK45Integrator(self.dynamics) } results = {} for method_name in methods: integrator = integrators[method_name] # Convergence analysis convergence = estimate_convergence_order(integrator, x0, t_span, dt_values) # Energy conservation (for frictionless system) result = integrator.integrate(x0, t_span, dt=min(dt_values)) energy_analysis = compute_energy_conservation( result['t'], result['x'], self.physics_params ) # Performance measurement import time start = time.time() _ = integrator.integrate(x0, t_span, dt=min(dt_values)) elapsed = time.time() - start results[method_name] = { 'convergence_order': convergence['convergence_order'], 'energy_drift': energy_analysis['relative_drift'], 'computation_time': elapsed, 'errors': convergence['errors'] } return results
``` --- ## 4. Performance Benchmarking ### 4.1 pytest-benchmark Integration ```python
# tests/test_benchmarks/performance/test_controller_benchmarks.py import pytest
from src.controllers.smc.classic_smc import ClassicalSMC class TestControllerPerformance: """Performance benchmarks for controller computations.""" @pytest.mark.benchmark(group="control_computation") def test_classical_smc_performance(self, benchmark): """Benchmark classical SMC control computation speed.""" controller = ClassicalSMC( gains=[10, 8, 15, 12, 50, 5], max_force=100, boundary_layer=0.01 ) state = np.array([0.1, 0.05, 0.08, 0.02, 0.03, 0.01]) result = benchmark(controller.compute_control, state, {}, {}) # Performance criteria stats = benchmark.stats assert stats['mean'] < 1e-3 # <1ms average @pytest.mark.benchmark(group="simulation") def test_full_simulation_performance(self, benchmark): """Benchmark end-to-end simulation performance.""" from src.core.simulation_runner import run_simulation controller = ClassicalSMC( gains=[10, 8, 15, 12, 50, 5], max_force=100, boundary_layer=0.01 ) def simulate(): return run_simulation( controller=controller, duration=1.0, dt=0.01, initial_state=[0.1, 0.1, 0, 0, 0, 0] ) result = benchmark(simulate) # Throughput criteria (100 steps in <100ms) assert benchmark.stats['mean'] < 0.1
``` ### 4.2 Regression Detection ```bash
# Run benchmarks and save baseline
pytest tests/test_benchmarks/ --benchmark-only --benchmark-save=baseline # Compare against baseline
pytest tests/test_benchmarks/ --benchmark-only --benchmark-compare=baseline # Fail if regression >5%
pytest tests/test_benchmarks/ --benchmark-only --benchmark-compare=baseline --benchmark-compare-fail=mean:5%
``` --- ## 5. Benchmark Configuration ### 5.1 Physics Uncertainty Scenarios ```yaml
# config.yaml - Benchmark configuration benchmarks: statistical: n_trials: 30 # CLT compliance confidence_level: 0.95 # 95% CI seed: 42 # Reproducibility physics_uncertainty: enabled: true n_scenarios: 10 variations: cart_mass: 0.05 # ±5% pendulum1_mass: 0.10 # ±10% pendulum2_mass: 0.10 # ±10% pendulum1_length: 0.05 # ±5% pendulum2_length: 0.05 # ±5% friction_cart: 0.20 # ±20% sensor_noise: enabled: false std_position: 0.001 # 1mm std_angle: 0.001 # ~0.06° integration: methods: ['Euler', 'RK4', 'RK45'] dt_values: [0.05, 0.01, 0.005, 0.001] convergence_analysis: true energy_conservation_check: true performance: regression_threshold: 0.05 # 5% slowdown alert baseline_file: 'benchmarks/baseline.json'
``` --- ## 6. Benchmark Execution ### 6.1 Statistical Benchmarking Examples ```python
# Example 1: Basic controller benchmarking
from src.benchmarks import run_trials
from src.controllers.smc.classic_smc import ClassicalSMC metrics_list, ci_results = run_trials( controller_factory=lambda: ClassicalSMC( gains=[10, 8, 15, 12, 50, 5], max_force=100, boundary_layer=0.01 ), cfg=config, n_trials=30, seed=42
) for metric, stats in ci_results.items(): print(f"{metric}: {stats['mean']:.4f} ± {stats['ci_width']:.4f}") # Example 2: Controller comparison
from src.benchmarks import compare_controllers comparison = compare_controllers( controller_a_factory=lambda: ClassicalSMC(gains=[10,8,15,12,50,5], max_force=100), controller_b_factory=lambda: AdaptiveSMC(gains=[10,8,15,12,0.5], max_force=100), cfg=config, n_trials=30, metric='ise'
) if comparison['significant']: print(f"Controller {comparison['better_controller']} is significantly better (p={comparison['p_value']:.4f})")
``` ### 6.2 Integration Benchmarking Examples ```python
# Example 1: Method comparison
from benchmarks import IntegrationBenchmark benchmark = IntegrationBenchmark()
results = benchmark.comprehensive_comparison() for method, metrics in results.items(): print(f"{method}:") print(f" Convergence order: {metrics['convergence_order']:.2f}") print(f" Energy drift: {metrics['energy_drift']:.2e}") print(f" Computation time: {metrics['computation_time']:.3f}s") # Example 2: Energy conservation validation
conservation = benchmark.validate_conservation_laws()
print(f"Max energy drift: {conservation['max_drift']:.2e}")
``` --- ## Summary The benchmarking framework provides: 1. **Statistical Rigor**: CLT-compliant sampling, 95% CI, hypothesis testing
2. **Modular Architecture**: Separate concerns (metrics, statistics, execution)
3. **Multiple Domains**: Statistical + Integration + Performance benchmarking
4. **Reproducibility**: Deterministic seeding, version-controlled configurations
5. **Automation**: pytest integration, CI/CD pipelines, regression detection **Next**: [Validation Methodology Guide](validation_methodology_guide.md)