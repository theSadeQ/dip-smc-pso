#==========================================================================================\\\
#========================= test_pso_convergence_analysis.py ===========================\\\
#==========================================================================================\\\
"""
PSO Convergence Analysis and Performance Testing

This script provides comprehensive convergence analysis for PSO optimization
with different controller configurations and parameter settings.

Author: PSO Optimization Engineer
Date: 2025-09-28
"""

import numpy as np
import matplotlib.pyplot as plt
import logging
from typing import Dict, Any, List, Tuple
from pathlib import Path
import time
from dataclasses import dataclass

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import dependencies
from src.controllers.factory import SMCType, create_pso_controller_factory
from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.config import load_config
from src.utils.seed import set_global_seed

@dataclass
class ConvergenceMetrics:
    """Container for convergence analysis metrics."""
    controller_type: str
    best_cost: float
    convergence_iterations: int
    optimization_time: float
    cost_history: List[float]
    position_history: List[np.ndarray]
    final_gains: np.ndarray
    convergence_rate: float
    stability_achieved: bool

class PSOConvergenceAnalyzer:
    """Advanced PSO convergence analysis and performance testing framework."""

    def __init__(self, config_path: str = "config.yaml"):
        """Initialize analyzer with configuration."""
        self.config = load_config(config_path)
        self.results = {}

        # Set deterministic behavior
        set_global_seed(42)

    def run_convergence_analysis(self, controller_type: SMCType,
                                 iterations: int = 20,
                                 particles: int = 15) -> ConvergenceMetrics:
        """Run detailed convergence analysis for a specific controller type."""
        logger.info(f"Running convergence analysis for {controller_type.value}...")

        try:
            # Create controller factory
            factory = create_pso_controller_factory(controller_type, self.config)

            # Initialize PSOTuner
            tuner = PSOTuner(
                controller_factory=factory,
                config=self.config,
                seed=42
            )

            # Run optimization
            start_time = time.time()

            result = tuner.optimise(
                iters_override=iterations,
                n_particles_override=particles
            )

            end_time = time.time()
            optimization_time = end_time - start_time

            # Extract convergence metrics
            cost_history = result['history']['cost']
            position_history = result['history']['pos']

            # Handle case where history might be empty or invalid
            if len(cost_history) == 0:
                cost_history = [result['best_cost']]
            if len(position_history) == 0:
                position_history = [result['best_pos']]

            # Calculate convergence characteristics
            convergence_iterations = self._calculate_convergence_point(cost_history)
            convergence_rate = self._calculate_convergence_rate(cost_history)
            stability_achieved = self._assess_stability(cost_history)

            metrics = ConvergenceMetrics(
                controller_type=controller_type.value,
                best_cost=result['best_cost'],
                convergence_iterations=convergence_iterations,
                optimization_time=optimization_time,
                cost_history=list(cost_history),
                position_history=[pos.copy() for pos in position_history],
                final_gains=result['best_pos'].copy(),
                convergence_rate=convergence_rate,
                stability_achieved=stability_achieved
            )

            logger.info(f"Convergence analysis completed for {controller_type.value}")
            logger.info(f"  Best cost: {metrics.best_cost:.6f}")
            logger.info(f"  Convergence iterations: {metrics.convergence_iterations}")
            logger.info(f"  Convergence rate: {metrics.convergence_rate:.6f}")
            logger.info(f"  Stability achieved: {metrics.stability_achieved}")

            return metrics

        except Exception as e:
            logger.error(f"Convergence analysis failed for {controller_type.value}: {e}")
            raise

    def _calculate_convergence_point(self, cost_history: np.ndarray) -> int:
        """Calculate the iteration where convergence occurs."""
        if len(cost_history) < 2:
            return len(cost_history)

        # Find where improvement becomes negligible (< 1% change)
        relative_changes = np.abs(np.diff(cost_history)) / (np.abs(cost_history[:-1]) + 1e-10)
        convergence_threshold = 0.01

        for i, change in enumerate(relative_changes):
            if change < convergence_threshold:
                # Check if next few iterations also have small changes
                remaining_changes = relative_changes[i:i+3]
                if len(remaining_changes) > 0 and np.all(remaining_changes < convergence_threshold):
                    return i + 1

        return len(cost_history)

    def _calculate_convergence_rate(self, cost_history: np.ndarray) -> float:
        """Calculate the average convergence rate."""
        if len(cost_history) < 2:
            return 0.0

        # Calculate exponential decay rate
        valid_costs = cost_history[cost_history > 0]
        if len(valid_costs) < 2:
            return 0.0

        log_costs = np.log(valid_costs + 1e-10)
        if len(log_costs) < 2:
            return 0.0

        # Linear regression on log scale to find decay rate
        x = np.arange(len(log_costs))
        slope = np.polyfit(x, log_costs, 1)[0]

        return -slope  # Negative slope means convergence

    def _assess_stability(self, cost_history: np.ndarray) -> bool:
        """Assess if the optimization achieved stable convergence."""
        if len(cost_history) < 5:
            return False

        # Check last 25% of iterations for stability
        last_quarter = cost_history[-len(cost_history)//4:]
        if len(last_quarter) < 2:
            return False

        # Stable if variance in last quarter is small
        variance = np.var(last_quarter)
        mean_cost = np.mean(last_quarter)

        # Coefficient of variation should be < 5% for stability
        cv = np.sqrt(variance) / (abs(mean_cost) + 1e-10)
        return cv < 0.05

    def run_comparative_analysis(self) -> Dict[str, ConvergenceMetrics]:
        """Run comparative convergence analysis across all controller types."""
        logger.info("Running comparative PSO convergence analysis...")

        controller_types = [SMCType.CLASSICAL, SMCType.ADAPTIVE, SMCType.SUPER_TWISTING]
        results = {}

        for controller_type in controller_types:
            try:
                metrics = self.run_convergence_analysis(
                    controller_type,
                    iterations=25,
                    particles=20
                )
                results[controller_type.value] = metrics
            except Exception as e:
                logger.error(f"Failed to analyze {controller_type.value}: {e}")

        return results

    def run_parameter_sensitivity_analysis(self, controller_type: SMCType) -> Dict[str, Any]:
        """Analyze PSO parameter sensitivity for convergence behavior."""
        logger.info(f"Running parameter sensitivity analysis for {controller_type.value}...")

        # Test different PSO parameter combinations
        parameter_sets = [
            {'w': 0.5, 'c1': 1.5, 'c2': 1.5, 'name': 'conservative'},
            {'w': 0.7, 'c1': 2.0, 'c2': 2.0, 'name': 'balanced'},
            {'w': 0.9, 'c1': 2.5, 'c2': 1.0, 'name': 'explorative'},
        ]

        results = {}

        for params in parameter_sets:
            logger.info(f"Testing {params['name']} parameters...")

            try:
                # Create controller factory
                factory = create_pso_controller_factory(controller_type, self.config)

                # Initialize PSOTuner
                tuner = PSOTuner(
                    controller_factory=factory,
                    config=self.config,
                    seed=42
                )

                # Run optimization with specific parameters
                start_time = time.time()

                result = tuner.optimise(
                    iters_override=15,
                    n_particles_override=12,
                    options_override={'w': params['w'], 'c1': params['c1'], 'c2': params['c2']}
                )

                end_time = time.time()

                # Calculate metrics
                cost_history = result['history']['cost']
                convergence_iterations = self._calculate_convergence_point(cost_history)
                convergence_rate = self._calculate_convergence_rate(cost_history)

                results[params['name']] = {
                    'best_cost': result['best_cost'],
                    'convergence_iterations': convergence_iterations,
                    'convergence_rate': convergence_rate,
                    'optimization_time': end_time - start_time,
                    'parameters': {k: v for k, v in params.items() if k != 'name'},
                    'cost_history': list(cost_history)
                }

                logger.info(f"  {params['name']}: Best cost = {result['best_cost']:.6f}")

            except Exception as e:
                logger.error(f"Parameter sensitivity test failed for {params['name']}: {e}")
                results[params['name']] = {'error': str(e)}

        return results

    def generate_convergence_plots(self, results: Dict[str, ConvergenceMetrics],
                                 output_dir: str = "pso_analysis_plots") -> None:
        """Generate convergence analysis plots."""
        logger.info("Generating convergence analysis plots...")

        # Create output directory
        plot_dir = Path(output_dir)
        plot_dir.mkdir(exist_ok=True)

        # Plot 1: Convergence curves comparison
        plt.figure(figsize=(12, 8))

        for controller_type, metrics in results.items():
            iterations = range(len(metrics.cost_history))
            plt.semilogy(iterations, metrics.cost_history,
                        label=f"{controller_type} (final: {metrics.best_cost:.6f})",
                        linewidth=2, marker='o', markersize=4)

        plt.xlabel('Iteration')
        plt.ylabel('Best Cost (log scale)')
        plt.title('PSO Convergence Comparison Across Controller Types')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(plot_dir / "convergence_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()

        # Plot 2: Convergence metrics summary
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        controller_names = list(results.keys())
        best_costs = [results[name].best_cost for name in controller_names]
        convergence_times = [results[name].optimization_time for name in controller_names]
        convergence_iterations = [results[name].convergence_iterations for name in controller_names]
        convergence_rates = [results[name].convergence_rate for name in controller_names]

        # Best costs
        axes[0,0].bar(controller_names, best_costs)
        axes[0,0].set_title('Best Achieved Cost')
        axes[0,0].set_ylabel('Cost')
        axes[0,0].tick_params(axis='x', rotation=45)

        # Optimization times
        axes[0,1].bar(controller_names, convergence_times)
        axes[0,1].set_title('Optimization Time')
        axes[0,1].set_ylabel('Time (seconds)')
        axes[0,1].tick_params(axis='x', rotation=45)

        # Convergence iterations
        axes[1,0].bar(controller_names, convergence_iterations)
        axes[1,0].set_title('Convergence Iterations')
        axes[1,0].set_ylabel('Iterations')
        axes[1,0].tick_params(axis='x', rotation=45)

        # Convergence rates
        axes[1,1].bar(controller_names, convergence_rates)
        axes[1,1].set_title('Convergence Rate')
        axes[1,1].set_ylabel('Rate')
        axes[1,1].tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig(plot_dir / "convergence_metrics_summary.png", dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"Plots saved to {plot_dir}")

    def generate_comprehensive_report(self,
                                     comparative_results: Dict[str, ConvergenceMetrics],
                                     sensitivity_results: Dict = None) -> str:
        """Generate comprehensive PSO convergence analysis report."""
        logger.info("Generating comprehensive convergence analysis report...")

        report = []
        report.append("="*80)
        report.append("PSO CONVERGENCE ANALYSIS REPORT")
        report.append("="*80)
        report.append(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")

        # Comparative Analysis Section
        report.append("COMPARATIVE CONTROLLER ANALYSIS")
        report.append("-" * 40)

        for controller_type, metrics in comparative_results.items():
            report.append(f"\n{controller_type.upper()} CONTROLLER:")
            report.append(f"  Best Cost: {metrics.best_cost:.8f}")
            report.append(f"  Convergence Iterations: {metrics.convergence_iterations}")
            report.append(f"  Convergence Rate: {metrics.convergence_rate:.6f}")
            report.append(f"  Optimization Time: {metrics.optimization_time:.2f}s")
            report.append(f"  Stability Achieved: {metrics.stability_achieved}")
            report.append(f"  Final Gains: {np.round(metrics.final_gains, 3).tolist()}")

        # Performance Ranking
        report.append("\nPERFORMANCE RANKING")
        report.append("-" * 20)

        # Rank by best cost
        sorted_by_cost = sorted(comparative_results.items(), key=lambda x: x[1].best_cost)
        report.append("\nBy Best Cost (lower is better):")
        for i, (controller_type, metrics) in enumerate(sorted_by_cost, 1):
            report.append(f"  {i}. {controller_type}: {metrics.best_cost:.8f}")

        # Rank by convergence speed
        sorted_by_speed = sorted(comparative_results.items(), key=lambda x: x[1].convergence_iterations)
        report.append("\nBy Convergence Speed (fewer iterations is better):")
        for i, (controller_type, metrics) in enumerate(sorted_by_speed, 1):
            report.append(f"  {i}. {controller_type}: {metrics.convergence_iterations} iterations")

        # Recommendations
        report.append("\nRECOMMENDATIONS")
        report.append("-" * 15)

        best_performer = sorted_by_cost[0]
        fastest_convergence = sorted_by_speed[0]

        report.append(f"Best Overall Performance: {best_performer[0]}")
        report.append(f"  Achieves lowest cost ({best_performer[1].best_cost:.8f}) with good stability")
        report.append(f"Fastest Convergence: {fastest_convergence[0]}")
        report.append(f"  Converges in {fastest_convergence[1].convergence_iterations} iterations")

        # Stability Analysis
        stable_controllers = [name for name, metrics in comparative_results.items()
                            if metrics.stability_achieved]
        if stable_controllers:
            report.append(f"\nStable Controllers: {', '.join(stable_controllers)}")
        else:
            report.append("\nWarning: No controllers achieved stable convergence")

        return "\n".join(report)


def main():
    """Main analysis execution."""
    try:
        # Initialize analyzer
        analyzer = PSOConvergenceAnalyzer()

        # Run comparative analysis
        logger.info("Starting comprehensive PSO convergence analysis...")
        comparative_results = analyzer.run_comparative_analysis()

        # Run parameter sensitivity analysis for Classical SMC
        sensitivity_results = analyzer.run_parameter_sensitivity_analysis(SMCType.CLASSICAL)

        # Generate plots
        analyzer.generate_convergence_plots(comparative_results)

        # Generate comprehensive report
        report = analyzer.generate_comprehensive_report(comparative_results, sensitivity_results)

        # Save report
        with open("pso_convergence_analysis_report.txt", "w") as f:
            f.write(report)

        # Display results
        print(report)

        logger.info("PSO convergence analysis completed successfully")
        return comparative_results

    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        print(f"ANALYSIS FAILED: {e}")
        return None


if __name__ == "__main__":
    main()