#==========================================================================================\\\
#===================== test_pso_convergence_validation.py ============================\\\
#==========================================================================================\\\
"""
PSO Convergence Validation with Corrected Stability Constraints

This script validates PSO convergence behavior with the corrected stability constraints
for GitHub Issue #6. Tests convergence properties, constraint satisfaction, and
optimization performance across all controller types.

Author: PSO Optimization Engineer
Date: 2025-09-28
"""

import numpy as np
import logging
from typing import Dict, Any, List, Tuple
import time
import json

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import dependencies
from src.controllers.factory import create_pso_controller_factory, SMCType
from src.optimization.algorithms.pso_optimizer import PSOTuner
from src.config import load_config
from src.utils.seed import set_global_seed


class PSOConvergenceValidator:
    """PSO convergence validation with corrected stability constraints."""

    def __init__(self, config_path: str = "config.yaml"):
        """Initialize validator with configuration."""
        self.config = load_config(config_path)
        set_global_seed(42)

    def test_pso_convergence_with_constraints(self) -> Dict[str, Any]:
        """Test PSO convergence with corrected stability constraints."""
        logger.info("Testing PSO convergence with corrected stability constraints...")

        results = {
            'test_name': 'PSO Convergence with Corrected Constraints',
            'controllers': {},
            'overall_metrics': {}
        }

        # Test all controller types
        controller_configs = [
            ('classical_smc', SMCType.CLASSICAL, 6),
            ('adaptive_smc', SMCType.ADAPTIVE, 5),
            ('sta_smc', SMCType.SUPER_TWISTING, 6),
            ('hybrid_adaptive_sta_smc', SMCType.HYBRID, 4)
        ]

        all_convergence_results = []
        overall_success = True

        for controller_name, smc_type, expected_gains in controller_configs:
            try:
                logger.info(f"Testing convergence for {controller_name}...")

                # Create PSO factory
                factory = create_pso_controller_factory(smc_type, self.config)

                # Create PSO tuner
                tuner = PSOTuner(
                    controller_factory=factory,
                    config=self.config,
                    seed=42
                )

                # Run PSO optimization with different configurations
                convergence_tests = {}

                # Test 1: Standard configuration
                start_time = time.time()
                standard_result = tuner.optimise(
                    iters_override=20,
                    n_particles_override=15
                )
                standard_time = time.time() - start_time

                convergence_tests['standard'] = {
                    'best_cost': float(standard_result['best_cost']),
                    'best_gains': standard_result['best_pos'].tolist(),
                    'convergence_time': standard_time,
                    'cost_history': standard_result['history']['cost'].tolist()
                }

                # Test 2: Aggressive configuration (more particles, more iterations)
                start_time = time.time()
                aggressive_result = tuner.optimise(
                    iters_override=30,
                    n_particles_override=25
                )
                aggressive_time = time.time() - start_time

                convergence_tests['aggressive'] = {
                    'best_cost': float(aggressive_result['best_cost']),
                    'best_gains': aggressive_result['best_pos'].tolist(),
                    'convergence_time': aggressive_time,
                    'cost_history': aggressive_result['history']['cost'].tolist()
                }

                # Test 3: Conservative configuration (fewer particles, fewer iterations)
                start_time = time.time()
                conservative_result = tuner.optimise(
                    iters_override=10,
                    n_particles_override=8
                )
                conservative_time = time.time() - start_time

                convergence_tests['conservative'] = {
                    'best_cost': float(conservative_result['best_cost']),
                    'best_gains': conservative_result['best_pos'].tolist(),
                    'convergence_time': conservative_time,
                    'cost_history': conservative_result['history']['cost'].tolist()
                }

                # Analyze convergence behavior
                convergence_analysis = self._analyze_convergence_behavior(convergence_tests, controller_name)

                # Validate constraint satisfaction
                constraint_validation = self._validate_constraint_satisfaction(
                    convergence_tests, controller_name, factory
                )

                # Performance metrics
                performance_metrics = self._calculate_performance_metrics(convergence_tests)

                controller_success = (
                    convergence_analysis['overall_success'] and
                    constraint_validation['all_constraints_satisfied'] and
                    performance_metrics['reasonable_performance']
                )

                results['controllers'][controller_name] = {
                    'convergence_tests': convergence_tests,
                    'convergence_analysis': convergence_analysis,
                    'constraint_validation': constraint_validation,
                    'performance_metrics': performance_metrics,
                    'success': controller_success
                }

                all_convergence_results.append({
                    'controller': controller_name,
                    'best_cost': min(test['best_cost'] for test in convergence_tests.values()),
                    'avg_time': np.mean([test['convergence_time'] for test in convergence_tests.values()]),
                    'constraints_satisfied': constraint_validation['all_constraints_satisfied']
                })

                if controller_success:
                    logger.info(f"‚úì {controller_name}: Convergence validation PASSED")
                else:
                    logger.warning(f"‚ö† {controller_name}: Convergence issues detected")
                    overall_success = False

            except Exception as e:
                results['controllers'][controller_name] = {
                    'error': str(e),
                    'success': False
                }
                overall_success = False
                logger.error(f"‚úó {controller_name}: Convergence validation failed - {e}")

        # Calculate overall metrics
        if all_convergence_results:
            results['overall_metrics'] = {
                'average_best_cost': np.mean([r['best_cost'] for r in all_convergence_results]),
                'average_convergence_time': np.mean([r['avg_time'] for r in all_convergence_results]),
                'constraint_satisfaction_rate': np.mean([r['constraints_satisfied'] for r in all_convergence_results]),
                'overall_success_rate': sum(1 for r in results['controllers'].values() if r.get('success', False)) / len(controller_configs)
            }

        results['overall_success'] = overall_success
        return results

    def _analyze_convergence_behavior(self, convergence_tests: Dict, controller_name: str) -> Dict[str, Any]:
        """Analyze convergence behavior patterns."""
        analysis = {
            'convergence_patterns': {},
            'cost_improvements': {},
            'stability_analysis': {},
            'overall_success': True
        }

        for test_name, test_result in convergence_tests.items():
            cost_history = test_result['cost_history']

            # Analyze convergence pattern
            if len(cost_history) >= 2:
                initial_cost = cost_history[0]
                final_cost = cost_history[-1]
                improvement_ratio = (initial_cost - final_cost) / initial_cost if initial_cost > 0 else 0

                # Check for monotonic improvement (generally decreasing cost)
                decreasing_trend = sum(1 for i in range(1, len(cost_history))
                                     if cost_history[i] <= cost_history[i-1])
                monotonicity = decreasing_trend / (len(cost_history) - 1) if len(cost_history) > 1 else 0

                # Convergence detection (plateau in final iterations)
                final_quarter = cost_history[-max(1, len(cost_history)//4):]
                convergence_stability = 1.0 - (np.std(final_quarter) / np.mean(final_quarter)) if np.mean(final_quarter) > 0 else 0

                analysis['convergence_patterns'][test_name] = {
                    'improvement_ratio': improvement_ratio,
                    'monotonicity': monotonicity,
                    'convergence_stability': convergence_stability,
                    'final_cost': final_cost,
                    'converged': convergence_stability > 0.95  # 95% stability threshold
                }

                analysis['cost_improvements'][test_name] = improvement_ratio

                # Check if convergence behavior is reasonable
                if improvement_ratio < 0.01 and final_cost > 100:  # Poor improvement
                    analysis['overall_success'] = False

        return analysis

    def _validate_constraint_satisfaction(self, convergence_tests: Dict, controller_name: str, factory) -> Dict[str, Any]:
        """Validate that optimized gains satisfy controller-specific constraints."""
        validation = {
            'constraint_tests': {},
            'all_constraints_satisfied': True
        }

        for test_name, test_result in convergence_tests.items():
            best_gains = np.array(test_result['best_gains'])
            constraint_results = {}

            # General positivity constraint
            constraint_results['all_positive'] = all(g > 0 for g in best_gains)

            # Controller-specific constraints
            if controller_name == 'sta_smc' and len(best_gains) >= 2:
                # Super-Twisting: K1 > K2 constraint
                K1, K2 = best_gains[0], best_gains[1]
                constraint_results['k1_greater_k2'] = K1 > K2
                constraint_results['k1_k2_ratio'] = K1 / K2 if K2 > 0 else float('inf')

                # Reasonable gain magnitudes
                constraint_results['reasonable_magnitudes'] = K1 < 200 and K2 < 200

            elif controller_name == 'hybrid_adaptive_sta_smc':
                # Hybrid: All surface parameters positive
                c1, lam1, c2, lam2 = best_gains
                constraint_results['surface_gains_positive'] = all(g > 0 for g in [c1, lam1, c2, lam2])
                constraint_results['reasonable_surface_ratios'] = (
                    lam1/c1 < 10 and lam2/c2 < 10 if c1 > 0 and c2 > 0 else False
                )

            elif controller_name == 'adaptive_smc':
                # Adaptive: adaptation rate within reasonable bounds
                if len(best_gains) >= 5:
                    gamma = best_gains[4]
                    constraint_results['adaptation_rate_reasonable'] = 0.1 <= gamma <= 20

            elif controller_name == 'classical_smc':
                # Classical: sliding surface stability
                if len(best_gains) >= 4:
                    k1, k2, lam1, lam2 = best_gains[:4]
                    constraint_results['surface_stability'] = (
                        lam1/k1 < 50 and lam2/k2 < 50 if k1 > 0 and k2 > 0 else False
                    )

            # Check if any constraint failed
            constraints_satisfied = all(constraint_results.values())
            if not constraints_satisfied:
                validation['all_constraints_satisfied'] = False

            validation['constraint_tests'][test_name] = {
                'gains': best_gains.tolist(),
                'constraints': constraint_results,
                'satisfied': constraints_satisfied
            }

        return validation

    def _calculate_performance_metrics(self, convergence_tests: Dict) -> Dict[str, Any]:
        """Calculate performance metrics for PSO optimization."""
        metrics = {
            'timing_analysis': {},
            'cost_analysis': {},
            'efficiency_metrics': {},
            'reasonable_performance': True
        }

        times = [test['convergence_time'] for test in convergence_tests.values()]
        costs = [test['best_cost'] for test in convergence_tests.values()]

        # Timing analysis
        metrics['timing_analysis'] = {
            'mean_time': np.mean(times),
            'std_time': np.std(times),
            'max_time': np.max(times),
            'min_time': np.min(times),
            'reasonable_timing': np.max(times) < 30.0  # Should complete within 30s
        }

        # Cost analysis
        metrics['cost_analysis'] = {
            'mean_cost': np.mean(costs),
            'std_cost': np.std(costs),
            'best_cost': np.min(costs),
            'worst_cost': np.max(costs),
            'cost_consistency': np.std(costs) / np.mean(costs) if np.mean(costs) > 0 else 0
        }

        # Efficiency metrics
        if len(convergence_tests) >= 3:
            # Compare aggressive vs conservative configurations
            aggressive_cost = convergence_tests['aggressive']['best_cost']
            conservative_cost = convergence_tests['conservative']['best_cost']
            aggressive_time = convergence_tests['aggressive']['convergence_time']
            conservative_time = convergence_tests['conservative']['convergence_time']

            metrics['efficiency_metrics'] = {
                'cost_improvement_ratio': (conservative_cost - aggressive_cost) / conservative_cost if conservative_cost > 0 else 0,
                'time_efficiency': aggressive_time / conservative_time if conservative_time > 0 else 1,
                'effort_worthwhile': (aggressive_cost < 0.8 * conservative_cost)  # 20% improvement threshold
            }

        # Overall performance assessment
        metrics['reasonable_performance'] = (
            metrics['timing_analysis']['reasonable_timing'] and
            metrics['cost_analysis']['best_cost'] < 1e6 and  # Not excessive penalty
            np.isfinite(metrics['cost_analysis']['mean_cost'])
        )

        return metrics

    def run_convergence_validation(self) -> Dict[str, Any]:
        """Run complete PSO convergence validation."""
        logger.info("Starting PSO convergence validation with corrected constraints...")

        validation_results = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'validation_type': 'PSO Convergence with Corrected Stability Constraints',
            'results': {}
        }

        # Run convergence tests
        convergence_results = self.test_pso_convergence_with_constraints()
        validation_results['results'] = convergence_results

        # Generate summary
        total_controllers = len(convergence_results['controllers'])
        successful_controllers = sum(1 for controller in convergence_results['controllers'].values()
                                   if controller.get('success', False))
        success_rate = (successful_controllers / total_controllers) * 100

        validation_results['summary'] = {
            'total_controllers_tested': total_controllers,
            'successful_controllers': successful_controllers,
            'success_rate': success_rate,
            'overall_success': convergence_results['overall_success'],
            'status': 'VALIDATED' if convergence_results['overall_success'] else 'ISSUES_DETECTED'
        }

        # Log results
        if convergence_results['overall_success']:
            logger.info(f"‚úì PSO Convergence Validation PASSED: {success_rate:.1f}% success rate")
        else:
            logger.warning(f"‚ö† PSO Convergence Validation: Issues detected ({success_rate:.1f}% success rate)")

        return validation_results


def main():
    """Main convergence validation execution."""
    print("=" * 80)
    print("PSO CONVERGENCE VALIDATION WITH CORRECTED STABILITY CONSTRAINTS")
    print("GitHub Issue #6 - Edge Case Resolution")
    print("=" * 80)

    validator = PSOConvergenceValidator()
    results = validator.run_convergence_validation()

    # Print summary
    summary = results['summary']
    print(f"\nVALIDATION SUMMARY:")
    print(f"Status: {summary['status']}")
    print(f"Success Rate: {summary['success_rate']:.1f}%")
    print(f"Controllers Tested: {summary['successful_controllers']}/{summary['total_controllers_tested']}")

    if summary['overall_success']:
        print("\nüéâ PSO CONVERGENCE VALIDATION: ALL TESTS PASSED")
        print("‚úÖ All controllers demonstrate proper convergence with corrected constraints")
        print("‚úÖ Stability constraints (K1 > K2 for STA-SMC) properly enforced")
        print("‚úÖ Parameter validation working correctly during optimization")
        print("‚úÖ Performance metrics within acceptable ranges")
    else:
        print("\n‚ö†Ô∏è  PSO CONVERGENCE VALIDATION: ISSUES DETECTED")
        print("Please review detailed results for specific convergence issues.")

    # Save results to file
    output_file = "pso_convergence_validation_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nDetailed results saved to: {output_file}")

    return results


if __name__ == "__main__":
    results = main()