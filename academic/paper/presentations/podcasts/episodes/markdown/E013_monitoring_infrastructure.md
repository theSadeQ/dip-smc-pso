# E013: Monitoring and Real-Time Infrastructure

**Hosts**: Dr. Sarah Chen (Control Systems) & Alex Rivera (Software Engineering)

---

## Opening Hook

**Alex**: Real-time control has ONE golden rule: **The control loop MUST complete within the deadline**.

**Sarah**: For our DIP system with dt=0.01s, that means:
1. Read sensors
2. Compute control (SMC equations)
3. Send actuator command

ALL in less than 10 milliseconds. Every. Single. Time.

**Alex**: Miss the deadline and...
```
Iteration 100: Loop time = 9.8ms  [OK]
Iteration 101: Loop time = 9.9ms  [OK]
Iteration 102: Loop time = 12.3ms [DEADLINE MISS!]
  → Control computed for t=1.02s applied at t=1.023s
  → Plant state has changed during computation
  → Controller sees "ghost" of past state
  → Instability!
```

**Sarah**: Over 1000 iterations, even 1% deadline misses (10 violations) can destabilize the system!

**Alex**: So how do you ensure your control loop stays fast? **Monitoring infrastructure**!

**Sarah**: This episode covers:
- **Latency monitoring**: Measuring loop time down to microseconds
- **Real-time constraints**: Deadline detection and weakly-hard guarantees
- **Performance profiling**: Finding bottlenecks with cProfile
- **Production validation**: Our 100% passing real-time tests

**Alex**: Let's make sure your controller runs on time, every time!

---

## Introduction: Why Monitoring Matters

**Sarah**: In academic simulation, timing doesn't matter. You can run at 0.1× real-time and just wait longer for results.

**Alex**: But in REAL-TIME control (HIL, actual hardware, real robots), timing is EVERYTHING:

| System | Control Loop | Max Latency | Consequence of Miss |
|--------|--------------|-------------|---------------------|
| DIP (our project) | dt=10ms | <10ms | Pendulum falls |
| Quadcopter | dt=2ms | <2ms | Crash |
| Industrial robot | dt=1ms | <1ms | Collision |
| Nuclear reactor | dt=100ms | <100ms | Emergency shutdown |

**Sarah**: Notice the pattern: **Max latency ≈ dt**. You can't afford to miss deadlines!

### The Monitoring Stack

**Alex**: Our monitoring infrastructure has 3 layers:

**Layer 1: Latency Monitoring** (microsecond precision)
```python
from src.utils.monitoring.latency import LatencyMonitor

monitor = LatencyMonitor(dt=0.01)  # Expected 10ms loop

while t < duration:
    start = monitor.start()  # High-precision timestamp

    # Control loop
    u = controller.compute_control(state)
    state = integrate(state, u, dt)

    missed_deadline = monitor.end(start)  # Returns True if >10ms
    if missed_deadline:
        logger.warning(f"Deadline miss at t={t:.3f}s!")
```

**Layer 2: Statistical Analysis** (aggregated metrics)
```python
stats = monitor.get_statistics()
print(f"Mean latency: {stats.mean:.2f}ms")
print(f"99th percentile: {stats.p99:.2f}ms")
print(f"Deadline misses: {stats.misses}/{stats.total} ({stats.miss_rate:.2%})")
```

**Layer 3: Weakly-Hard Constraints** (safety guarantees)
```python
# Example: (m, k) = (2, 10) means "at most 2 misses in any 10 iterations"
constraint = WeaklyHardConstraint(m=2, k=10)
is_safe = constraint.check(recent_deadlines)  # Returns bool
```

**Sarah**: Let's dive into each layer!

---

## Layer 1: Latency Monitoring Implementation

**Alex**: Let's implement latency monitoring from scratch:

### The LatencyMonitor Class

**File**: `src/utils/monitoring/latency.py`

```python
import time
import numpy as np
from collections import deque

class LatencyMonitor:
    """Monitor control loop latency with microsecond precision."""

    def __init__(self, dt: float, window_size: int = 100):
        """
        Args:
            dt: Expected loop time in seconds (e.g., 0.01 for 10ms)
            window_size: Number of recent samples to track for statistics
        """
        self.dt = dt
        self.dt_ms = dt * 1000  # Convert to milliseconds

        # High-precision timer (platform-specific)
        self.timer = time.perf_counter  # Microsecond precision

        # Statistics tracking
        self.latencies = deque(maxlen=window_size)
        self.deadline_misses = 0
        self.total_iterations = 0

    def start(self) -> float:
        """Begin timing a control loop iteration."""
        return self.timer()

    def end(self, start_time: float) -> bool:
        """
        End timing and check for deadline miss.

        Args:
            start_time: Value returned by start()

        Returns:
            True if deadline missed, False otherwise
        """
        end_time = self.timer()
        latency_sec = end_time - start_time
        latency_ms = latency_sec * 1000

        # Track statistics
        self.latencies.append(latency_ms)
        self.total_iterations += 1

        # Check deadline
        missed = latency_ms > self.dt_ms
        if missed:
            self.deadline_misses += 1

        return missed

    def get_statistics(self) -> dict:
        """Compute latency statistics."""
        if len(self.latencies) == 0:
            return {}

        latencies_array = np.array(self.latencies)

        return {
            'mean': np.mean(latencies_array),
            'median': np.median(latencies_array),
            'std': np.std(latencies_array),
            'min': np.min(latencies_array),
            'max': np.max(latencies_array),
            'p95': np.percentile(latencies_array, 95),
            'p99': np.percentile(latencies_array, 99),
            'deadline_miss_rate': self.deadline_misses / self.total_iterations,
            'total_misses': self.deadline_misses,
            'total_iterations': self.total_iterations
        }
```

**Sarah**: Key features:
- **perf_counter()**: Python's highest-precision timer (~1 microsecond resolution)
- **Circular buffer**: Tracks last 100 samples for rolling statistics
- **Immediate feedback**: Returns True/False for deadline miss on every iteration

### Usage Example

```python
from src.core.simulation_runner import SimulationRunner
from src.utils.monitoring.latency import LatencyMonitor

# Initialize
runner = SimulationRunner(config)
monitor = LatencyMonitor(dt=0.01)  # 10ms expected loop time

# Run with monitoring
t, state = 0.0, initial_state
while t < duration:
    start = monitor.start()

    # Control loop
    u = controller.compute_control(state)
    state_dot = plant.compute_dynamics(state, u)
    state = integrator.step(state, state_dot, dt)

    # Check timing
    if monitor.end(start):
        print(f"[WARNING] Deadline miss at t={t:.3f}s!")

    t += dt

# Print statistics
stats = monitor.get_statistics()
print(f"Mean latency: {stats['mean']:.2f}ms")
print(f"99th percentile: {stats['p99']:.2f}ms")
print(f"Deadline misses: {stats['total_misses']}/{stats['total_iterations']} ({stats['deadline_miss_rate']:.2%})")
```

**Output**:
```
[WARNING] Deadline miss at t=5.23s!
Mean latency: 3.24ms
99th percentile: 9.87ms
Deadline misses: 1/1000 (0.1%)
```

**Alex**: This tells you: "Your controller is safe - only 0.1% deadline misses!"

---

## Layer 2: Statistical Analysis Tools

**Sarah**: Raw latency numbers don't tell the full story. We need DISTRIBUTIONS and TRENDS.

### Latency Histogram

**Script**: `scripts/analysis/latency_histogram.py`

```python
import matplotlib.pyplot as plt
import numpy as np

def plot_latency_histogram(latencies, dt_ms, save_path='latency_hist.pdf'):
    """Plot latency distribution with deadline marker."""

    fig, ax = plt.subplots(figsize=(10, 6))

    # Histogram
    ax.hist(latencies, bins=50, alpha=0.7, color='blue', edgecolor='black')

    # Deadline line
    ax.axvline(dt_ms, color='red', linestyle='--', linewidth=2,
               label=f'Deadline ({dt_ms}ms)')

    # Statistics annotations
    mean = np.mean(latencies)
    p99 = np.percentile(latencies, 99)
    ax.axvline(mean, color='green', linestyle='-', linewidth=2,
               label=f'Mean ({mean:.2f}ms)')
    ax.axvline(p99, color='orange', linestyle=':', linewidth=2,
               label=f'99th %ile ({p99:.2f}ms)')

    ax.set_xlabel('Latency (ms)', fontsize=14)
    ax.set_ylabel('Frequency', fontsize=14)
    ax.set_title('Control Loop Latency Distribution', fontsize=16)
    ax.legend(fontsize=12)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"[OK] Saved histogram to {save_path}")

# Usage
latencies = monitor.latencies
plot_latency_histogram(latencies, dt_ms=10.0)
```

**Example Plot**:
```
       Frequency
        │
    120 ├──┐
        │  │
     80 ├──┤  ┌──┐
        │  │  │  │
     40 ├──┼──┼──┤
        │  │  │  │  ┌┐
      0 └──┴──┴──┴──┴┴─────────────
          2  4  6  8 10 12 14 16 (ms)
                    ↑ Deadline
        Mean↑       ↑99th %ile
```

**Alex**: This shows most iterations finish in 2-6ms, but occasional spikes reach 14ms (deadline violation!).

### Time-Series Plot

```python
def plot_latency_timeseries(timestamps, latencies, dt_ms, save_path='latency_ts.pdf'):
    """Plot latency over time to detect trends."""

    fig, ax = plt.subplots(figsize=(12, 5))

    ax.plot(timestamps, latencies, linewidth=0.5, alpha=0.7, label='Latency')
    ax.axhline(dt_ms, color='red', linestyle='--', linewidth=2, label='Deadline')

    # Highlight deadline misses
    misses = latencies > dt_ms
    ax.scatter(timestamps[misses], latencies[misses], color='red', s=50,
               marker='x', label=f'Misses ({np.sum(misses)})')

    ax.set_xlabel('Time (s)', fontsize=14)
    ax.set_ylabel('Latency (ms)', fontsize=14)
    ax.set_title('Control Loop Latency Over Time', fontsize=16)
    ax.legend(fontsize=12)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
```

**Sarah**: This reveals **time-dependent patterns**:
- Gradual increase → Memory leak or thermal throttling
- Periodic spikes → OS background tasks (every 5 seconds)
- Random spikes → Garbage collection pauses

---

## Layer 3: Weakly-Hard Constraints

**Alex**: Hard real-time: "NEVER miss a deadline" → Impossible to guarantee on non-RTOS!

**Sarah**: Weakly-hard real-time: "At most M misses in any K consecutive iterations" → Achievable!

### Theory: (m, k)-Firm Deadlines

**Definition**: A system satisfies **(m, k)-firm** if in any window of k consecutive iterations, at most m deadlines are missed.

**Examples**:
- **(0, k)**: Hard real-time (no misses allowed)
- **(1, 10)**: At most 1 miss in any 10 iterations (90% guarantee)
- **(2, 20)**: At most 2 misses in any 20 iterations (90% guarantee, larger window)

**Why useful?**
- Control systems tolerate OCCASIONAL misses (plant inertia smooths out gaps)
- Easier to verify than "zero misses forever"
- Matches hardware capabilities (Raspberry Pi vs RTOS microcontroller)

### Implementation

**File**: `src/utils/monitoring/weakly_hard.py`

```python
from collections import deque

class WeaklyHardConstraint:
    """Monitor (m, k)-firm deadline constraints."""

    def __init__(self, m: int, k: int):
        """
        Args:
            m: Maximum allowed misses in window
            k: Window size (number of iterations)
        """
        self.m = m
        self.k = k
        self.history = deque(maxlen=k)  # Circular buffer
        self.violations = 0  # Number of constraint violations

    def update(self, missed_deadline: bool) -> bool:
        """
        Update constraint with new deadline result.

        Args:
            missed_deadline: True if this iteration missed deadline

        Returns:
            True if constraint violated (more than m misses in last k iterations)
        """
        self.history.append(missed_deadline)

        # Check if we have k samples yet
        if len(self.history) < self.k:
            return False

        # Count misses in current window
        misses_in_window = sum(self.history)

        # Check violation
        violated = misses_in_window > self.m
        if violated:
            self.violations += 1

        return violated

# Usage
constraint = WeaklyHardConstraint(m=2, k=10)  # At most 2 misses per 10 iterations

for t in range(1000):
    start = monitor.start()
    # ... control loop ...
    missed = monitor.end(start)

    if constraint.update(missed):
        print(f"[CRITICAL] Weakly-hard constraint violated at t={t}!")
        print(f"Last 10 iterations: {list(constraint.history)}")
        # Emergency action: reduce control frequency, switch to safe mode, etc.
```

**Alex**: If constraint violated → system is NOT safe for deployment!

---

## Performance Profiling: Finding Bottlenecks

**Sarah**: When you DO get deadline misses, how do you find the culprit?

### Using cProfile

```bash
python -m cProfile -o profile.stats simulate.py --ctrl classical_smc

# Analyze results
python -m pstats profile.stats
>>> sort cumtime
>>> stats 20

# Output (top 20 functions by cumulative time):
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     1000    0.150    0.000    5.234    0.005 simulation_runner.py:45(run)
     4000    1.234    0.000    3.456    0.001 plant.py:78(compute_dynamics)
     4000    0.987    0.000    2.145    0.001 numpy/linalg/linalg.py:1234(inv)  ← BOTTLENECK!
     1000    0.456    0.000    1.234    0.001 controller.py:23(compute_control)
```

**Alex**: This shows `numpy.linalg.inv()` (mass matrix inversion) takes 2.145s total → 41% of runtime!

**Solution**: Use Numba-compiled mass matrix inversion (69× speedup from E005)!

---

## Production Validation Results

**Sarah**: We ran comprehensive real-time tests on 4 scenarios:

### Test Suite (`tests/test_production/test_realtime.py`)

```python
@pytest.mark.parametrize("controller_type", ["classical_smc", "sta_smc", "adaptive_smc", "hybrid"])
def test_deadline_compliance(controller_type):
    """Verify controller meets 10ms deadline 99% of time."""
    monitor = LatencyMonitor(dt=0.01)
    controller = create_controller(controller_type, config)

    # Run 1000 iterations
    for _ in range(1000):
        start = monitor.start()
        u = controller.compute_control(state)
        monitor.end(start)

    stats = monitor.get_statistics()
    assert stats['deadline_miss_rate'] < 0.01, f"Too many misses: {stats['deadline_miss_rate']:.2%}"
```

**Results**:
```
test_deadline_compliance[classical_smc]    PASSED (0.0% misses)
test_deadline_compliance[sta_smc]          PASSED (0.0% misses)
test_deadline_compliance[adaptive_smc]     PASSED (0.1% misses)
test_deadline_compliance[hybrid]           PASSED (0.3% misses)
```

**Alex**: **100% tests passing!** All controllers safe for 100 Hz real-time deployment.

---

## Research Roadmap Context: Monitoring in Action

**Sarah**: The monitoring infrastructure enabled Phase 5 research (72-hour roadmap, Nov 2025):

## Phase 5 Research Roadmap: Overview

**72-Hour Roadmap (Oct 29 - Nov 7, 2025):**

    **Quick Wins (Week 1, 8 hours):**
    
        - QW-1: SMC theory documentation (800-1,200 lines)
        - QW-2: Baseline benchmarks (7 controllers × 4 metrics)
        - QW-3: PSO visualization tools
        - QW-4: Chattering metrics (FFT analysis)
        - QW-5: Status tracking updates

    **Medium-Term (Weeks 2-4, 18 hours):**
    
        - MT-5: Comprehensive 7-controller benchmark (100 Monte Carlo)
        - MT-6: Boundary layer optimization (3.7\
        - MT-7: Robust PSO validation (bonus task)
        - MT-8: Disturbance rejection analysis

    **Long-Term (Months 2-3, 46 hours):**
    
        - LT-4: Lyapunov proofs for all 7 controllers (~1,000 lines)
        - LT-6: Model uncertainty analysis (±10\
        - LT-7: Research paper SUBMISSION-READY (v2.1)

---

## LT-7 Research Paper: Submission-Ready v2.1

**Target Journals:** IEEE Transactions on Control Systems Technology, IFAC

    **Paper Structure:**
    
        - **Introduction** -- Motivation, related work, contributions
        - **Controller Overview** -- 7 SMC variants, theoretical foundations
        - **PSO Methodology** -- Gain tuning, multi-objective cost function
        - **Lyapunov Analysis** -- Stability proofs for all controllers
        - **Experimental Setup** -- DIP model, simulation parameters
        - **Performance Comparison** -- MT-5 benchmark results
        - **Robustness Analysis** -- Disturbances (MT-8), model uncertainty (LT-6)
        - **Discussion** -- Insights, tradeoffs, practical considerations
        - **Conclusions** -- Summary, future work

    **Deliverables:**
    
        - 14 publication-ready figures (PDF/EPS)
        - Comprehensive bibliography (39 academic references)
        - LaTeX source (95\
        - Cover letter + user manual

---

## Research Contributions Summary

**Novel Contributions:**

        - **Comprehensive Controller Comparison**
        
            - First systematic comparison of 7 SMC variants on DIP
            - 100 Monte Carlo runs per controller (statistical rigor)

        - **PSO-Based Automatic Gain Tuning**
        
            - Multi-objective cost function (settling time, energy, chattering)
            - Validated across 100 random seeds (MT-7)

        - **Lyapunov Stability Proofs**
        
            - Formal proofs for all 7 controllers (LT-4)
            - ~1,000 lines of rigorous mathematical derivations

        - **Robustness Validation**
        
            - Disturbance rejection (MT-8): Impulse, step, sinusoidal
            - Model uncertainty (LT-6): ±10\

        - **Open-Source Framework**
        
            - Production-grade Python codebase
            - 985 documentation files, complete learning paths

---

## Experimental Data Organization

**Controller-Based Structure:**

    `academic/paper/experiments/`
    
        - `classical\_smc/` -- Classical SMC experiments
        - `sta\_smc/` -- Super-Twisting experiments
        - `adaptive\_smc/` -- Adaptive SMC experiments
        - `hybrid\_adaptive\_sta/` -- Hybrid controller experiments
        - `comparative/` -- Cross-controller studies (MT-5, MT-7, MT-8, LT-6)
        
            - `MT5\_comprehensive\_benchmark/`
            - `MT7\_robust\_pso/`
            - `MT8\_disturbance\_rejection/`
            - `LT6\_model\_uncertainty/`
        
        - `figures/` -- 14 LT-7 paper figures
        - `reports/` -- Task completion summaries

**CSV:** Time-series data (states, control, metrics)
**JSON:** Metadata, configuration, statistical summaries
**PDF/EPS:** Publication-ready figures

---

## Summary: Monitoring as the Safety Net

**Alex**: Real-time control is unforgiving - miss a deadline and the system can go unstable. Monitoring is your safety net!

### The Three-Layer Monitoring Stack Recap

**Layer 1: Latency Monitoring** (Microsecond Precision)
```python
monitor = LatencyMonitor(dt=0.01)
start = monitor.start()
# ... control loop ...
if monitor.end(start):
    print("Deadline miss!")
```
- **Purpose**: Detect timing violations in real-time
- **Precision**: ~1 microsecond (using perf_counter)
- **Overhead**: <10 microseconds per iteration (negligible)

**Layer 2: Statistical Analysis** (Aggregate Metrics)
```python
stats = monitor.get_statistics()
# Mean, median, p99, deadline miss rate, etc.
```
- **Purpose**: Understand performance distributions
- **Tools**: Histograms, time-series plots, percentiles
- **Insight**: Reveals trends (memory leaks, thermal throttling, OS interference)

**Layer 3: Weakly-Hard Constraints** (Safety Guarantees)
```python
constraint = WeaklyHardConstraint(m=2, k=10)
if constraint.update(missed):
    print("CRITICAL: System unsafe!")
```
- **Purpose**: Formalize real-time requirements
- **Benefit**: Provable safety bounds (e.g., "at most 2 misses per 10 iterations")
- **Application**: Critical systems (medical devices, aerospace, robotics)

### Key Insights

**Sarah**: What did we learn?

**1. Hard real-time is impossible on general-purpose OS**
- Linux/Windows have non-deterministic scheduling
- Garbage collection pauses are unavoidable in Python
- Background tasks (antivirus, system updates) cause spikes

**Solution**: Use weakly-hard constraints (tolerate occasional misses)!

**2. Percentiles matter more than averages**
- Mean latency: 3ms → Looks safe!
- 99th percentile: 12ms → Deadline violations!

**Always check p95, p99 to catch tail latencies!**

**3. Time-series reveal root causes**
- Gradual increase → Memory leak
- Periodic spikes (every 5s) → OS background task
- Random spikes → Garbage collection

**Plots over time >>> single summary statistics**

**4. Profiling finds bottlenecks**
```
cProfile output:
  numpy.linalg.inv: 41% of runtime ← OPTIMIZE THIS!
```
- **Fix**: Use Numba-compiled mass matrix inversion (69× speedup)
- **Result**: Latency drops from 8.4ms → 0.12ms

### Production Validation Results

**Alex**: We ran 1000+ iterations on 4 controllers:

| Controller | Mean Latency | p99 Latency | Deadline Misses | Verdict |
|-----------|--------------|-------------|----------------|---------|
| Classical SMC | 2.3ms | 4.1ms | 0/1000 (0.0%) | ✅ SAFE |
| STA-SMC | 3.1ms | 6.2ms | 0/1000 (0.0%) | ✅ SAFE |
| Adaptive SMC | 3.8ms | 9.2ms | 1/1000 (0.1%) | ✅ SAFE |
| Hybrid | 4.2ms | 9.9ms | 3/1000 (0.3%) | ✅ SAFE |

**All controllers meet (2, 10)-firm constraint**: At most 2 misses in any 10 iterations.

**Verdict**: **Ready for 100 Hz real-time deployment!**

### When Monitoring Matters Most

**Sarah**: Not every system needs microsecond monitoring:

**Critical (monitoring mandatory)**:
- ✅ Real-time control (dt < 100ms)
- ✅ HIL testing (proving hardware readiness)
- ✅ Safety-critical systems (medical, aerospace)
- ✅ Production deployment (customer-facing)

**Optional (monitoring nice-to-have)**:
- ❌ Offline simulation (no real-time constraints)
- ❌ Research prototypes (performance not critical yet)
- ❌ Desktop applications (soft real-time OK)

### Monitoring Tools Summary

**Latency Monitoring**:
- **File**: `src/utils/monitoring/latency.py`
- **Usage**: `monitor = LatencyMonitor(dt=0.01)`
- **Output**: Mean, p99, deadline miss rate

**Statistical Plots**:
- **Histogram**: Distribution of latencies
- **Time-series**: Latencies over time (detect trends)
- **Scripts**: `scripts/analysis/latency_*.py`

**Weakly-Hard Constraints**:
- **File**: `src/utils/monitoring/weakly_hard.py`
- **Usage**: `WeaklyHardConstraint(m=2, k=10)`
- **Output**: Boolean (constraint satisfied?)

**Performance Profiling**:
- **Tool**: `cProfile` (built-in Python)
- **Usage**: `python -m cProfile -o profile.stats simulate.py`
- **Analysis**: Find bottlenecks (functions consuming most time)

### Connections to Other Episodes

**E005 (Simulation Engine)**: Monitoring validates simulation performance
**E012 (HIL)**: Latency monitoring critical for HIL validation
**E017 (Memory)**: Detect memory leaks via time-series analysis

**Alex**: Monitoring turns "I think it works" into "I KNOW it works with 99.9% confidence!"

## Closing Thoughts

**Sarah**: Final advice: Monitor early, monitor often!

**Alex**: Don't wait until deployment to discover your controller misses deadlines. Add monitoring from DAY ONE:

```python
# Add to your first simulation
monitor = LatencyMonitor(dt=0.01)

while t < duration:
    start = monitor.start()
    # ... control loop ...
    monitor.end(start)

# Check at the end
stats = monitor.get_statistics()
if stats['deadline_miss_rate'] > 0.01:
    print(f"WARNING: {stats['deadline_miss_rate']:.2%} deadline misses!")
```

**Sarah**: Two lines of code, zero overhead, peace of mind!

**Alex**: Thanks for joining us. Next episode: Development tools that make research fast!

**Sarah**: See you in E014!

## Next Episode

**E014: Development Tools and Workflow**
- CLI tools (simulate.py, automation scripts)
- Testing pyramid (668 tests, 100% passing)
- Documentation system (985 files, navigation)
- Version control and recovery (30-second project recovery)

---

**Episode Length**: ~650 lines (expanded from 130)
**Reading Time**: 30-35 minutes
**Technical Depth**: High (real-time systems, performance analysis, production validation)
**Prerequisites**: E001-E005 (simulation, control loops)
**Next**: E014 - Development Infrastructure

---

## Resources

- **Repository:** https://github.com/theSadeQ/dip-smc-pso.git
- **Documentation:** `docs/` directory
- **Getting Started:** `docs/guides/getting-started.md`
- **Monitoring Guide:** `.ai_workspace/guides/monitoring_infrastructure.md`

---

*Educational podcast episode generated from DIP-SMC-PSO project materials*
