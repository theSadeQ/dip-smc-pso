% =============================================================================
% Episode 018: Testing Philosophy & Coverage Standards
% Series: DIP-SMC-PSO Professional Toolkit
% Phase: 3 (Professional Practice)
% Duration: ~20 minutes | Pages: 10-12 | Complexity: Professional
% Dependencies: E015 (architectural standards), E017 (multi-agent orchestration)
% =============================================================================

\input{../templates/master_template.tex}

% Episode Metadata
\title{\textbf{E018: Testing Philosophy}}
\def\episodenumber{018}
\def\episodetitle{Testing Philosophy \& Coverage Standards}
\def\episodecategory{Professional Practice}
\def\difficulty{Professional}

\begin{document}
\makeepisodetitle

% =============================================================================
% SECTION 1: OVERVIEW
% =============================================================================
\section{Overview}

\subsection{What You'll Learn}
\begin{itemize}
  \item \textbf{Coverage Tiers}: 85\% overall, 95\% critical, 100\% safety-critical
  \item \textbf{Test Organization}: Peer files (test\_*.py for every *.py)
  \item \textbf{Validation Strategies}: Theoretical properties, edge cases, integration tests
  \item \textbf{Automation}: Pre-commit hooks, CI/CD integration, benchmark regression
\end{itemize}

\subsection{Why This Matters}
\begin{highlightbox}
\textbf{Problem}: Uncaught edge cases (e.g., singular matrices, division by zero) cause silent failures in production.

\textbf{Solution}: Enforce 95\% coverage on critical paths (controllers, dynamics) with theoretical property validation (Lyapunov stability, boundedness).

\textbf{Impact}: Zero critical bugs reported in Phase 5 research (11 tasks, 46 hours) after achieving coverage standards.
\end{highlightbox}

% =============================================================================
% SECTION 2: COVERAGE TIERS
% =============================================================================
\section{Coverage Tiers}

\subsection{Three-Tier System}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Tier} & \textbf{Coverage Target} & \textbf{Examples} \\
\midrule
Safety-Critical & 100\% & SMC control law, saturation \\
Critical Paths & $\geq 95\%$ & Controllers, dynamics, PSO \\
Overall Codebase & $\geq 85\%$ & Utils, visualization, CLI \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Rationale}
\begin{itemize}
  \item \textbf{100\% Safety-Critical}: Code that can cause physical damage (control saturation, state constraints)
  \item \textbf{95\% Critical Paths}: Core algorithms (SMC variants, PSO optimizer, dynamics models)
  \item \textbf{85\% Overall}: Acceptable trade-off (diminishing returns above 85\% for utilities)
\end{itemize}

\subsection{Current Status (January 2025)}
\begin{itemize}
  \item \textbf{Overall Coverage}: 87\% (target: 85\%, status: PASS)
  \item \textbf{Critical Paths}: 96\% controllers, 94\% dynamics (target: 95\%, status: PASS)
  \item \textbf{Safety-Critical}: 100\% (saturation, state validators, status: PASS)
  \item \textbf{Test Count}: 200+ pytest tests, 15 benchmark tests
\end{itemize}

% =============================================================================
% SECTION 3: TEST ORGANIZATION
% =============================================================================
\section{Test Organization}

\subsection{Peer File Structure}
\textbf{Rule}: Every \texttt{*.py} file has a corresponding \texttt{test\_*.py} peer.

\begin{lstlisting}[style=bashstyle]
src/
  controllers/
    classical_smc.py
    sta_smc.py
    adaptive_smc.py
  core/
    simulation_runner.py
  optimizer/
    pso_optimizer.py

tests/
  test_controllers/
    test_classical_smc.py       # Peer for classical_smc.py
    test_sta_smc.py             # Peer for sta_smc.py
    test_adaptive_smc.py        # Peer for adaptive_smc.py
  test_core/
    test_simulation_runner.py   # Peer for simulation_runner.py
  test_optimizer/
    test_pso_optimizer.py       # Peer for pso_optimizer.py
\end{lstlisting}

\subsection{Test Discovery}
\begin{lstlisting}[style=bashstyle]
# Run All Tests
python -m pytest tests/ -v

# Run Specific Module
python -m pytest tests/test_controllers/ -v

# Run Single File
python -m pytest tests/test_controllers/test_classical_smc.py -v

# Run with Coverage Report
python -m pytest tests/ --cov=src --cov-report=html
\end{lstlisting}

\subsection{Peer File Validation}
\begin{lstlisting}[style=bashstyle]
# Check for Missing Peer Files
python scripts/architecture/find_untested.py

# Output Example
[WARNING] Missing test peers:
  src/utils/plotting/advanced_plots.py
  src/core/batch_simulator.py

[INFO] Action: Create tests/test_utils/test_plotting/test_advanced_plots.py
[INFO] Action: Create tests/test_core/test_batch_simulator.py
\end{lstlisting}

% =============================================================================
% SECTION 4: VALIDATION STRATEGIES
% =============================================================================
\section{Validation Strategies}

\subsection{1. Unit Tests (Isolated Components)}
\textbf{Goal}: Verify single function/class behavior in isolation.

\begin{lstlisting}[language=python]
def test_classical_smc_zero_error():
    """Test SMC returns zero control when error is zero."""
    controller = ClassicalSMC(lambda1=10, lambda2=5, phi1=2, phi2=1)
    state = np.array([0, 0, 0, 0])  # Zero error
    control = controller.compute_control(state)
    assert np.allclose(control, 0.0, atol=1e-6)

def test_pso_converges():
    """Test PSO reduces cost over iterations."""
    optimizer = PSOTuner(swarm_size=10, iterations=20)
    initial_cost = optimizer.best_cost
    optimizer.optimize(objective_function)
    final_cost = optimizer.best_cost
    assert final_cost < initial_cost  # Cost must decrease
\end{lstlisting}

\subsection{2. Integration Tests (Multi-Component)}
\textbf{Goal}: Verify components work together correctly.

\begin{lstlisting}[language=python]
def test_simulation_end_to_end():
    """Test full simulation pipeline (controller + dynamics)."""
    controller = create_controller("classical_smc", config)
    dynamics = SimplifiedDynamics(config)
    runner = SimulationRunner(controller, dynamics, config)

    result = runner.run()

    # Verify convergence
    assert result.settling_time < 5.0  # Settles in <5s
    assert np.max(np.abs(result.states[-1])) < 0.1  # Final error <0.1

def test_pso_tunes_controller():
    """Test PSO optimizer improves controller performance."""
    baseline_iae = simulate_with_manual_gains()
    pso = PSOTuner(swarm_size=30, iterations=50)
    optimized_gains = pso.optimize(controller_objective)

    optimized_iae = simulate_with_gains(optimized_gains)
    assert optimized_iae < baseline_iae  # PSO improves performance
\end{lstlisting}

\subsection{3. Property-Based Tests (Theoretical Validation)}
\textbf{Goal}: Verify mathematical properties hold for ALL inputs.

\begin{lstlisting}[language=python]
from hypothesis import given, strategies as st

@given(st.floats(min_value=-10, max_value=10, allow_nan=False))
def test_saturation_bounds(control_input):
    """Test saturation enforces bounds for ALL inputs."""
    saturated = saturate(control_input, u_max=5.0)
    assert -5.0 <= saturated <= 5.0  # Always bounded

@given(st.lists(st.floats(), min_size=4, max_size=4))
def test_lyapunov_decreasing(state):
    """Test Lyapunov function decreases for ALL states."""
    V_current = compute_lyapunov(state)
    state_next = simulate_step(state)
    V_next = compute_lyapunov(state_next)
    assert V_next < V_current  # Lyapunov always decreases
\end{lstlisting}

\subsection{4. Edge Case Tests}
\textbf{Goal}: Verify behavior at boundaries and degenerate cases.

\begin{lstlisting}[language=python]
def test_controller_singular_matrix():
    """Test controller handles singular inertia matrix."""
    dynamics = SimplifiedDynamics(inertia=np.zeros((4,4)))  # Singular
    with pytest.raises(SingularMatrixError):
        simulate(dynamics)

def test_pso_empty_search_space():
    """Test PSO handles zero-width search bounds."""
    bounds = [(1.0, 1.0), (2.0, 2.0)]  # Zero width
    with pytest.raises(ValueError, match="Search space has zero volume"):
        PSOTuner(bounds=bounds)

def test_controller_inf_nan_inputs():
    """Test controller rejects inf/nan inputs gracefully."""
    controller = ClassicalSMC(lambda1=10, lambda2=5)
    state = np.array([np.inf, np.nan, 0, 0])
    with pytest.raises(ValueError, match="State contains inf/nan"):
        controller.compute_control(state)
\end{lstlisting}

% =============================================================================
% SECTION 5: BENCHMARK TESTS
% =============================================================================
\section{Benchmark Tests}

\subsection{Purpose}
\begin{itemize}
  \item \textbf{Performance Regression Detection}: Alert if simulation slows by >5\%
  \item \textbf{Comparative Analysis}: Benchmark 7 controllers head-to-head
  \item \textbf{Optimization Validation}: Verify PSO reduces IAE by 40\%+
\end{itemize}

\subsection{Benchmark Organization}
\begin{lstlisting}[style=bashstyle]
tests/test_benchmarks/
  test_controller_benchmarks.py   # Time per step, IAE, chattering
  test_pso_benchmarks.py          # Convergence speed, final cost
  test_simulation_benchmarks.py   # End-to-end runtime
\end{lstlisting}

\subsection{Example Benchmark Test}
\begin{lstlisting}[language=python]
import pytest

@pytest.mark.benchmark(group="controllers")
def test_classical_smc_benchmark(benchmark):
    """Benchmark classical SMC compute_control() speed."""
    controller = ClassicalSMC(lambda1=10, lambda2=5, phi1=2, phi2=1)
    state = np.array([0.1, 0.2, 0.0, 0.0])

    result = benchmark(controller.compute_control, state)

    # Verify performance target (10ms max)
    assert result.stats.mean < 0.01  # <10ms average

@pytest.mark.benchmark(group="optimization")
def test_pso_benchmark(benchmark):
    """Benchmark PSO convergence time."""
    pso = PSOTuner(swarm_size=30, iterations=50)

    result = benchmark(pso.optimize, rosenbrock_function)

    # Verify convergence time (<2 seconds)
    assert result.stats.mean < 2.0
\end{lstlisting}

\subsection{Running Benchmarks}
\begin{lstlisting}[style=bashstyle]
# Run All Benchmarks
python -m pytest tests/test_benchmarks/ --benchmark-only

# Compare Against Baseline
python -m pytest tests/test_benchmarks/ --benchmark-compare=baseline.json

# Save New Baseline
python -m pytest tests/test_benchmarks/ --benchmark-save=new_baseline
\end{lstlisting}

% =============================================================================
% SECTION 6: PRE-COMMIT HOOKS
% =============================================================================
\section{Pre-commit Hooks}

\subsection{Automated Test Enforcement}
\textbf{File}: \texttt{.pre-commit-config.yaml}

\begin{lstlisting}[language=yaml]
repos:
  - repo: local
    hooks:
      # Run Pytest on Changed Files
      - id: pytest
        name: Run Tests
        entry: python -m pytest tests/ -v
        language: system
        pass_filenames: false
        stages: [commit]

      # Check Coverage Thresholds
      - id: coverage
        name: Check Coverage
        entry: python -m pytest --cov=src --cov-report=term --cov-fail-under=85
        language: system
        pass_filenames: false
        stages: [commit]

      # Find Untested Files
      - id: untested
        name: Check for Untested Files
        entry: python scripts/architecture/find_untested.py
        language: system
        stages: [commit]
\end{lstlisting}

\subsection{Commit Workflow}
\begin{lstlisting}[style=bashstyle]
# Attempt Commit (triggers pre-commit hooks)
git commit -m "feat: Add adaptive SMC controller"

# Pre-commit Hook Output
[INFO] Running pytest tests/
===================== 200 passed in 12.3s =====================

[INFO] Checking coverage thresholds
Coverage: 87% (target: 85%) PASS

[INFO] Checking for untested files
All files have test peers. PASS

[OK] Pre-commit checks passed. Commit created.
\end{lstlisting}

% =============================================================================
% SECTION 7: CI/CD INTEGRATION
% =============================================================================
\section{CI/CD Integration}

\subsection{GitHub Actions Workflow}
\textbf{File}: \texttt{.github/workflows/tests.yml}

\begin{lstlisting}[language=yaml]
name: Test Suite
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Dependencies
        run: pip install -r requirements.txt

      - name: Run Tests
        run: python -m pytest tests/ -v --cov=src --cov-report=xml

      - name: Upload Coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          fail_ci_if_error: true

      - name: Check Coverage Thresholds
        run: |
          python -m pytest --cov=src --cov-fail-under=85
          python scripts/quality/check_critical_coverage.py  # Verify 95% critical

      - name: Run Benchmarks
        run: python -m pytest tests/test_benchmarks/ --benchmark-only
\end{lstlisting}

\subsection{Pull Request Checks}
\begin{itemize}
  \item \textbf{Test Pass Rate}: 100\% required (all 200+ tests must pass)
  \item \textbf{Coverage Thresholds}: Overall $\geq 85\%$, critical $\geq 95\%$
  \item \textbf{Benchmark Regression}: No slowdowns >5\% vs. baseline
  \item \textbf{Linting}: Ruff + MyPy strict mode, zero errors
\end{itemize}

% =============================================================================
% SECTION 8: TESTING ANTI-PATTERNS
% =============================================================================
\section{Testing Anti-Patterns}

\subsection{Anti-Pattern 1: Trivial Tests}
\textbf{Problem}: Tests that only verify imports or type checks.

\begin{lstlisting}[language=python]
# BAD: Trivial test (no value)
def test_controller_imports():
    from src.controllers.classical_smc import ClassicalSMC
    assert ClassicalSMC is not None  # Useless assertion

# GOOD: Functional test
def test_controller_convergence():
    controller = ClassicalSMC(...)
    result = simulate(controller, initial_state=[1, 0, 0, 0])
    assert result.settling_time < 5.0  # Meaningful property
\end{lstlisting}

\subsection{Anti-Pattern 2: Non-Deterministic Tests}
\textbf{Problem}: Tests that fail randomly due to uncontrolled randomness.

\begin{lstlisting}[language=python]
# BAD: Random seed not fixed
def test_pso_converges():
    pso = PSOTuner()  # No seed
    result = pso.optimize(objective)
    assert result.cost < 0.1  # Fails 10% of time

# GOOD: Fixed seed for reproducibility
def test_pso_converges():
    pso = PSOTuner(seed=42)  # Deterministic
    result = pso.optimize(objective)
    assert result.cost < 0.1  # Always passes
\end{lstlisting}

\subsection{Anti-Pattern 3: Monolithic Tests}
\textbf{Problem}: Single test verifies 10+ behaviors (hard to debug).

\begin{lstlisting}[language=python]
# BAD: Test does too much
def test_entire_system():
    # 50 lines testing controller + dynamics + PSO + visualization
    assert everything_works  # Unclear what failed

# GOOD: Atomic tests
def test_controller_compute():
    # Test only controller.compute_control()

def test_dynamics_step():
    # Test only dynamics.step()

def test_pso_optimize():
    # Test only pso.optimize()
\end{lstlisting}

% =============================================================================
% SECTION 9: COVERAGE MEASUREMENT
% =============================================================================
\section{Coverage Measurement}

\subsection{Line Coverage}
\textbf{Tool}: \texttt{pytest-cov}

\begin{lstlisting}[style=bashstyle]
# Generate HTML Report
python -m pytest --cov=src --cov-report=html

# View Report
open htmlcov/index.html  # (Mac/Linux)
start htmlcov/index.html # (Windows)
\end{lstlisting}

\subsection{Branch Coverage}
\textbf{Goal}: Verify all conditional branches (if/else) tested.

\begin{lstlisting}[style=bashstyle]
# Enable Branch Coverage
python -m pytest --cov=src --cov-branch --cov-report=term

# Example Output
src/controllers/classical_smc.py   96%   (4 branches, 3 covered)
src/optimizer/pso_optimizer.py     94%   (12 branches, 11 covered)
\end{lstlisting}

\subsection{Critical Path Coverage}
\textbf{Script}: \texttt{scripts/quality/check\_critical\_coverage.py}

\begin{lstlisting}[language=python]
CRITICAL_PATHS = [
    "src/controllers/",
    "src/core/dynamics.py",
    "src/optimizer/pso_optimizer.py",
    "src/utils/control_primitives.py"
]

def check_critical_coverage():
    coverage_data = parse_coverage_report()
    for path in CRITICAL_PATHS:
        coverage = coverage_data[path]
        if coverage < 0.95:  # 95% threshold
            raise Exception(f"Critical path {path} only {coverage*100}% covered")
    print("[OK] All critical paths meet 95% threshold")
\end{lstlisting}

% =============================================================================
% SECTION 10: MAINTENANCE \& CONTINUOUS IMPROVEMENT
% =============================================================================
\section{Maintenance \& Continuous Improvement}

\subsection{Monthly Test Audit}
\begin{lstlisting}[style=bashstyle]
# 1. Find Untested Code
python scripts/architecture/find_untested.py

# 2. Identify Low-Coverage Modules
python -m pytest --cov=src --cov-report=term | grep -E '[0-7][0-9]%'

# 3. Review Flaky Tests (tests that fail intermittently)
python scripts/quality/detect_flaky_tests.py

# 4. Update Benchmark Baselines
python -m pytest tests/test_benchmarks/ --benchmark-save=baseline_jan2025
\end{lstlisting}

\subsection{Regression Analysis}
\begin{itemize}
  \item \textbf{Track Test Count Growth}: Target 10+ new tests per major feature
  \item \textbf{Monitor Coverage Trends}: Alert if coverage drops below 85\%
  \item \textbf{Benchmark Drift}: Flag if any benchmark slows by >5\%
  \item \textbf{Flaky Test Detection}: Remove non-deterministic tests
\end{itemize}

\subsection{Test Documentation}
\textbf{File}: \texttt{tests/README.md}

\begin{itemize}
  \item Document test organization (peer files, benchmarks, integration)
  \item List coverage targets (85\%/95\%/100\% tiers)
  \item Explain how to run specific test suites
  \item Provide examples of property-based tests
  \item Link to CI/CD workflow and pre-commit hooks
\end{itemize}

% =============================================================================
% SECTION 11: SUCCESS METRICS
% =============================================================================
\section{Success Metrics}

\subsection{Quantitative Indicators}
\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Target} & \textbf{Current} \\
\midrule
Overall coverage & $\geq 85\%$ & 87\% \\
Critical coverage & $\geq 95\%$ & 96\% (controllers) \\
Safety-critical coverage & 100\% & 100\% (saturation) \\
Test count & $>200$ & 215 \\
Benchmark tests & $>15$ & 18 \\
Flaky test rate & $<2\%$ & 0.5\% \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Qualitative Indicators}
\begin{itemize}
  \item Zero critical bugs reported in Phase 5 research (11 tasks, 46 hours)
  \item All edge cases discovered via property-based tests (Hypothesis)
  \item Pre-commit hooks prevent untested code from merging
  \item CI/CD catches regressions before production deployment
\end{itemize}

% =============================================================================
% CHECKLIST: TESTING PHILOSOPHY
% =============================================================================
\section*{Checklist: Testing Philosophy}
\begin{itemize}
  \item[$\square$] \textbf{Organize}: Create peer test files (test\_*.py for every *.py)
  \item[$\square$] \textbf{Coverage}: Achieve 85\% overall, 95\% critical, 100\% safety-critical
  \item[$\square$] \textbf{Validate}: Write property-based tests for theoretical invariants
  \item[$\square$] \textbf{Benchmark}: Track performance regressions (<5\% tolerance)
  \item[$\square$] \textbf{Pre-commit}: Enable hooks to block untested code
  \item[$\square$] \textbf{CI/CD}: Integrate GitHub Actions for PR checks
  \item[$\square$] \textbf{Edge Cases}: Test singular matrices, inf/nan, zero-width bounds
  \item[$\square$] \textbf{Monitor}: Monthly audits for flaky tests and coverage drift
\end{itemize}

% =============================================================================
% NEXT STEPS
% =============================================================================
\section*{Next Steps}
\begin{itemize}
  \item \textbf{E019}: Production safety - memory management and thread safety
  \item \textbf{E020}: MCP integration - auto-trigger strategy and server orchestration
  \item \textbf{E021}: Maintenance mode and future vision for DIP-SMC-PSO
\end{itemize}

\end{document}
