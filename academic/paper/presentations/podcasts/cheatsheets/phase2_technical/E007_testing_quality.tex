% ==============================================================================
% EPISODE 007: TESTING AND QUALITY ASSURANCE
% ==============================================================================
\input{../templates/master_template.tex}
\input{../templates/tikz_components.tex}

% Episode-specific title
\renewcommand{\episodetitle}{E007: Testing \& QA}

\begin{document}

% ==============================================================================
% TITLE PAGE
% ==============================================================================
\makeepisodetitle{E007: Testing and Quality Assurance}{4,563 Tests in 45 Seconds}{2}{15-20 minutes}

% ==============================================================================
% PAGE 1: TEST PYRAMID & COVERAGE
% ==============================================================================
\learningobjective{Understand test pyramid, coverage standards (85/95/100\%), property-based testing, and quality gates for research vs production}

\section*{The Testing Challenge}

\begin{keypoint}
\textbf{Question:} How many tests to validate 105,000 lines of code?

\textbf{Answer:} 4,563 tests - BUT quality > quantity!

\textbf{Real question:} What deserves a test? When do you have enough?
\end{keypoint}

\subsection*{Three Hard Questions}

\begin{warning}
\begin{itemize}
    \item \textbf{1.} \textbf{What to test?} One input or all possible inputs? (All is impossible $\Rightarrow$ sample strategically)
    \item \textbf{2.} \textbf{When to stop?} Can always write more tests - what's "good enough"?
    \item \textbf{3.} \textbf{What to validate?} Implementation details (break on refactor) vs behavior (stable contract)?
\end{itemize}

\textbf{Testing is strategic choice-making under time constraints}
\end{warning}

\section*{Test Suite Breakdown: 4,563 Tests}

\begin{tcolorbox}[colback=primary!5, colframe=primary, title=\faFlask\ Test Suite Distribution]
\begin{tabular}{llll}
\toprule
\textbf{Level} & \textbf{Count} & \textbf{Percent} & \textbf{Time/Test} \\
\midrule
Unit Tests & 3,678 & 81\% & 200 $\mu$s \\
Integration Tests & 681 & 15\% & 50 ms \\
System Tests & 182 & 4\% & 2 s \\
Browser Tests & 22 & 0.5\% & 5 s \\
\textbf{TOTAL} & \textbf{4,563} & \textbf{100\%} & \textbf{45 seconds} \\
\bottomrule
\end{tabular}
\end{tcolorbox}

\section*{The Test Pyramid}

\begin{center}
\begin{tikzpicture}[scale=0.9]
    % Pyramid layers
    \fill[primary!30] (0,0) -- (8,0) -- (7,2) -- (1,2) -- cycle;
    \fill[secondary!30] (1,2) -- (7,2) -- (5.5,4) -- (2.5,4) -- cycle;
    \fill[accent!30] (2.5,4) -- (5.5,4) -- (4.5,5.5) -- (3.5,5.5) -- cycle;
    \fill[warning!30] (3.5,5.5) -- (4.5,5.5) -- (4,6.5) -- cycle;

    % Labels
    \node[font=\Large\bfseries] at (4,0.8) {Unit Tests (3,678)};
    \node at (4,0.3) {\small 8 seconds - Fast \& Massive};

    \node[font=\large\bfseries] at (4,2.8) {Integration (681)};
    \node at (4,2.3) {\small 15 seconds - Module interactions};

    \node[font=\bfseries] at (4,4.5) {System (182)};
    \node at (4,4.1) {\tiny 20 seconds - Full pipeline};

    \node[font=\small\bfseries] at (4,5.9) {Browser (22)};
    \node[font=\tiny] at (4,5.6) {2 seconds - UI};

    % Run frequency annotations
    \draw[->, thick, color=primary] (8.5,1) -- (9.5,1);
    \node[right, color=primary] at (9.5,1) {\small Every few minutes};

    \draw[->, thick, color=secondary] (8.5,3) -- (9.5,3);
    \node[right, color=secondary] at (9.5,3) {\small Before commits};

    \draw[->, thick, color=accent] (8.5,4.75) -- (9.5,4.75);
    \node[right, color=accent] at (9.5,4.75) {\small In CI};

    \draw[->, thick, color=warning] (8.5,6) -- (9.5,6);
    \node[right, color=warning] at (9.5,6) {\small Nightly};
\end{tikzpicture}
\end{center}

\begin{tip}
\textbf{Pyramid Philosophy:}
\begin{itemize}
    \item \textbf{Wide base:} Lots of fast tests (run constantly during development)
    \item \textbf{Narrow top:} Few slow tests (run before commits, in CI)
    \item \textbf{Speed enables feedback:} 45 seconds total $\Rightarrow$ run every few minutes!
\end{itemize}
\end{tip}

\subsection*{Unit Tests: The Foundation}

\begin{example}
\textbf{3,678 unit tests, 200 microseconds each (8 seconds total)}

\textbf{Example:} Classical SMC has 51 unit tests covering:
\begin{itemize}
    \item Zero state (equilibrium)
    \item Maximum gains (saturation)
    \item Boundary layer transitions ($s \to 0$)
    \item Edge cases (NaN, infinity, negative values)
\end{itemize}

\textbf{Pattern:} Pass known state $\to$ Get control signal $\to$ Verify matches expected
\end{example}

\subsection*{Integration Tests: Module Interactions}

\begin{multicols}{2}

\textbf{681 integration tests, 50 ms each (15 seconds total)}

\textbf{What they test:}
\begin{itemize}
    \item Factory + Config: Parse YAML, create controller
    \item Controller + Dynamics: Interface compatibility
    \item PSO + Simulator: Batch evaluation
\end{itemize}

\columnbreak

\textbf{Example:}
\begin{lstlisting}[style=python, numbers=none]
def test_factory_config():
    config = load_config("test.yaml")
    ctrl = create_controller(
        config.controller.type,
        config.controller.params
    )
    assert ctrl.gains == config.gains
\end{lstlisting}

\end{multicols}

% ==============================================================================
% PAGE 2: COVERAGE STANDARDS
% ==============================================================================
\newpage

\section*{Coverage Standards: 85 / 95 / 100}

\begin{keypoint}
\textbf{Three-tier coverage requirements:}
\begin{itemize}
    \item \textbf{1.} \textbf{Overall project:} 85\% minimum (aggregate across all files)
    \item \textbf{2.} \textbf{Critical modules:} 95\% minimum (controllers, dynamics, PSO)
    \item \textbf{3.} \textbf{Safety-critical:} 100\% required (saturation, validation, monitoring)
\end{itemize}
\end{keypoint}

\subsection*{Why Different Standards?}

\begin{multicols}{2}

\textbf{Risk-based prioritization:}

\textbf{Utility function} (formats logs):
\begin{itemize}
    \item Failure $\Rightarrow$ Garbled log entry
    \item \textbf{Cost:} Annoying
    \item \textbf{Coverage:} 85\% OK
\end{itemize}

\columnbreak

\textbf{Saturation function} (limits force):
\begin{itemize}
    \item Failure $\Rightarrow$ Command 10,000 N to 150 N actuator
    \item \textbf{Cost:} Broken hardware!
    \item \textbf{Coverage:} 100\% MANDATORY
\end{itemize}

\end{multicols}

\subsection*{The 100\% Coverage List (10 Modules)}

\begin{tcolorbox}[colback=warning!10, colframe=warning, title=\faExclamationTriangle\ Safety-Critical Modules]
\textbf{Safety Modules:}
\begin{itemize}
    \item Saturation - Prevents actuator damage (10,000 N $\to$ 150 N max)
    \item Validation - Stops physically impossible configs (negative mass!)
    \item Deadband - Prevents actuator oscillation near setpoint
\end{itemize}

\textbf{Correctness Modules:}
\begin{itemize}
    \item Reproducibility - Deterministic random seeds (peer review requirement!)
    \item State Manager - Prevents simulation corruption
    \item Config Validator - Catches errors before simulation
\end{itemize}

\textbf{Core Interfaces:}
\begin{itemize}
    \item Base Controller - Inherited by all 7 controllers
    \item Dynamics Interface - Swappable plant models
    \item PSO Bounds - Keeps optimization within valid ranges
\end{itemize}

\textbf{Monitoring:}
\begin{itemize}
    \item Latency Tracker - Detects missed control deadlines
\end{itemize}
\end{tcolorbox}

\begin{warning}
\textbf{Why reproducibility is critical:}

If reviewer can't reproduce results (bad random seeds) $\Rightarrow$ \textbf{Paper invalid!}

Consequences: Rejected paper $\to$ Broken \$50,000 robot

Reproducibility is not optional in research software.
\end{warning}

\subsection*{CI Enforcement}

\begin{example}
\textbf{Pull Request Rules:}
\begin{itemize}
    \item \textbf{1.} Add 100 lines to critical module
    \item \textbf{2.} Must add tests to maintain 95\%+ coverage
    \item \textbf{3.} If coverage drops below 95\% $\Rightarrow$ \textbf{Build FAILS}
    \item \textbf{4.} Cannot merge until tests added
\end{itemize}

\textbf{This prevents "I'll add tests later" syndrome!}
\end{example}

% ==============================================================================
% PAGE 3: PROPERTY-BASED TESTING
% ==============================================================================
\newpage

\section*{Property-Based Testing with Hypothesis}

\begin{keypoint}
\textbf{Traditional testing:} Write test with ONE specific input

\textbf{Property-based testing:} Write property that holds for ALL inputs

\textbf{Hypothesis framework:} Generates hundreds of random inputs, checks property for each
\end{keypoint}

\subsection*{Example: Saturation Function}

\begin{multicols}{2}

\textbf{Traditional Test:}
\begin{lstlisting}[style=python, numbers=none]
def test_saturation():
    result = saturate(200, max=150)
    assert result == 150
# Tests ONE case (200)
\end{lstlisting}

\textbf{What about:} 151? 10,000? 1 million?

\columnbreak

\textbf{Property-Based Test:}
\begin{lstlisting}[style=python, numbers=none]
@given(value=st.floats(
    min_value=151, max_value=1e6))
def test_saturation_property(value):
    result = saturate(value, max=150)
    assert result == 150
# Tests 100 random cases!
\end{lstlisting}

\end{multicols}

\begin{tip}
\textbf{It's like having a robot stress-test your code while you sleep!}

Hypothesis generates edge cases you never thought of: \texttt{NaN}, \texttt{inf}, negative zero, etc.
\end{tip}

\subsection*{Properties We Test}

\begin{tcolorbox}[colback=accent!10, colframe=accent, title=\faFlask\ System Properties]
\textbf{Controller Properties:}
\begin{itemize}
    \item Control signal must be bounded ($|u| \leq u_{\max}$)
    \item Control must not contain NaN or infinity
    \item Control must be deterministic (same state $\Rightarrow$ same output)
\end{itemize}

\textbf{Dynamics Properties:}
\begin{itemize}
    \item State derivatives must be finite (no explosions!)
    \item Energy conserved in absence of friction
    \item Linearization matches finite-difference approximation
\end{itemize}

\textbf{PSO Properties:}
\begin{itemize}
    \item Best cost must never increase (monotonic improvement)
    \item Final best particle within search bounds
    \item Optimization reproducible with same seed
\end{itemize}
\end{tcolorbox}

\section*{Coverage Campaign: Week 3 Bug Hunt}

\begin{example}
\textbf{December 20-21, 2025: The 16.5-Hour Sprint}

\textbf{Mission:} Bring 10 critical modules to 100\% coverage before holidays

\textbf{Results:}
\begin{itemize}
    \item 668 tests created
    \item 11 modules validated (beat goal by one!)
    \item 2 silent killers found and fixed same-day
\end{itemize}

\textbf{Felt like defusing bombs while clock ticked down}
\end{example}

\subsection*{Bug 1: Factory API Mismatch}

\begin{warning}
\textbf{Problem:} Factory expected gains as \texttt{list}, config provided \texttt{numpy.ndarray}

\textbf{Symptom:} Worked in most cases, failed when serializing to JSON

\textbf{Fix:} Explicitly convert to list in factory

\textbf{Found via:} Integration test for controller state serialization
\end{warning}

\subsection*{Bug 2: Memory Leak in Adaptive Controller}

\begin{warning}
\textbf{The Silent Killer:}

Adaptive controller stored reference to EVERY simulation's full history (for debugging).

Never released memory (hoarding!).

\textbf{Impact:}
\begin{itemize}
    \item After 1,000 simulations (typical PSO run): 500 MB RAM
    \item Overnight optimizations crashed at hour 9 of 10-hour run
\end{itemize}

\textbf{Fix:} Use \texttt{weakref} - "Remember where object is, but don't hold it hostage"

\textbf{Found via:} Property-based test running 10,000 consecutive simulations, asserting memory growth = 0
\end{warning}

% ==============================================================================
% PAGE 4: QUALITY GATES & QUICK REFERENCE
% ==============================================================================
\newpage

\section*{Test Execution: 45 Seconds for 4,563 Tests}

\begin{tcolorbox}[colback=secondary!10, colframe=secondary, title=\faClock\ Execution Breakdown]
\begin{tabular}{lll}
\toprule
\textbf{Test Type} & \textbf{Time} & \textbf{Why So Fast?} \\
\midrule
Unit (3,678) & 8 seconds & No I/O, pure functions \\
Integration (681) & 15 seconds & Load configs, few timesteps \\
System (182) & 20 seconds & Simplified dynamics + Numba JIT \\
Browser (22) & 2 seconds & Parallel with pytest-xdist \\
\textbf{TOTAL} & \textbf{45 seconds} & \textbf{10 ms/test average} \\
\bottomrule
\end{tabular}
\end{tcolorbox}

\begin{tip}
\textbf{Why speed matters:}

10-minute tests $\Rightarrow$ Developers don't run during development $\Rightarrow$ Commit broken code $\Rightarrow$ Wait for CI failure $\Rightarrow$ Slow iteration

45-second tests $\Rightarrow$ Run every few minutes locally $\Rightarrow$ Catch failures before commit $\Rightarrow$ Fast feedback loop
\end{tip}

\section*{Quality Gates: Research vs Production}

\begin{tcolorbox}[colback=background, colframe=codeblock, title=\faCheckSquare\ 8 Quality Gates (5/8 Pass)]
\textbf{Gates We PASS (Research-Ready):}
\begin{itemize}
    \item \textbf{1.} Zero critical bugs (all P0 issues resolved)
    \item \textbf{2.} 100\% test pass rate (4,563/4,563 tests)
    \item \textbf{3.} Memory validated (10,000 sims, zero growth)
    \item \textbf{4.} Thread-safe (11/11 parallel PSO tests pass)
    \item \textbf{5.} Zero high-priority issues
\end{itemize}

\textbf{Gates We FAIL (Production Blockers):}
\begin{itemize}
    \setcounter{enumi}{5}
    \item \textbf{1.} Coverage measurement broken (reports 2.86\%, real is 89\%)
    \item \textbf{2.} No production CI/CD (dev pipelines only)
    \item \textbf{3.} No hardware validation (never run on actual robot/PLC)
\end{itemize}
\end{tcolorbox}

\begin{keypoint}
\textbf{Bottom Line:}

\textbf{Research-Ready} (5/8) \iconKey: Can publish papers, run experiments, validate theories

\textbf{Production-Ready} (8/8) \iconWarning: Can deploy to industrial plant (need gates 6-8)

\textbf{Verdict:} Science is sound. Engineering needs hardening for production.
\end{keypoint}

\subsection*{Quick Reference: Testing Commands}

\quickref{Run Full Test Suite}{
\begin{lstlisting}[style=python, numbers=none]
# All 4,563 tests (45 seconds)
pytest tests/

# With coverage report
pytest tests/ --cov=src --cov-report=html

# Parallel execution (faster on multi-core)
pytest tests/ -n auto
\end{lstlisting}
}

\quickref{Run Specific Test Levels}{
\begin{lstlisting}[style=python, numbers=none]
# Unit tests only (8 seconds)
pytest tests/test_unit/

# Integration tests (15 seconds)
pytest tests/test_integration/

# System tests (20 seconds)
pytest tests/test_system/

# Browser tests (2 seconds)
pytest tests/test_browser/
\end{lstlisting}
}

\quickref{Property-Based Testing}{
\begin{lstlisting}[style=python, numbers=none]
from hypothesis import given
from hypothesis import strategies as st

@given(value=st.floats(min_value=151, max_value=1e6))
def test_saturation_property(value):
    result = saturate(value, max=150)
    assert result == 150
    assert result <= 150  # Property!
\end{lstlisting}
}

\subsection*{Key Takeaways}

\begin{summary}
\textbf{Test Pyramid:} 81\% unit (fast), 15\% integration, 4\% system, 0.5\% browser

\textbf{Coverage Tiers:} 85\% overall, 95\% critical, 100\% safety-critical

\textbf{Property-Based Testing:} Hypothesis generates 100 random cases, finds edge cases you never thought of

\textbf{Quality Gates:} 5/8 pass (research-ready), need 8/8 for production

\textbf{Speed Enables Feedback:} 45 seconds for 4,563 tests $\Rightarrow$ run every few minutes

\textbf{Bugs Found:} Factory API mismatch + Memory leak (500 MB after 1,000 sims)
\end{summary}

\subsection*{What's Next?}

\begin{keypoint}
\textbf{E008: Research Outputs \& Publications} - 11 research tasks, submission-ready paper (v2.1), 14 figures

\textbf{Remember:} Testing is strategic choice-making under constraints. Quality > quantity!
\end{keypoint}

\end{document}
